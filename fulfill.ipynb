{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PSxUchiha/inpainting-gans/blob/main/fulfill.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "id": "1n-E0VqU7o96",
        "outputId": "2816e950-a242-4853-f243-dbcbc679b605"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f75fd322-87fa-4bfd-9cdf-34401828391d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f75fd322-87fa-4bfd-9cdf-34401828391d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download 'paulchambaz/google-street-view'\n",
        "! ls\n",
        "! mkdir -p train/street\n",
        "! unzip google-street-view.zip -d train/street"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv7RsMte71PZ",
        "outputId": "53e12e94-aebb-4778-f28c-cc83fe1e123a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "google-street-view.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "google-street-view.zip\tkaggle.json  sample_data  train\n",
            "Archive:  google-street-view.zip\n",
            "replace train/street/dataset/0.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoXgEbQ3C4e0"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "yQPN8hhD6736"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "0-qA-34X6739"
      },
      "outputs": [],
      "source": [
        "dataPath = 'train'\n",
        "imageSize = 128\n",
        "batchSize = 64\n",
        "localSize = 64\n",
        "niter = 25\n",
        "preniter = 50\n",
        "lr = 0.001\n",
        "alpha = 0.01\n",
        "beta1 = 0.5\n",
        "beta2 = 0.99\n",
        "hole_min = 32\n",
        "hole_max = 48\n",
        "workers = 2\n",
        "cuda = 1\n",
        "ndf = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp1PxuwQ673_",
        "outputId": "5294bfd7-6639-4300-81df-1c6e6ad55819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  8732\n",
            "CUDA device name: Tesla T4\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "manualSeed = random.randint(1,10000)\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "print(f'CUDA device name: {torch.cuda.get_device_name(\"cuda:0\")}')\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "print(device)\n",
        "nc = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "kahLmBC4674B"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "hlURfNX4674B"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(nc, ndf, 5, stride=1, padding=2, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 2, stride=2, padding=0, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 2, 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 2, stride=2, padding=0, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=2, dilation=2, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=4, dilation=4, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=8, dilation=8, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=16, dilation=16, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 4, 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ndf * 4, ndf * 2, 4, stride=2, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 2, 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ndf * 2, ndf, 4, stride=2, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf, int(ndf / 2), 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.BatchNorm2d(int(ndf / 2)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(int(ndf / 2), nc, 3, stride=1, padding=1, dilation=1, bias=True),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "4N-3j1Tk674D"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc_d = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 8, ndf * 8, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 8, ndf * 8, 4, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.disc_l = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf * 2, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 8, ndf * 8, 4, stride=2, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ndf * 8, ndf * 8, 4, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(ndf * 16, 1, 1, stride=1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x_global = self.disc_d(x1)\n",
        "        x_local = self.disc_l(x2)\n",
        "        x = torch.cat((x_global, x_local), 1)\n",
        "        output = self.classifier(x)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "qZhVPXtT674E"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    for m in m.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.ConvTranspose2d):\n",
        "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.normal_(0, 1)\n",
        "            m.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "MOTwgj3m674F"
      },
      "outputs": [],
      "source": [
        "def get_points():\n",
        "    points = []\n",
        "    mask = []\n",
        "    for i in range(batchSize):\n",
        "        x1, y1 = np.random.randint(0, imageSize - localSize + 1, 2)\n",
        "        x2, y2 = np.array([x1, y1]) + localSize\n",
        "        points.append([x1, y1, x2, y2])\n",
        "\n",
        "        w, h = np.random.randint(hole_min, hole_max + 1, 2)\n",
        "        p1 = x1 + np.random.randint(0, localSize - w)\n",
        "        q1 = y1 + np.random.randint(0, localSize - h)\n",
        "        p2 = p1 + w\n",
        "        q2 = q1 + h\n",
        "\n",
        "        m = np.zeros((1, imageSize, imageSize), dtype=np.uint8)\n",
        "        m[:, q1:q2 + 1, p1:p2 + 1] = 1\n",
        "        mask.append(m)\n",
        "\n",
        "    return np.array(points), np.array(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "-MDVG9cY674G"
      },
      "outputs": [],
      "source": [
        "def crop_local_patches(images, points, bsize):\n",
        "    cropped_images = torch.zeros([bsize, nc, imageSize // 2, imageSize // 2], dtype=torch.float32, device='cuda:0')\n",
        "    for i in range(bsize):\n",
        "        t = Variable(images[i])\n",
        "        cropped_images[i] = t[:, points[i][0]:points[i][2], points[i][1]:points[i][3]]\n",
        "\n",
        "    return cropped_images.cuda()\n",
        "\n",
        "def save_checkpoint(state, curr_epoch):\n",
        "    torch.save(state, './models/netG_e%d.pth.tar' % (curr_epoch))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WK6PvG4674I"
      },
      "source": [
        "### Initialising Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD-B_Bvf674K",
        "outputId": "2ad9fadb-7e54-4373-f701-b04f016fb0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
            "    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
            "    (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (23): ReLU(inplace=True)\n",
            "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
            "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
            "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (34): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (38): ReLU(inplace=True)\n",
            "    (39): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (41): ReLU(inplace=True)\n",
            "    (42): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (44): ReLU(inplace=True)\n",
            "    (45): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (46): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (47): ReLU(inplace=True)\n",
            "    (48): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (49): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (disc_d): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "  )\n",
            "  (disc_l): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "netG = Generator().to(device)\n",
        "weights_init(netG)\n",
        "print(netG)\n",
        "\n",
        "netD = Discriminator().to(device)\n",
        "weights_init(netD)\n",
        "print(netD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRIBKQS674M"
      },
      "source": [
        "### Choosing Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "z50nCvtK674M"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "criterion2 = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvyWnFg0674N"
      },
      "source": [
        "### Initialise optimisers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "F8K1sOAb674O"
      },
      "outputs": [],
      "source": [
        "real_label = 1\n",
        "fake_label = 0\n",
        "# setup optimizers\n",
        "optG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "pL05s4Bx674P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxjY6cf_674P"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Td4rdsLQ674Q"
      },
      "outputs": [],
      "source": [
        "traindir = os.path.join(dataPath, 'street')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        dsets.ImageFolder(traindir, transforms.Compose([\n",
        "            transforms.Resize(128),\n",
        "            #transforms.CenterCrop(128),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                std=[0.5, 0.5, 0.5])\n",
        "        ])),\n",
        "        batch_size=batchSize, shuffle=True,\n",
        "        num_workers=workers, pin_memory=True)\n",
        "\n",
        "errD_all = AverageMeter()\n",
        "errG_all = AverageMeter()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29k4iRDQ674Q"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMSIwJFr674R",
        "outputId": "f48f9211-1bde-4ee1-8904-89541ff3c6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRETRAIN [0/25][0/157] Loss_G: 0.0257\n",
            "PRETRAIN [0/25][1/157] Loss_G: 0.0235\n",
            "PRETRAIN [0/25][2/157] Loss_G: 0.0214\n",
            "PRETRAIN [0/25][3/157] Loss_G: 0.0210\n",
            "PRETRAIN [0/25][4/157] Loss_G: 0.0219\n",
            "PRETRAIN [0/25][5/157] Loss_G: 0.0207\n",
            "PRETRAIN [0/25][6/157] Loss_G: 0.0206\n",
            "PRETRAIN [0/25][7/157] Loss_G: 0.0196\n",
            "PRETRAIN [0/25][8/157] Loss_G: 0.0222\n",
            "PRETRAIN [0/25][9/157] Loss_G: 0.0211\n",
            "PRETRAIN [0/25][10/157] Loss_G: 0.0228\n",
            "PRETRAIN [0/25][11/157] Loss_G: 0.0173\n",
            "PRETRAIN [0/25][12/157] Loss_G: 0.0262\n",
            "PRETRAIN [0/25][13/157] Loss_G: 0.0195\n",
            "PRETRAIN [0/25][14/157] Loss_G: 0.0195\n",
            "PRETRAIN [0/25][15/157] Loss_G: 0.0179\n",
            "PRETRAIN [0/25][16/157] Loss_G: 0.0167\n",
            "PRETRAIN [0/25][17/157] Loss_G: 0.0186\n",
            "PRETRAIN [0/25][18/157] Loss_G: 0.0186\n",
            "PRETRAIN [0/25][19/157] Loss_G: 0.0194\n",
            "PRETRAIN [0/25][20/157] Loss_G: 0.0188\n",
            "PRETRAIN [0/25][21/157] Loss_G: 0.0230\n",
            "PRETRAIN [0/25][22/157] Loss_G: 0.0276\n",
            "PRETRAIN [0/25][23/157] Loss_G: 0.0230\n",
            "PRETRAIN [0/25][24/157] Loss_G: 0.0201\n",
            "PRETRAIN [0/25][25/157] Loss_G: 0.0182\n",
            "PRETRAIN [0/25][26/157] Loss_G: 0.0190\n",
            "PRETRAIN [0/25][27/157] Loss_G: 0.0183\n",
            "PRETRAIN [0/25][28/157] Loss_G: 0.0218\n",
            "PRETRAIN [0/25][29/157] Loss_G: 0.0204\n",
            "PRETRAIN [0/25][30/157] Loss_G: 0.0220\n",
            "PRETRAIN [0/25][31/157] Loss_G: 0.0171\n",
            "PRETRAIN [0/25][32/157] Loss_G: 0.0188\n",
            "PRETRAIN [0/25][33/157] Loss_G: 0.0206\n",
            "PRETRAIN [0/25][34/157] Loss_G: 0.0204\n",
            "PRETRAIN [0/25][35/157] Loss_G: 0.0170\n",
            "PRETRAIN [0/25][36/157] Loss_G: 0.0192\n",
            "PRETRAIN [0/25][37/157] Loss_G: 0.0174\n",
            "PRETRAIN [0/25][38/157] Loss_G: 0.0160\n",
            "PRETRAIN [0/25][39/157] Loss_G: 0.0172\n",
            "PRETRAIN [0/25][40/157] Loss_G: 0.0188\n",
            "PRETRAIN [0/25][41/157] Loss_G: 0.0187\n",
            "PRETRAIN [0/25][42/157] Loss_G: 0.0175\n",
            "PRETRAIN [0/25][43/157] Loss_G: 0.0171\n",
            "PRETRAIN [0/25][44/157] Loss_G: 0.0167\n",
            "PRETRAIN [0/25][45/157] Loss_G: 0.0168\n",
            "PRETRAIN [0/25][46/157] Loss_G: 0.0194\n",
            "PRETRAIN [0/25][47/157] Loss_G: 0.0172\n",
            "PRETRAIN [0/25][48/157] Loss_G: 0.0183\n",
            "PRETRAIN [0/25][49/157] Loss_G: 0.0187\n",
            "PRETRAIN [0/25][50/157] Loss_G: 0.0188\n",
            "PRETRAIN [0/25][51/157] Loss_G: 0.0167\n",
            "PRETRAIN [0/25][52/157] Loss_G: 0.0159\n",
            "PRETRAIN [0/25][53/157] Loss_G: 0.0159\n",
            "PRETRAIN [0/25][54/157] Loss_G: 0.0165\n",
            "PRETRAIN [0/25][55/157] Loss_G: 0.0164\n",
            "PRETRAIN [0/25][56/157] Loss_G: 0.0167\n",
            "PRETRAIN [0/25][57/157] Loss_G: 0.0174\n",
            "PRETRAIN [0/25][58/157] Loss_G: 0.0187\n",
            "PRETRAIN [0/25][59/157] Loss_G: 0.0197\n",
            "PRETRAIN [0/25][60/157] Loss_G: 0.0162\n",
            "PRETRAIN [0/25][61/157] Loss_G: 0.0159\n",
            "PRETRAIN [0/25][62/157] Loss_G: 0.0198\n",
            "PRETRAIN [0/25][63/157] Loss_G: 0.0212\n",
            "PRETRAIN [0/25][64/157] Loss_G: 0.0178\n",
            "PRETRAIN [0/25][65/157] Loss_G: 0.0169\n",
            "PRETRAIN [0/25][66/157] Loss_G: 0.0193\n",
            "PRETRAIN [0/25][67/157] Loss_G: 0.0204\n",
            "PRETRAIN [0/25][68/157] Loss_G: 0.0195\n",
            "PRETRAIN [0/25][69/157] Loss_G: 0.0169\n",
            "PRETRAIN [0/25][70/157] Loss_G: 0.0180\n",
            "PRETRAIN [0/25][71/157] Loss_G: 0.0218\n",
            "PRETRAIN [0/25][72/157] Loss_G: 0.0213\n",
            "PRETRAIN [0/25][73/157] Loss_G: 0.0178\n",
            "PRETRAIN [0/25][74/157] Loss_G: 0.0206\n",
            "PRETRAIN [0/25][75/157] Loss_G: 0.0191\n",
            "PRETRAIN [0/25][76/157] Loss_G: 0.0195\n",
            "PRETRAIN [0/25][77/157] Loss_G: 0.0202\n",
            "PRETRAIN [0/25][78/157] Loss_G: 0.0171\n",
            "PRETRAIN [0/25][79/157] Loss_G: 0.0173\n",
            "PRETRAIN [0/25][80/157] Loss_G: 0.0183\n",
            "PRETRAIN [0/25][81/157] Loss_G: 0.0150\n",
            "PRETRAIN [0/25][82/157] Loss_G: 0.0151\n",
            "PRETRAIN [0/25][83/157] Loss_G: 0.0165\n",
            "PRETRAIN [0/25][84/157] Loss_G: 0.0158\n",
            "PRETRAIN [0/25][85/157] Loss_G: 0.0162\n",
            "PRETRAIN [0/25][86/157] Loss_G: 0.0159\n",
            "PRETRAIN [0/25][87/157] Loss_G: 0.0174\n",
            "PRETRAIN [0/25][88/157] Loss_G: 0.0151\n",
            "PRETRAIN [0/25][89/157] Loss_G: 0.0162\n",
            "PRETRAIN [0/25][90/157] Loss_G: 0.0138\n",
            "PRETRAIN [0/25][91/157] Loss_G: 0.0165\n",
            "PRETRAIN [0/25][92/157] Loss_G: 0.0157\n",
            "PRETRAIN [0/25][93/157] Loss_G: 0.0164\n",
            "PRETRAIN [0/25][94/157] Loss_G: 0.0158\n",
            "PRETRAIN [0/25][95/157] Loss_G: 0.0157\n",
            "PRETRAIN [0/25][96/157] Loss_G: 0.0186\n",
            "PRETRAIN [0/25][97/157] Loss_G: 0.0181\n",
            "PRETRAIN [0/25][98/157] Loss_G: 0.0185\n",
            "PRETRAIN [0/25][99/157] Loss_G: 0.0198\n",
            "PRETRAIN [0/25][100/157] Loss_G: 0.0166\n",
            "PRETRAIN [0/25][101/157] Loss_G: 0.0189\n",
            "PRETRAIN [0/25][102/157] Loss_G: 0.0153\n",
            "PRETRAIN [0/25][103/157] Loss_G: 0.0139\n",
            "PRETRAIN [0/25][104/157] Loss_G: 0.0141\n",
            "PRETRAIN [0/25][105/157] Loss_G: 0.0140\n",
            "PRETRAIN [0/25][106/157] Loss_G: 0.0146\n",
            "PRETRAIN [0/25][107/157] Loss_G: 0.0153\n",
            "PRETRAIN [0/25][108/157] Loss_G: 0.0130\n",
            "PRETRAIN [0/25][109/157] Loss_G: 0.0156\n",
            "PRETRAIN [0/25][110/157] Loss_G: 0.0156\n",
            "PRETRAIN [0/25][111/157] Loss_G: 0.0175\n",
            "PRETRAIN [0/25][112/157] Loss_G: 0.0166\n",
            "PRETRAIN [0/25][113/157] Loss_G: 0.0173\n",
            "PRETRAIN [0/25][114/157] Loss_G: 0.0165\n",
            "PRETRAIN [0/25][115/157] Loss_G: 0.0162\n",
            "PRETRAIN [0/25][116/157] Loss_G: 0.0142\n",
            "PRETRAIN [0/25][117/157] Loss_G: 0.0148\n",
            "PRETRAIN [0/25][118/157] Loss_G: 0.0159\n",
            "PRETRAIN [0/25][119/157] Loss_G: 0.0172\n",
            "PRETRAIN [0/25][120/157] Loss_G: 0.0147\n",
            "PRETRAIN [0/25][121/157] Loss_G: 0.0145\n",
            "PRETRAIN [0/25][122/157] Loss_G: 0.0156\n",
            "PRETRAIN [0/25][123/157] Loss_G: 0.0158\n",
            "PRETRAIN [0/25][124/157] Loss_G: 0.0142\n",
            "PRETRAIN [0/25][125/157] Loss_G: 0.0167\n",
            "PRETRAIN [0/25][126/157] Loss_G: 0.0162\n",
            "PRETRAIN [0/25][127/157] Loss_G: 0.0182\n",
            "PRETRAIN [0/25][128/157] Loss_G: 0.0146\n",
            "PRETRAIN [0/25][129/157] Loss_G: 0.0155\n",
            "PRETRAIN [0/25][130/157] Loss_G: 0.0176\n",
            "PRETRAIN [0/25][131/157] Loss_G: 0.0144\n",
            "PRETRAIN [0/25][132/157] Loss_G: 0.0140\n",
            "PRETRAIN [0/25][133/157] Loss_G: 0.0135\n",
            "PRETRAIN [0/25][134/157] Loss_G: 0.0154\n",
            "PRETRAIN [0/25][135/157] Loss_G: 0.0183\n",
            "PRETRAIN [0/25][136/157] Loss_G: 0.0153\n",
            "PRETRAIN [0/25][137/157] Loss_G: 0.0168\n",
            "PRETRAIN [0/25][138/157] Loss_G: 0.0143\n",
            "PRETRAIN [0/25][139/157] Loss_G: 0.0140\n",
            "PRETRAIN [0/25][140/157] Loss_G: 0.0159\n",
            "PRETRAIN [0/25][141/157] Loss_G: 0.0193\n",
            "PRETRAIN [0/25][142/157] Loss_G: 0.0166\n",
            "PRETRAIN [0/25][143/157] Loss_G: 0.0184\n",
            "PRETRAIN [0/25][144/157] Loss_G: 0.0275\n",
            "PRETRAIN [0/25][145/157] Loss_G: 0.0196\n",
            "PRETRAIN [0/25][146/157] Loss_G: 0.0170\n",
            "PRETRAIN [0/25][147/157] Loss_G: 0.0176\n",
            "PRETRAIN [0/25][148/157] Loss_G: 0.0171\n",
            "PRETRAIN [0/25][149/157] Loss_G: 0.0198\n",
            "PRETRAIN [0/25][150/157] Loss_G: 0.0209\n",
            "PRETRAIN [0/25][151/157] Loss_G: 0.0221\n",
            "PRETRAIN [0/25][152/157] Loss_G: 0.0215\n",
            "PRETRAIN [0/25][153/157] Loss_G: 0.0181\n",
            "PRETRAIN [0/25][154/157] Loss_G: 0.0176\n",
            "PRETRAIN [0/25][155/157] Loss_G: 0.0185\n",
            "PRETRAIN [0/25][156/157] Loss_G: 0.0115\n",
            "Time elapsed Epoch 0: 116 seconds\n",
            "PRETRAIN [1/25][0/157] Loss_G: 0.0177\n",
            "PRETRAIN [1/25][1/157] Loss_G: 0.0188\n",
            "PRETRAIN [1/25][2/157] Loss_G: 0.0176\n",
            "PRETRAIN [1/25][3/157] Loss_G: 0.0159\n",
            "PRETRAIN [1/25][4/157] Loss_G: 0.0158\n",
            "PRETRAIN [1/25][5/157] Loss_G: 0.0143\n",
            "PRETRAIN [1/25][6/157] Loss_G: 0.0155\n",
            "PRETRAIN [1/25][7/157] Loss_G: 0.0148\n",
            "PRETRAIN [1/25][8/157] Loss_G: 0.0158\n",
            "PRETRAIN [1/25][9/157] Loss_G: 0.0143\n",
            "PRETRAIN [1/25][10/157] Loss_G: 0.0182\n",
            "PRETRAIN [1/25][11/157] Loss_G: 0.0136\n",
            "PRETRAIN [1/25][12/157] Loss_G: 0.0157\n",
            "PRETRAIN [1/25][13/157] Loss_G: 0.0164\n",
            "PRETRAIN [1/25][14/157] Loss_G: 0.0135\n",
            "PRETRAIN [1/25][15/157] Loss_G: 0.0153\n",
            "PRETRAIN [1/25][16/157] Loss_G: 0.0150\n",
            "PRETRAIN [1/25][17/157] Loss_G: 0.0133\n",
            "PRETRAIN [1/25][18/157] Loss_G: 0.0141\n",
            "PRETRAIN [1/25][19/157] Loss_G: 0.0179\n",
            "PRETRAIN [1/25][20/157] Loss_G: 0.0175\n",
            "PRETRAIN [1/25][21/157] Loss_G: 0.0150\n",
            "PRETRAIN [1/25][22/157] Loss_G: 0.0151\n",
            "PRETRAIN [1/25][23/157] Loss_G: 0.0175\n",
            "PRETRAIN [1/25][24/157] Loss_G: 0.0135\n",
            "PRETRAIN [1/25][25/157] Loss_G: 0.0137\n",
            "PRETRAIN [1/25][26/157] Loss_G: 0.0139\n",
            "PRETRAIN [1/25][27/157] Loss_G: 0.0130\n",
            "PRETRAIN [1/25][28/157] Loss_G: 0.0161\n",
            "PRETRAIN [1/25][29/157] Loss_G: 0.0142\n",
            "PRETRAIN [1/25][30/157] Loss_G: 0.0162\n",
            "PRETRAIN [1/25][31/157] Loss_G: 0.0170\n",
            "PRETRAIN [1/25][32/157] Loss_G: 0.0146\n",
            "PRETRAIN [1/25][33/157] Loss_G: 0.0159\n",
            "PRETRAIN [1/25][34/157] Loss_G: 0.0188\n",
            "PRETRAIN [1/25][35/157] Loss_G: 0.0219\n",
            "PRETRAIN [1/25][36/157] Loss_G: 0.0239\n",
            "PRETRAIN [1/25][37/157] Loss_G: 0.0187\n",
            "PRETRAIN [1/25][38/157] Loss_G: 0.0163\n",
            "PRETRAIN [1/25][39/157] Loss_G: 0.0176\n",
            "PRETRAIN [1/25][40/157] Loss_G: 0.0163\n",
            "PRETRAIN [1/25][41/157] Loss_G: 0.0182\n",
            "PRETRAIN [1/25][42/157] Loss_G: 0.0176\n",
            "PRETRAIN [1/25][43/157] Loss_G: 0.0142\n",
            "PRETRAIN [1/25][44/157] Loss_G: 0.0151\n",
            "PRETRAIN [1/25][45/157] Loss_G: 0.0151\n",
            "PRETRAIN [1/25][46/157] Loss_G: 0.0164\n",
            "PRETRAIN [1/25][47/157] Loss_G: 0.0171\n",
            "PRETRAIN [1/25][48/157] Loss_G: 0.0162\n",
            "PRETRAIN [1/25][49/157] Loss_G: 0.0177\n",
            "PRETRAIN [1/25][50/157] Loss_G: 0.0151\n",
            "PRETRAIN [1/25][51/157] Loss_G: 0.0151\n",
            "PRETRAIN [1/25][52/157] Loss_G: 0.0164\n",
            "PRETRAIN [1/25][53/157] Loss_G: 0.0144\n",
            "PRETRAIN [1/25][54/157] Loss_G: 0.0141\n",
            "PRETRAIN [1/25][55/157] Loss_G: 0.0144\n",
            "PRETRAIN [1/25][56/157] Loss_G: 0.0163\n",
            "PRETRAIN [1/25][57/157] Loss_G: 0.0153\n",
            "PRETRAIN [1/25][58/157] Loss_G: 0.0149\n",
            "PRETRAIN [1/25][59/157] Loss_G: 0.0135\n",
            "PRETRAIN [1/25][60/157] Loss_G: 0.0151\n",
            "PRETRAIN [1/25][61/157] Loss_G: 0.0139\n",
            "PRETRAIN [1/25][62/157] Loss_G: 0.0122\n",
            "PRETRAIN [1/25][63/157] Loss_G: 0.0140\n",
            "PRETRAIN [1/25][64/157] Loss_G: 0.0151\n",
            "PRETRAIN [1/25][65/157] Loss_G: 0.0139\n",
            "PRETRAIN [1/25][66/157] Loss_G: 0.0138\n",
            "PRETRAIN [1/25][67/157] Loss_G: 0.0131\n",
            "PRETRAIN [1/25][68/157] Loss_G: 0.0140\n",
            "PRETRAIN [1/25][69/157] Loss_G: 0.0177\n",
            "PRETRAIN [1/25][70/157] Loss_G: 0.0175\n",
            "PRETRAIN [1/25][71/157] Loss_G: 0.0147\n",
            "PRETRAIN [1/25][72/157] Loss_G: 0.0160\n",
            "PRETRAIN [1/25][73/157] Loss_G: 0.0140\n",
            "PRETRAIN [1/25][74/157] Loss_G: 0.0167\n",
            "PRETRAIN [1/25][75/157] Loss_G: 0.0164\n",
            "PRETRAIN [1/25][76/157] Loss_G: 0.0150\n",
            "PRETRAIN [1/25][77/157] Loss_G: 0.0160\n",
            "PRETRAIN [1/25][78/157] Loss_G: 0.0143\n",
            "PRETRAIN [1/25][79/157] Loss_G: 0.0141\n",
            "PRETRAIN [1/25][80/157] Loss_G: 0.0135\n",
            "PRETRAIN [1/25][81/157] Loss_G: 0.0190\n",
            "PRETRAIN [1/25][82/157] Loss_G: 0.0214\n",
            "PRETRAIN [1/25][83/157] Loss_G: 0.0189\n",
            "PRETRAIN [1/25][84/157] Loss_G: 0.0159\n",
            "PRETRAIN [1/25][85/157] Loss_G: 0.0177\n",
            "PRETRAIN [1/25][86/157] Loss_G: 0.0160\n",
            "PRETRAIN [1/25][87/157] Loss_G: 0.0161\n",
            "PRETRAIN [1/25][88/157] Loss_G: 0.0177\n",
            "PRETRAIN [1/25][89/157] Loss_G: 0.0159\n",
            "PRETRAIN [1/25][90/157] Loss_G: 0.0135\n",
            "PRETRAIN [1/25][91/157] Loss_G: 0.0124\n",
            "PRETRAIN [1/25][92/157] Loss_G: 0.0154\n",
            "PRETRAIN [1/25][93/157] Loss_G: 0.0155\n",
            "PRETRAIN [1/25][94/157] Loss_G: 0.0129\n",
            "PRETRAIN [1/25][95/157] Loss_G: 0.0123\n",
            "PRETRAIN [1/25][96/157] Loss_G: 0.0127\n",
            "PRETRAIN [1/25][97/157] Loss_G: 0.0153\n",
            "PRETRAIN [1/25][98/157] Loss_G: 0.0175\n",
            "PRETRAIN [1/25][99/157] Loss_G: 0.0174\n",
            "PRETRAIN [1/25][100/157] Loss_G: 0.0145\n",
            "PRETRAIN [1/25][101/157] Loss_G: 0.0140\n",
            "PRETRAIN [1/25][102/157] Loss_G: 0.0138\n",
            "PRETRAIN [1/25][103/157] Loss_G: 0.0134\n",
            "PRETRAIN [1/25][104/157] Loss_G: 0.0117\n",
            "PRETRAIN [1/25][105/157] Loss_G: 0.0129\n",
            "PRETRAIN [1/25][106/157] Loss_G: 0.0131\n",
            "PRETRAIN [1/25][107/157] Loss_G: 0.0140\n",
            "PRETRAIN [1/25][108/157] Loss_G: 0.0130\n",
            "PRETRAIN [1/25][109/157] Loss_G: 0.0146\n",
            "PRETRAIN [1/25][110/157] Loss_G: 0.0145\n",
            "PRETRAIN [1/25][111/157] Loss_G: 0.0148\n",
            "PRETRAIN [1/25][112/157] Loss_G: 0.0143\n",
            "PRETRAIN [1/25][113/157] Loss_G: 0.0133\n",
            "PRETRAIN [1/25][114/157] Loss_G: 0.0122\n",
            "PRETRAIN [1/25][115/157] Loss_G: 0.0134\n",
            "PRETRAIN [1/25][116/157] Loss_G: 0.0126\n",
            "PRETRAIN [1/25][117/157] Loss_G: 0.0126\n",
            "PRETRAIN [1/25][118/157] Loss_G: 0.0115\n",
            "PRETRAIN [1/25][119/157] Loss_G: 0.0106\n",
            "PRETRAIN [1/25][120/157] Loss_G: 0.0138\n",
            "PRETRAIN [1/25][121/157] Loss_G: 0.0138\n",
            "PRETRAIN [1/25][122/157] Loss_G: 0.0157\n",
            "PRETRAIN [1/25][123/157] Loss_G: 0.0147\n",
            "PRETRAIN [1/25][124/157] Loss_G: 0.0135\n",
            "PRETRAIN [1/25][125/157] Loss_G: 0.0137\n",
            "PRETRAIN [1/25][126/157] Loss_G: 0.0117\n",
            "PRETRAIN [1/25][127/157] Loss_G: 0.0125\n",
            "PRETRAIN [1/25][128/157] Loss_G: 0.0146\n",
            "PRETRAIN [1/25][129/157] Loss_G: 0.0126\n",
            "PRETRAIN [1/25][130/157] Loss_G: 0.0105\n",
            "PRETRAIN [1/25][131/157] Loss_G: 0.0133\n",
            "PRETRAIN [1/25][132/157] Loss_G: 0.0116\n",
            "PRETRAIN [1/25][133/157] Loss_G: 0.0132\n",
            "PRETRAIN [1/25][134/157] Loss_G: 0.0136\n",
            "PRETRAIN [1/25][135/157] Loss_G: 0.0135\n",
            "PRETRAIN [1/25][136/157] Loss_G: 0.0147\n",
            "PRETRAIN [1/25][137/157] Loss_G: 0.0154\n",
            "PRETRAIN [1/25][138/157] Loss_G: 0.0193\n",
            "PRETRAIN [1/25][139/157] Loss_G: 0.0170\n",
            "PRETRAIN [1/25][140/157] Loss_G: 0.0170\n",
            "PRETRAIN [1/25][141/157] Loss_G: 0.0162\n",
            "PRETRAIN [1/25][142/157] Loss_G: 0.0157\n",
            "PRETRAIN [1/25][143/157] Loss_G: 0.0143\n",
            "PRETRAIN [1/25][144/157] Loss_G: 0.0153\n",
            "PRETRAIN [1/25][145/157] Loss_G: 0.0134\n",
            "PRETRAIN [1/25][146/157] Loss_G: 0.0162\n",
            "PRETRAIN [1/25][147/157] Loss_G: 0.0163\n",
            "PRETRAIN [1/25][148/157] Loss_G: 0.0168\n",
            "PRETRAIN [1/25][149/157] Loss_G: 0.0185\n",
            "PRETRAIN [1/25][150/157] Loss_G: 0.0229\n",
            "PRETRAIN [1/25][151/157] Loss_G: 0.0597\n",
            "PRETRAIN [1/25][152/157] Loss_G: 0.1500\n",
            "PRETRAIN [1/25][153/157] Loss_G: 0.1946\n",
            "PRETRAIN [1/25][154/157] Loss_G: 0.2518\n",
            "PRETRAIN [1/25][155/157] Loss_G: 0.2213\n",
            "PRETRAIN [1/25][156/157] Loss_G: 0.1980\n",
            "Time elapsed Epoch 1: 116 seconds\n",
            "PRETRAIN [2/25][0/157] Loss_G: 0.1553\n",
            "PRETRAIN [2/25][1/157] Loss_G: 0.1494\n",
            "PRETRAIN [2/25][2/157] Loss_G: 0.1228\n",
            "PRETRAIN [2/25][3/157] Loss_G: 0.1145\n",
            "PRETRAIN [2/25][4/157] Loss_G: 0.1114\n",
            "PRETRAIN [2/25][5/157] Loss_G: 0.1002\n",
            "PRETRAIN [2/25][6/157] Loss_G: 0.0826\n",
            "PRETRAIN [2/25][7/157] Loss_G: 0.0845\n",
            "PRETRAIN [2/25][8/157] Loss_G: 0.0837\n",
            "PRETRAIN [2/25][9/157] Loss_G: 0.0836\n",
            "PRETRAIN [2/25][10/157] Loss_G: 0.0693\n",
            "PRETRAIN [2/25][11/157] Loss_G: 0.0587\n",
            "PRETRAIN [2/25][12/157] Loss_G: 0.0669\n",
            "PRETRAIN [2/25][13/157] Loss_G: 0.0532\n",
            "PRETRAIN [2/25][14/157] Loss_G: 0.0496\n",
            "PRETRAIN [2/25][15/157] Loss_G: 0.0502\n",
            "PRETRAIN [2/25][16/157] Loss_G: 0.0598\n",
            "PRETRAIN [2/25][17/157] Loss_G: 0.0746\n",
            "PRETRAIN [2/25][18/157] Loss_G: 0.0678\n",
            "PRETRAIN [2/25][19/157] Loss_G: 0.0523\n",
            "PRETRAIN [2/25][20/157] Loss_G: 0.0446\n",
            "PRETRAIN [2/25][21/157] Loss_G: 0.0430\n",
            "PRETRAIN [2/25][22/157] Loss_G: 0.0434\n",
            "PRETRAIN [2/25][23/157] Loss_G: 0.0375\n",
            "PRETRAIN [2/25][24/157] Loss_G: 0.0334\n",
            "PRETRAIN [2/25][25/157] Loss_G: 0.0401\n",
            "PRETRAIN [2/25][26/157] Loss_G: 0.0372\n",
            "PRETRAIN [2/25][27/157] Loss_G: 0.0349\n",
            "PRETRAIN [2/25][28/157] Loss_G: 0.0295\n",
            "PRETRAIN [2/25][29/157] Loss_G: 0.0326\n",
            "PRETRAIN [2/25][30/157] Loss_G: 0.0295\n",
            "PRETRAIN [2/25][31/157] Loss_G: 0.0276\n",
            "PRETRAIN [2/25][32/157] Loss_G: 0.0288\n",
            "PRETRAIN [2/25][33/157] Loss_G: 0.0282\n",
            "PRETRAIN [2/25][34/157] Loss_G: 0.0270\n",
            "PRETRAIN [2/25][35/157] Loss_G: 0.0300\n",
            "PRETRAIN [2/25][36/157] Loss_G: 0.0365\n",
            "PRETRAIN [2/25][37/157] Loss_G: 0.0363\n",
            "PRETRAIN [2/25][38/157] Loss_G: 0.0328\n",
            "PRETRAIN [2/25][39/157] Loss_G: 0.0276\n",
            "PRETRAIN [2/25][40/157] Loss_G: 0.0268\n",
            "PRETRAIN [2/25][41/157] Loss_G: 0.0223\n",
            "PRETRAIN [2/25][42/157] Loss_G: 0.0234\n",
            "PRETRAIN [2/25][43/157] Loss_G: 0.0222\n",
            "PRETRAIN [2/25][44/157] Loss_G: 0.0238\n",
            "PRETRAIN [2/25][45/157] Loss_G: 0.0267\n",
            "PRETRAIN [2/25][46/157] Loss_G: 0.0229\n",
            "PRETRAIN [2/25][47/157] Loss_G: 0.0254\n",
            "PRETRAIN [2/25][48/157] Loss_G: 0.0223\n",
            "PRETRAIN [2/25][49/157] Loss_G: 0.0222\n",
            "PRETRAIN [2/25][50/157] Loss_G: 0.0231\n",
            "PRETRAIN [2/25][51/157] Loss_G: 0.0185\n",
            "PRETRAIN [2/25][52/157] Loss_G: 0.0247\n",
            "PRETRAIN [2/25][53/157] Loss_G: 0.0246\n",
            "PRETRAIN [2/25][54/157] Loss_G: 0.0240\n",
            "PRETRAIN [2/25][55/157] Loss_G: 0.0221\n",
            "PRETRAIN [2/25][56/157] Loss_G: 0.0200\n",
            "PRETRAIN [2/25][57/157] Loss_G: 0.0172\n",
            "PRETRAIN [2/25][58/157] Loss_G: 0.0188\n",
            "PRETRAIN [2/25][59/157] Loss_G: 0.0196\n",
            "PRETRAIN [2/25][60/157] Loss_G: 0.0187\n",
            "PRETRAIN [2/25][61/157] Loss_G: 0.0189\n",
            "PRETRAIN [2/25][62/157] Loss_G: 0.0202\n",
            "PRETRAIN [2/25][63/157] Loss_G: 0.0216\n",
            "PRETRAIN [2/25][64/157] Loss_G: 0.0193\n",
            "PRETRAIN [2/25][65/157] Loss_G: 0.0196\n",
            "PRETRAIN [2/25][66/157] Loss_G: 0.0203\n",
            "PRETRAIN [2/25][67/157] Loss_G: 0.0217\n",
            "PRETRAIN [2/25][68/157] Loss_G: 0.0228\n",
            "PRETRAIN [2/25][69/157] Loss_G: 0.0220\n",
            "PRETRAIN [2/25][70/157] Loss_G: 0.0202\n",
            "PRETRAIN [2/25][71/157] Loss_G: 0.0189\n",
            "PRETRAIN [2/25][72/157] Loss_G: 0.0187\n",
            "PRETRAIN [2/25][73/157] Loss_G: 0.0166\n",
            "PRETRAIN [2/25][74/157] Loss_G: 0.0162\n",
            "PRETRAIN [2/25][75/157] Loss_G: 0.0184\n",
            "PRETRAIN [2/25][76/157] Loss_G: 0.0165\n",
            "PRETRAIN [2/25][77/157] Loss_G: 0.0165\n",
            "PRETRAIN [2/25][78/157] Loss_G: 0.0170\n",
            "PRETRAIN [2/25][79/157] Loss_G: 0.0201\n",
            "PRETRAIN [2/25][80/157] Loss_G: 0.0163\n",
            "PRETRAIN [2/25][81/157] Loss_G: 0.0236\n",
            "PRETRAIN [2/25][82/157] Loss_G: 0.0179\n",
            "PRETRAIN [2/25][83/157] Loss_G: 0.0217\n",
            "PRETRAIN [2/25][84/157] Loss_G: 0.0231\n",
            "PRETRAIN [2/25][85/157] Loss_G: 0.0216\n",
            "PRETRAIN [2/25][86/157] Loss_G: 0.0191\n",
            "PRETRAIN [2/25][87/157] Loss_G: 0.0184\n",
            "PRETRAIN [2/25][88/157] Loss_G: 0.0197\n",
            "PRETRAIN [2/25][89/157] Loss_G: 0.0158\n",
            "PRETRAIN [2/25][90/157] Loss_G: 0.0147\n",
            "PRETRAIN [2/25][91/157] Loss_G: 0.0184\n",
            "PRETRAIN [2/25][92/157] Loss_G: 0.0174\n",
            "PRETRAIN [2/25][93/157] Loss_G: 0.0172\n",
            "PRETRAIN [2/25][94/157] Loss_G: 0.0185\n",
            "PRETRAIN [2/25][95/157] Loss_G: 0.0164\n",
            "PRETRAIN [2/25][96/157] Loss_G: 0.0152\n",
            "PRETRAIN [2/25][97/157] Loss_G: 0.0150\n",
            "PRETRAIN [2/25][98/157] Loss_G: 0.0143\n",
            "PRETRAIN [2/25][99/157] Loss_G: 0.0156\n",
            "PRETRAIN [2/25][100/157] Loss_G: 0.0151\n",
            "PRETRAIN [2/25][101/157] Loss_G: 0.0158\n",
            "PRETRAIN [2/25][102/157] Loss_G: 0.0136\n",
            "PRETRAIN [2/25][103/157] Loss_G: 0.0138\n",
            "PRETRAIN [2/25][104/157] Loss_G: 0.0152\n",
            "PRETRAIN [2/25][105/157] Loss_G: 0.0134\n",
            "PRETRAIN [2/25][106/157] Loss_G: 0.0175\n",
            "PRETRAIN [2/25][107/157] Loss_G: 0.0158\n",
            "PRETRAIN [2/25][108/157] Loss_G: 0.0155\n",
            "PRETRAIN [2/25][109/157] Loss_G: 0.0170\n",
            "PRETRAIN [2/25][110/157] Loss_G: 0.0180\n",
            "PRETRAIN [2/25][111/157] Loss_G: 0.0167\n",
            "PRETRAIN [2/25][112/157] Loss_G: 0.0154\n",
            "PRETRAIN [2/25][113/157] Loss_G: 0.0184\n",
            "PRETRAIN [2/25][114/157] Loss_G: 0.0147\n",
            "PRETRAIN [2/25][115/157] Loss_G: 0.0141\n",
            "PRETRAIN [2/25][116/157] Loss_G: 0.0150\n",
            "PRETRAIN [2/25][117/157] Loss_G: 0.0166\n",
            "PRETRAIN [2/25][118/157] Loss_G: 0.0160\n",
            "PRETRAIN [2/25][119/157] Loss_G: 0.0145\n",
            "PRETRAIN [2/25][120/157] Loss_G: 0.0149\n",
            "PRETRAIN [2/25][121/157] Loss_G: 0.0138\n",
            "PRETRAIN [2/25][122/157] Loss_G: 0.0170\n",
            "PRETRAIN [2/25][123/157] Loss_G: 0.0160\n",
            "PRETRAIN [2/25][124/157] Loss_G: 0.0202\n",
            "PRETRAIN [2/25][125/157] Loss_G: 0.0239\n",
            "PRETRAIN [2/25][126/157] Loss_G: 0.0185\n",
            "PRETRAIN [2/25][127/157] Loss_G: 0.0162\n",
            "PRETRAIN [2/25][128/157] Loss_G: 0.0168\n",
            "PRETRAIN [2/25][129/157] Loss_G: 0.0152\n",
            "PRETRAIN [2/25][130/157] Loss_G: 0.0133\n",
            "PRETRAIN [2/25][131/157] Loss_G: 0.0124\n",
            "PRETRAIN [2/25][132/157] Loss_G: 0.0157\n",
            "PRETRAIN [2/25][133/157] Loss_G: 0.0128\n",
            "PRETRAIN [2/25][134/157] Loss_G: 0.0151\n",
            "PRETRAIN [2/25][135/157] Loss_G: 0.0152\n",
            "PRETRAIN [2/25][136/157] Loss_G: 0.0150\n",
            "PRETRAIN [2/25][137/157] Loss_G: 0.0155\n",
            "PRETRAIN [2/25][138/157] Loss_G: 0.0135\n",
            "PRETRAIN [2/25][139/157] Loss_G: 0.0134\n",
            "PRETRAIN [2/25][140/157] Loss_G: 0.0138\n",
            "PRETRAIN [2/25][141/157] Loss_G: 0.0139\n",
            "PRETRAIN [2/25][142/157] Loss_G: 0.0123\n",
            "PRETRAIN [2/25][143/157] Loss_G: 0.0160\n",
            "PRETRAIN [2/25][144/157] Loss_G: 0.0182\n",
            "PRETRAIN [2/25][145/157] Loss_G: 0.0167\n",
            "PRETRAIN [2/25][146/157] Loss_G: 0.0136\n",
            "PRETRAIN [2/25][147/157] Loss_G: 0.0133\n",
            "PRETRAIN [2/25][148/157] Loss_G: 0.0150\n",
            "PRETRAIN [2/25][149/157] Loss_G: 0.0156\n",
            "PRETRAIN [2/25][150/157] Loss_G: 0.0147\n",
            "PRETRAIN [2/25][151/157] Loss_G: 0.0148\n",
            "PRETRAIN [2/25][152/157] Loss_G: 0.0317\n",
            "PRETRAIN [2/25][153/157] Loss_G: 0.0331\n",
            "PRETRAIN [2/25][154/157] Loss_G: 0.0321\n",
            "PRETRAIN [2/25][155/157] Loss_G: 0.0364\n",
            "PRETRAIN [2/25][156/157] Loss_G: 0.0392\n",
            "Time elapsed Epoch 2: 115 seconds\n",
            "PRETRAIN [3/25][0/157] Loss_G: 0.0290\n",
            "PRETRAIN [3/25][1/157] Loss_G: 0.0248\n",
            "PRETRAIN [3/25][2/157] Loss_G: 0.0246\n",
            "PRETRAIN [3/25][3/157] Loss_G: 0.0207\n",
            "PRETRAIN [3/25][4/157] Loss_G: 0.0193\n",
            "PRETRAIN [3/25][5/157] Loss_G: 0.0157\n",
            "PRETRAIN [3/25][6/157] Loss_G: 0.0188\n",
            "PRETRAIN [3/25][7/157] Loss_G: 0.0170\n",
            "PRETRAIN [3/25][8/157] Loss_G: 0.0165\n",
            "PRETRAIN [3/25][9/157] Loss_G: 0.0160\n",
            "PRETRAIN [3/25][10/157] Loss_G: 0.0144\n",
            "PRETRAIN [3/25][11/157] Loss_G: 0.0168\n",
            "PRETRAIN [3/25][12/157] Loss_G: 0.0153\n",
            "PRETRAIN [3/25][13/157] Loss_G: 0.0153\n",
            "PRETRAIN [3/25][14/157] Loss_G: 0.0146\n",
            "PRETRAIN [3/25][15/157] Loss_G: 0.0152\n",
            "PRETRAIN [3/25][16/157] Loss_G: 0.0145\n",
            "PRETRAIN [3/25][17/157] Loss_G: 0.0149\n",
            "PRETRAIN [3/25][18/157] Loss_G: 0.0132\n",
            "PRETRAIN [3/25][19/157] Loss_G: 0.0173\n",
            "PRETRAIN [3/25][20/157] Loss_G: 0.0147\n",
            "PRETRAIN [3/25][21/157] Loss_G: 0.0149\n",
            "PRETRAIN [3/25][22/157] Loss_G: 0.0139\n",
            "PRETRAIN [3/25][23/157] Loss_G: 0.0141\n",
            "PRETRAIN [3/25][24/157] Loss_G: 0.0148\n",
            "PRETRAIN [3/25][25/157] Loss_G: 0.0138\n",
            "PRETRAIN [3/25][26/157] Loss_G: 0.0130\n",
            "PRETRAIN [3/25][27/157] Loss_G: 0.0152\n",
            "PRETRAIN [3/25][28/157] Loss_G: 0.0152\n",
            "PRETRAIN [3/25][29/157] Loss_G: 0.0137\n",
            "PRETRAIN [3/25][30/157] Loss_G: 0.0142\n",
            "PRETRAIN [3/25][31/157] Loss_G: 0.0126\n",
            "PRETRAIN [3/25][32/157] Loss_G: 0.0135\n",
            "PRETRAIN [3/25][33/157] Loss_G: 0.0118\n",
            "PRETRAIN [3/25][34/157] Loss_G: 0.0111\n",
            "PRETRAIN [3/25][35/157] Loss_G: 0.0156\n",
            "PRETRAIN [3/25][36/157] Loss_G: 0.0128\n",
            "PRETRAIN [3/25][37/157] Loss_G: 0.0150\n",
            "PRETRAIN [3/25][38/157] Loss_G: 0.0124\n",
            "PRETRAIN [3/25][39/157] Loss_G: 0.0144\n",
            "PRETRAIN [3/25][40/157] Loss_G: 0.0155\n",
            "PRETRAIN [3/25][41/157] Loss_G: 0.0147\n",
            "PRETRAIN [3/25][42/157] Loss_G: 0.0147\n",
            "PRETRAIN [3/25][43/157] Loss_G: 0.0133\n",
            "PRETRAIN [3/25][44/157] Loss_G: 0.0134\n",
            "PRETRAIN [3/25][45/157] Loss_G: 0.0125\n",
            "PRETRAIN [3/25][46/157] Loss_G: 0.0127\n",
            "PRETRAIN [3/25][47/157] Loss_G: 0.0119\n",
            "PRETRAIN [3/25][48/157] Loss_G: 0.0135\n",
            "PRETRAIN [3/25][49/157] Loss_G: 0.0134\n",
            "PRETRAIN [3/25][50/157] Loss_G: 0.0169\n",
            "PRETRAIN [3/25][51/157] Loss_G: 0.0160\n",
            "PRETRAIN [3/25][52/157] Loss_G: 0.0162\n",
            "PRETRAIN [3/25][53/157] Loss_G: 0.0164\n",
            "PRETRAIN [3/25][54/157] Loss_G: 0.0167\n",
            "PRETRAIN [3/25][55/157] Loss_G: 0.0175\n",
            "PRETRAIN [3/25][56/157] Loss_G: 0.0188\n",
            "PRETRAIN [3/25][57/157] Loss_G: 0.0146\n",
            "PRETRAIN [3/25][58/157] Loss_G: 0.0128\n",
            "PRETRAIN [3/25][59/157] Loss_G: 0.0153\n",
            "PRETRAIN [3/25][60/157] Loss_G: 0.0155\n",
            "PRETRAIN [3/25][61/157] Loss_G: 0.0120\n",
            "PRETRAIN [3/25][62/157] Loss_G: 0.0130\n",
            "PRETRAIN [3/25][63/157] Loss_G: 0.0130\n",
            "PRETRAIN [3/25][64/157] Loss_G: 0.0147\n",
            "PRETRAIN [3/25][65/157] Loss_G: 0.0127\n",
            "PRETRAIN [3/25][66/157] Loss_G: 0.0125\n",
            "PRETRAIN [3/25][67/157] Loss_G: 0.0139\n",
            "PRETRAIN [3/25][68/157] Loss_G: 0.0126\n",
            "PRETRAIN [3/25][69/157] Loss_G: 0.0131\n",
            "PRETRAIN [3/25][70/157] Loss_G: 0.0123\n",
            "PRETRAIN [3/25][71/157] Loss_G: 0.0145\n",
            "PRETRAIN [3/25][72/157] Loss_G: 0.0146\n",
            "PRETRAIN [3/25][73/157] Loss_G: 0.0169\n",
            "PRETRAIN [3/25][74/157] Loss_G: 0.0120\n",
            "PRETRAIN [3/25][75/157] Loss_G: 0.0142\n",
            "PRETRAIN [3/25][76/157] Loss_G: 0.0115\n",
            "PRETRAIN [3/25][77/157] Loss_G: 0.0122\n",
            "PRETRAIN [3/25][78/157] Loss_G: 0.0125\n",
            "PRETRAIN [3/25][79/157] Loss_G: 0.0133\n",
            "PRETRAIN [3/25][80/157] Loss_G: 0.0136\n",
            "PRETRAIN [3/25][81/157] Loss_G: 0.0132\n",
            "PRETRAIN [3/25][82/157] Loss_G: 0.0112\n",
            "PRETRAIN [3/25][83/157] Loss_G: 0.0127\n",
            "PRETRAIN [3/25][84/157] Loss_G: 0.0150\n",
            "PRETRAIN [3/25][85/157] Loss_G: 0.0138\n",
            "PRETRAIN [3/25][86/157] Loss_G: 0.0147\n",
            "PRETRAIN [3/25][87/157] Loss_G: 0.0126\n",
            "PRETRAIN [3/25][88/157] Loss_G: 0.0122\n",
            "PRETRAIN [3/25][89/157] Loss_G: 0.0155\n",
            "PRETRAIN [3/25][90/157] Loss_G: 0.0146\n",
            "PRETRAIN [3/25][91/157] Loss_G: 0.0117\n",
            "PRETRAIN [3/25][92/157] Loss_G: 0.0125\n",
            "PRETRAIN [3/25][93/157] Loss_G: 0.0213\n",
            "PRETRAIN [3/25][94/157] Loss_G: 0.0244\n",
            "PRETRAIN [3/25][95/157] Loss_G: 0.0190\n",
            "PRETRAIN [3/25][96/157] Loss_G: 0.0173\n",
            "PRETRAIN [3/25][97/157] Loss_G: 0.0154\n",
            "PRETRAIN [3/25][98/157] Loss_G: 0.0139\n",
            "PRETRAIN [3/25][99/157] Loss_G: 0.0144\n",
            "PRETRAIN [3/25][100/157] Loss_G: 0.0154\n",
            "PRETRAIN [3/25][101/157] Loss_G: 0.0134\n",
            "PRETRAIN [3/25][102/157] Loss_G: 0.0147\n",
            "PRETRAIN [3/25][103/157] Loss_G: 0.0142\n",
            "PRETRAIN [3/25][104/157] Loss_G: 0.0145\n",
            "PRETRAIN [3/25][105/157] Loss_G: 0.0124\n",
            "PRETRAIN [3/25][106/157] Loss_G: 0.0135\n",
            "PRETRAIN [3/25][107/157] Loss_G: 0.0127\n",
            "PRETRAIN [3/25][108/157] Loss_G: 0.0138\n",
            "PRETRAIN [3/25][109/157] Loss_G: 0.0145\n",
            "PRETRAIN [3/25][110/157] Loss_G: 0.0131\n",
            "PRETRAIN [3/25][111/157] Loss_G: 0.0136\n",
            "PRETRAIN [3/25][112/157] Loss_G: 0.0122\n",
            "PRETRAIN [3/25][113/157] Loss_G: 0.0147\n",
            "PRETRAIN [3/25][114/157] Loss_G: 0.0180\n",
            "PRETRAIN [3/25][115/157] Loss_G: 0.0162\n",
            "PRETRAIN [3/25][116/157] Loss_G: 0.0124\n",
            "PRETRAIN [3/25][117/157] Loss_G: 0.0124\n",
            "PRETRAIN [3/25][118/157] Loss_G: 0.0139\n",
            "PRETRAIN [3/25][119/157] Loss_G: 0.0150\n",
            "PRETRAIN [3/25][120/157] Loss_G: 0.0122\n",
            "PRETRAIN [3/25][121/157] Loss_G: 0.0128\n",
            "PRETRAIN [3/25][122/157] Loss_G: 0.0149\n",
            "PRETRAIN [3/25][123/157] Loss_G: 0.0127\n",
            "PRETRAIN [3/25][124/157] Loss_G: 0.0143\n",
            "PRETRAIN [3/25][125/157] Loss_G: 0.0123\n",
            "PRETRAIN [3/25][126/157] Loss_G: 0.0122\n",
            "PRETRAIN [3/25][127/157] Loss_G: 0.0128\n",
            "PRETRAIN [3/25][128/157] Loss_G: 0.0135\n",
            "PRETRAIN [3/25][129/157] Loss_G: 0.0116\n",
            "PRETRAIN [3/25][130/157] Loss_G: 0.0130\n",
            "PRETRAIN [3/25][131/157] Loss_G: 0.0124\n",
            "PRETRAIN [3/25][132/157] Loss_G: 0.0122\n",
            "PRETRAIN [3/25][133/157] Loss_G: 0.0130\n",
            "PRETRAIN [3/25][134/157] Loss_G: 0.0143\n",
            "PRETRAIN [3/25][135/157] Loss_G: 0.0123\n",
            "PRETRAIN [3/25][136/157] Loss_G: 0.0140\n",
            "PRETRAIN [3/25][137/157] Loss_G: 0.0125\n",
            "PRETRAIN [3/25][138/157] Loss_G: 0.0139\n",
            "PRETRAIN [3/25][139/157] Loss_G: 0.0132\n",
            "PRETRAIN [3/25][140/157] Loss_G: 0.0128\n",
            "PRETRAIN [3/25][141/157] Loss_G: 0.0126\n",
            "PRETRAIN [3/25][142/157] Loss_G: 0.0120\n",
            "PRETRAIN [3/25][143/157] Loss_G: 0.0110\n",
            "PRETRAIN [3/25][144/157] Loss_G: 0.0163\n",
            "PRETRAIN [3/25][145/157] Loss_G: 0.0132\n",
            "PRETRAIN [3/25][146/157] Loss_G: 0.0129\n",
            "PRETRAIN [3/25][147/157] Loss_G: 0.0119\n",
            "PRETRAIN [3/25][148/157] Loss_G: 0.0126\n",
            "PRETRAIN [3/25][149/157] Loss_G: 0.0118\n",
            "PRETRAIN [3/25][150/157] Loss_G: 0.0130\n",
            "PRETRAIN [3/25][151/157] Loss_G: 0.0128\n",
            "PRETRAIN [3/25][152/157] Loss_G: 0.0135\n",
            "PRETRAIN [3/25][153/157] Loss_G: 0.0209\n",
            "PRETRAIN [3/25][154/157] Loss_G: 0.0165\n",
            "PRETRAIN [3/25][155/157] Loss_G: 0.0144\n",
            "PRETRAIN [3/25][156/157] Loss_G: 0.0135\n",
            "Time elapsed Epoch 3: 115 seconds\n",
            "PRETRAIN [4/25][0/157] Loss_G: 0.0148\n",
            "PRETRAIN [4/25][1/157] Loss_G: 0.0155\n",
            "PRETRAIN [4/25][2/157] Loss_G: 0.0133\n",
            "PRETRAIN [4/25][3/157] Loss_G: 0.0146\n",
            "PRETRAIN [4/25][4/157] Loss_G: 0.0141\n",
            "PRETRAIN [4/25][5/157] Loss_G: 0.0149\n",
            "PRETRAIN [4/25][6/157] Loss_G: 0.0149\n",
            "PRETRAIN [4/25][7/157] Loss_G: 0.0133\n",
            "PRETRAIN [4/25][8/157] Loss_G: 0.0162\n",
            "PRETRAIN [4/25][9/157] Loss_G: 0.0124\n",
            "PRETRAIN [4/25][10/157] Loss_G: 0.0169\n",
            "PRETRAIN [4/25][11/157] Loss_G: 0.0127\n",
            "PRETRAIN [4/25][12/157] Loss_G: 0.0149\n",
            "PRETRAIN [4/25][13/157] Loss_G: 0.0181\n",
            "PRETRAIN [4/25][14/157] Loss_G: 0.0135\n",
            "PRETRAIN [4/25][15/157] Loss_G: 0.0144\n",
            "PRETRAIN [4/25][16/157] Loss_G: 0.0143\n",
            "PRETRAIN [4/25][17/157] Loss_G: 0.0168\n",
            "PRETRAIN [4/25][18/157] Loss_G: 0.0134\n",
            "PRETRAIN [4/25][19/157] Loss_G: 0.0116\n",
            "PRETRAIN [4/25][20/157] Loss_G: 0.0107\n",
            "PRETRAIN [4/25][21/157] Loss_G: 0.0103\n",
            "PRETRAIN [4/25][22/157] Loss_G: 0.0128\n",
            "PRETRAIN [4/25][23/157] Loss_G: 0.0108\n",
            "PRETRAIN [4/25][24/157] Loss_G: 0.0113\n",
            "PRETRAIN [4/25][25/157] Loss_G: 0.0104\n",
            "PRETRAIN [4/25][26/157] Loss_G: 0.0122\n",
            "PRETRAIN [4/25][27/157] Loss_G: 0.0119\n",
            "PRETRAIN [4/25][28/157] Loss_G: 0.0124\n",
            "PRETRAIN [4/25][29/157] Loss_G: 0.0129\n",
            "PRETRAIN [4/25][30/157] Loss_G: 0.0127\n",
            "PRETRAIN [4/25][31/157] Loss_G: 0.0121\n",
            "PRETRAIN [4/25][32/157] Loss_G: 0.0123\n",
            "PRETRAIN [4/25][33/157] Loss_G: 0.0131\n",
            "PRETRAIN [4/25][34/157] Loss_G: 0.0114\n",
            "PRETRAIN [4/25][35/157] Loss_G: 0.0118\n",
            "PRETRAIN [4/25][36/157] Loss_G: 0.0136\n",
            "PRETRAIN [4/25][37/157] Loss_G: 0.0132\n",
            "PRETRAIN [4/25][38/157] Loss_G: 0.0144\n",
            "PRETRAIN [4/25][39/157] Loss_G: 0.0147\n",
            "PRETRAIN [4/25][40/157] Loss_G: 0.0167\n",
            "PRETRAIN [4/25][41/157] Loss_G: 0.0163\n",
            "PRETRAIN [4/25][42/157] Loss_G: 0.0115\n",
            "PRETRAIN [4/25][43/157] Loss_G: 0.0143\n",
            "PRETRAIN [4/25][44/157] Loss_G: 0.0125\n",
            "PRETRAIN [4/25][45/157] Loss_G: 0.0135\n",
            "PRETRAIN [4/25][46/157] Loss_G: 0.0125\n",
            "PRETRAIN [4/25][47/157] Loss_G: 0.0122\n",
            "PRETRAIN [4/25][48/157] Loss_G: 0.0114\n",
            "PRETRAIN [4/25][49/157] Loss_G: 0.0119\n",
            "PRETRAIN [4/25][50/157] Loss_G: 0.0133\n",
            "PRETRAIN [4/25][51/157] Loss_G: 0.0153\n",
            "PRETRAIN [4/25][52/157] Loss_G: 0.0134\n",
            "PRETRAIN [4/25][53/157] Loss_G: 0.0115\n",
            "PRETRAIN [4/25][54/157] Loss_G: 0.0108\n",
            "PRETRAIN [4/25][55/157] Loss_G: 0.0129\n",
            "PRETRAIN [4/25][56/157] Loss_G: 0.0127\n",
            "PRETRAIN [4/25][57/157] Loss_G: 0.0128\n",
            "PRETRAIN [4/25][58/157] Loss_G: 0.0142\n",
            "PRETRAIN [4/25][59/157] Loss_G: 0.0148\n",
            "PRETRAIN [4/25][60/157] Loss_G: 0.0154\n",
            "PRETRAIN [4/25][61/157] Loss_G: 0.0182\n",
            "PRETRAIN [4/25][62/157] Loss_G: 0.0206\n",
            "PRETRAIN [4/25][63/157] Loss_G: 0.0192\n",
            "PRETRAIN [4/25][64/157] Loss_G: 0.0141\n",
            "PRETRAIN [4/25][65/157] Loss_G: 0.0139\n",
            "PRETRAIN [4/25][66/157] Loss_G: 0.0148\n",
            "PRETRAIN [4/25][67/157] Loss_G: 0.0122\n",
            "PRETRAIN [4/25][68/157] Loss_G: 0.0114\n",
            "PRETRAIN [4/25][69/157] Loss_G: 0.0117\n",
            "PRETRAIN [4/25][70/157] Loss_G: 0.0140\n",
            "PRETRAIN [4/25][71/157] Loss_G: 0.0124\n",
            "PRETRAIN [4/25][72/157] Loss_G: 0.0107\n",
            "PRETRAIN [4/25][73/157] Loss_G: 0.0122\n",
            "PRETRAIN [4/25][74/157] Loss_G: 0.0156\n",
            "PRETRAIN [4/25][75/157] Loss_G: 0.0122\n",
            "PRETRAIN [4/25][76/157] Loss_G: 0.0132\n",
            "PRETRAIN [4/25][77/157] Loss_G: 0.0142\n",
            "PRETRAIN [4/25][78/157] Loss_G: 0.0137\n",
            "PRETRAIN [4/25][79/157] Loss_G: 0.0105\n",
            "PRETRAIN [4/25][80/157] Loss_G: 0.0129\n",
            "PRETRAIN [4/25][81/157] Loss_G: 0.0128\n",
            "PRETRAIN [4/25][82/157] Loss_G: 0.0126\n",
            "PRETRAIN [4/25][83/157] Loss_G: 0.0161\n",
            "PRETRAIN [4/25][84/157] Loss_G: 0.0143\n",
            "PRETRAIN [4/25][85/157] Loss_G: 0.0123\n",
            "PRETRAIN [4/25][86/157] Loss_G: 0.0145\n",
            "PRETRAIN [4/25][87/157] Loss_G: 0.0110\n",
            "PRETRAIN [4/25][88/157] Loss_G: 0.0136\n",
            "PRETRAIN [4/25][89/157] Loss_G: 0.0160\n",
            "PRETRAIN [4/25][90/157] Loss_G: 0.0138\n",
            "PRETRAIN [4/25][91/157] Loss_G: 0.0127\n",
            "PRETRAIN [4/25][92/157] Loss_G: 0.0109\n",
            "PRETRAIN [4/25][93/157] Loss_G: 0.0109\n",
            "PRETRAIN [4/25][94/157] Loss_G: 0.0120\n",
            "PRETRAIN [4/25][95/157] Loss_G: 0.0129\n",
            "PRETRAIN [4/25][96/157] Loss_G: 0.0103\n",
            "PRETRAIN [4/25][97/157] Loss_G: 0.0125\n",
            "PRETRAIN [4/25][98/157] Loss_G: 0.0100\n",
            "PRETRAIN [4/25][99/157] Loss_G: 0.0111\n",
            "PRETRAIN [4/25][100/157] Loss_G: 0.0111\n",
            "PRETRAIN [4/25][101/157] Loss_G: 0.0097\n",
            "PRETRAIN [4/25][102/157] Loss_G: 0.0110\n",
            "PRETRAIN [4/25][103/157] Loss_G: 0.0108\n",
            "PRETRAIN [4/25][104/157] Loss_G: 0.0103\n",
            "PRETRAIN [4/25][105/157] Loss_G: 0.0102\n",
            "PRETRAIN [4/25][106/157] Loss_G: 0.0108\n",
            "PRETRAIN [4/25][107/157] Loss_G: 0.0138\n",
            "PRETRAIN [4/25][108/157] Loss_G: 0.0165\n",
            "PRETRAIN [4/25][109/157] Loss_G: 0.0130\n",
            "PRETRAIN [4/25][110/157] Loss_G: 0.0140\n",
            "PRETRAIN [4/25][111/157] Loss_G: 0.0154\n",
            "PRETRAIN [4/25][112/157] Loss_G: 0.0134\n",
            "PRETRAIN [4/25][113/157] Loss_G: 0.0125\n",
            "PRETRAIN [4/25][114/157] Loss_G: 0.0133\n",
            "PRETRAIN [4/25][115/157] Loss_G: 0.0127\n",
            "PRETRAIN [4/25][116/157] Loss_G: 0.0111\n",
            "PRETRAIN [4/25][117/157] Loss_G: 0.0145\n",
            "PRETRAIN [4/25][118/157] Loss_G: 0.0114\n",
            "PRETRAIN [4/25][119/157] Loss_G: 0.0123\n",
            "PRETRAIN [4/25][120/157] Loss_G: 0.0124\n",
            "PRETRAIN [4/25][121/157] Loss_G: 0.0185\n",
            "PRETRAIN [4/25][122/157] Loss_G: 0.0177\n",
            "PRETRAIN [4/25][123/157] Loss_G: 0.0185\n",
            "PRETRAIN [4/25][124/157] Loss_G: 0.0173\n",
            "PRETRAIN [4/25][125/157] Loss_G: 0.0174\n",
            "PRETRAIN [4/25][126/157] Loss_G: 0.0181\n",
            "PRETRAIN [4/25][127/157] Loss_G: 0.0150\n",
            "PRETRAIN [4/25][128/157] Loss_G: 0.0142\n",
            "PRETRAIN [4/25][129/157] Loss_G: 0.0162\n",
            "PRETRAIN [4/25][130/157] Loss_G: 0.0141\n",
            "PRETRAIN [4/25][131/157] Loss_G: 0.0105\n",
            "PRETRAIN [4/25][132/157] Loss_G: 0.0131\n",
            "PRETRAIN [4/25][133/157] Loss_G: 0.0116\n",
            "PRETRAIN [4/25][134/157] Loss_G: 0.0123\n",
            "PRETRAIN [4/25][135/157] Loss_G: 0.0127\n",
            "PRETRAIN [4/25][136/157] Loss_G: 0.0119\n",
            "PRETRAIN [4/25][137/157] Loss_G: 0.0111\n",
            "PRETRAIN [4/25][138/157] Loss_G: 0.0129\n",
            "PRETRAIN [4/25][139/157] Loss_G: 0.0120\n",
            "PRETRAIN [4/25][140/157] Loss_G: 0.0125\n",
            "PRETRAIN [4/25][141/157] Loss_G: 0.0105\n",
            "PRETRAIN [4/25][142/157] Loss_G: 0.0140\n",
            "PRETRAIN [4/25][143/157] Loss_G: 0.0121\n",
            "PRETRAIN [4/25][144/157] Loss_G: 0.0142\n",
            "PRETRAIN [4/25][145/157] Loss_G: 0.0102\n",
            "PRETRAIN [4/25][146/157] Loss_G: 0.0127\n",
            "PRETRAIN [4/25][147/157] Loss_G: 0.0111\n",
            "PRETRAIN [4/25][148/157] Loss_G: 0.0106\n",
            "PRETRAIN [4/25][149/157] Loss_G: 0.0129\n",
            "PRETRAIN [4/25][150/157] Loss_G: 0.0103\n",
            "PRETRAIN [4/25][151/157] Loss_G: 0.0110\n",
            "PRETRAIN [4/25][152/157] Loss_G: 0.0158\n",
            "PRETRAIN [4/25][153/157] Loss_G: 0.0329\n",
            "PRETRAIN [4/25][154/157] Loss_G: 0.0413\n",
            "PRETRAIN [4/25][155/157] Loss_G: 0.0460\n",
            "PRETRAIN [4/25][156/157] Loss_G: 0.0342\n",
            "Time elapsed Epoch 4: 115 seconds\n",
            "PRETRAIN [5/25][0/157] Loss_G: 0.0398\n",
            "PRETRAIN [5/25][1/157] Loss_G: 0.0500\n",
            "PRETRAIN [5/25][2/157] Loss_G: 0.0327\n",
            "PRETRAIN [5/25][3/157] Loss_G: 0.0299\n",
            "PRETRAIN [5/25][4/157] Loss_G: 0.0263\n",
            "PRETRAIN [5/25][5/157] Loss_G: 0.0226\n",
            "PRETRAIN [5/25][6/157] Loss_G: 0.0203\n",
            "PRETRAIN [5/25][7/157] Loss_G: 0.0198\n",
            "PRETRAIN [5/25][8/157] Loss_G: 0.0220\n",
            "PRETRAIN [5/25][9/157] Loss_G: 0.0174\n",
            "PRETRAIN [5/25][10/157] Loss_G: 0.0183\n",
            "PRETRAIN [5/25][11/157] Loss_G: 0.0157\n",
            "PRETRAIN [5/25][12/157] Loss_G: 0.0144\n",
            "PRETRAIN [5/25][13/157] Loss_G: 0.0153\n",
            "PRETRAIN [5/25][14/157] Loss_G: 0.0145\n",
            "PRETRAIN [5/25][15/157] Loss_G: 0.0182\n",
            "PRETRAIN [5/25][16/157] Loss_G: 0.0157\n",
            "PRETRAIN [5/25][17/157] Loss_G: 0.0177\n",
            "PRETRAIN [5/25][18/157] Loss_G: 0.0152\n",
            "PRETRAIN [5/25][19/157] Loss_G: 0.0143\n",
            "PRETRAIN [5/25][20/157] Loss_G: 0.0179\n",
            "PRETRAIN [5/25][21/157] Loss_G: 0.0129\n",
            "PRETRAIN [5/25][22/157] Loss_G: 0.0166\n",
            "PRETRAIN [5/25][23/157] Loss_G: 0.0133\n",
            "PRETRAIN [5/25][24/157] Loss_G: 0.0178\n",
            "PRETRAIN [5/25][25/157] Loss_G: 0.0143\n",
            "PRETRAIN [5/25][26/157] Loss_G: 0.0138\n",
            "PRETRAIN [5/25][27/157] Loss_G: 0.0270\n",
            "PRETRAIN [5/25][28/157] Loss_G: 0.0280\n",
            "PRETRAIN [5/25][29/157] Loss_G: 0.0209\n",
            "PRETRAIN [5/25][30/157] Loss_G: 0.0186\n",
            "PRETRAIN [5/25][31/157] Loss_G: 0.0154\n",
            "PRETRAIN [5/25][32/157] Loss_G: 0.0155\n",
            "PRETRAIN [5/25][33/157] Loss_G: 0.0146\n",
            "PRETRAIN [5/25][34/157] Loss_G: 0.0145\n",
            "PRETRAIN [5/25][35/157] Loss_G: 0.0136\n",
            "PRETRAIN [5/25][36/157] Loss_G: 0.0132\n",
            "PRETRAIN [5/25][37/157] Loss_G: 0.0156\n",
            "PRETRAIN [5/25][38/157] Loss_G: 0.0150\n",
            "PRETRAIN [5/25][39/157] Loss_G: 0.0159\n",
            "PRETRAIN [5/25][40/157] Loss_G: 0.0139\n",
            "PRETRAIN [5/25][41/157] Loss_G: 0.0161\n",
            "PRETRAIN [5/25][42/157] Loss_G: 0.0120\n",
            "PRETRAIN [5/25][43/157] Loss_G: 0.0155\n",
            "PRETRAIN [5/25][44/157] Loss_G: 0.0119\n",
            "PRETRAIN [5/25][45/157] Loss_G: 0.0135\n",
            "PRETRAIN [5/25][46/157] Loss_G: 0.0145\n",
            "PRETRAIN [5/25][47/157] Loss_G: 0.0166\n",
            "PRETRAIN [5/25][48/157] Loss_G: 0.0122\n",
            "PRETRAIN [5/25][49/157] Loss_G: 0.0147\n",
            "PRETRAIN [5/25][50/157] Loss_G: 0.0120\n",
            "PRETRAIN [5/25][51/157] Loss_G: 0.0120\n",
            "PRETRAIN [5/25][52/157] Loss_G: 0.0141\n",
            "PRETRAIN [5/25][53/157] Loss_G: 0.0144\n",
            "PRETRAIN [5/25][54/157] Loss_G: 0.0127\n",
            "PRETRAIN [5/25][55/157] Loss_G: 0.0141\n",
            "PRETRAIN [5/25][56/157] Loss_G: 0.0130\n",
            "PRETRAIN [5/25][57/157] Loss_G: 0.0133\n",
            "PRETRAIN [5/25][58/157] Loss_G: 0.0107\n",
            "PRETRAIN [5/25][59/157] Loss_G: 0.0118\n",
            "PRETRAIN [5/25][60/157] Loss_G: 0.0118\n",
            "PRETRAIN [5/25][61/157] Loss_G: 0.0114\n",
            "PRETRAIN [5/25][62/157] Loss_G: 0.0123\n",
            "PRETRAIN [5/25][63/157] Loss_G: 0.0153\n",
            "PRETRAIN [5/25][64/157] Loss_G: 0.0142\n",
            "PRETRAIN [5/25][65/157] Loss_G: 0.0139\n",
            "PRETRAIN [5/25][66/157] Loss_G: 0.0131\n",
            "PRETRAIN [5/25][67/157] Loss_G: 0.0119\n",
            "PRETRAIN [5/25][68/157] Loss_G: 0.0111\n",
            "PRETRAIN [5/25][69/157] Loss_G: 0.0129\n",
            "PRETRAIN [5/25][70/157] Loss_G: 0.0133\n",
            "PRETRAIN [5/25][71/157] Loss_G: 0.0128\n",
            "PRETRAIN [5/25][72/157] Loss_G: 0.0116\n",
            "PRETRAIN [5/25][73/157] Loss_G: 0.0104\n",
            "PRETRAIN [5/25][74/157] Loss_G: 0.0128\n",
            "PRETRAIN [5/25][75/157] Loss_G: 0.0100\n",
            "PRETRAIN [5/25][76/157] Loss_G: 0.0123\n",
            "PRETRAIN [5/25][77/157] Loss_G: 0.0117\n",
            "PRETRAIN [5/25][78/157] Loss_G: 0.0117\n",
            "PRETRAIN [5/25][79/157] Loss_G: 0.0119\n",
            "PRETRAIN [5/25][80/157] Loss_G: 0.0115\n",
            "PRETRAIN [5/25][81/157] Loss_G: 0.0102\n",
            "PRETRAIN [5/25][82/157] Loss_G: 0.0132\n",
            "PRETRAIN [5/25][83/157] Loss_G: 0.0112\n",
            "PRETRAIN [5/25][84/157] Loss_G: 0.0128\n",
            "PRETRAIN [5/25][85/157] Loss_G: 0.0122\n",
            "PRETRAIN [5/25][86/157] Loss_G: 0.0100\n",
            "PRETRAIN [5/25][87/157] Loss_G: 0.0108\n",
            "PRETRAIN [5/25][88/157] Loss_G: 0.0121\n",
            "PRETRAIN [5/25][89/157] Loss_G: 0.0112\n",
            "PRETRAIN [5/25][90/157] Loss_G: 0.0112\n",
            "PRETRAIN [5/25][91/157] Loss_G: 0.0117\n",
            "PRETRAIN [5/25][92/157] Loss_G: 0.0106\n",
            "PRETRAIN [5/25][93/157] Loss_G: 0.0130\n",
            "PRETRAIN [5/25][94/157] Loss_G: 0.0111\n",
            "PRETRAIN [5/25][95/157] Loss_G: 0.0111\n",
            "PRETRAIN [5/25][96/157] Loss_G: 0.0127\n",
            "PRETRAIN [5/25][97/157] Loss_G: 0.0118\n",
            "PRETRAIN [5/25][98/157] Loss_G: 0.0123\n",
            "PRETRAIN [5/25][99/157] Loss_G: 0.0136\n",
            "PRETRAIN [5/25][100/157] Loss_G: 0.0110\n",
            "PRETRAIN [5/25][101/157] Loss_G: 0.0151\n",
            "PRETRAIN [5/25][102/157] Loss_G: 0.0115\n",
            "PRETRAIN [5/25][103/157] Loss_G: 0.0125\n",
            "PRETRAIN [5/25][104/157] Loss_G: 0.0111\n",
            "PRETRAIN [5/25][105/157] Loss_G: 0.0125\n",
            "PRETRAIN [5/25][106/157] Loss_G: 0.0108\n",
            "PRETRAIN [5/25][107/157] Loss_G: 0.0108\n",
            "PRETRAIN [5/25][108/157] Loss_G: 0.0146\n",
            "PRETRAIN [5/25][109/157] Loss_G: 0.0132\n",
            "PRETRAIN [5/25][110/157] Loss_G: 0.0125\n",
            "PRETRAIN [5/25][111/157] Loss_G: 0.0101\n",
            "PRETRAIN [5/25][112/157] Loss_G: 0.0109\n",
            "PRETRAIN [5/25][113/157] Loss_G: 0.0110\n",
            "PRETRAIN [5/25][114/157] Loss_G: 0.0111\n",
            "PRETRAIN [5/25][115/157] Loss_G: 0.0116\n",
            "PRETRAIN [5/25][116/157] Loss_G: 0.0102\n",
            "PRETRAIN [5/25][117/157] Loss_G: 0.0113\n",
            "PRETRAIN [5/25][118/157] Loss_G: 0.0138\n",
            "PRETRAIN [5/25][119/157] Loss_G: 0.0125\n",
            "PRETRAIN [5/25][120/157] Loss_G: 0.0150\n",
            "PRETRAIN [5/25][121/157] Loss_G: 0.0166\n",
            "PRETRAIN [5/25][122/157] Loss_G: 0.0152\n",
            "PRETRAIN [5/25][123/157] Loss_G: 0.0127\n",
            "PRETRAIN [5/25][124/157] Loss_G: 0.0192\n",
            "PRETRAIN [5/25][125/157] Loss_G: 0.0151\n",
            "PRETRAIN [5/25][126/157] Loss_G: 0.0154\n",
            "PRETRAIN [5/25][127/157] Loss_G: 0.0121\n",
            "PRETRAIN [5/25][128/157] Loss_G: 0.0118\n",
            "PRETRAIN [5/25][129/157] Loss_G: 0.0117\n",
            "PRETRAIN [5/25][130/157] Loss_G: 0.0132\n",
            "PRETRAIN [5/25][131/157] Loss_G: 0.0122\n",
            "PRETRAIN [5/25][132/157] Loss_G: 0.0137\n",
            "PRETRAIN [5/25][133/157] Loss_G: 0.0141\n",
            "PRETRAIN [5/25][134/157] Loss_G: 0.0140\n",
            "PRETRAIN [5/25][135/157] Loss_G: 0.0115\n",
            "PRETRAIN [5/25][136/157] Loss_G: 0.0119\n",
            "PRETRAIN [5/25][137/157] Loss_G: 0.0114\n",
            "PRETRAIN [5/25][138/157] Loss_G: 0.0120\n",
            "PRETRAIN [5/25][139/157] Loss_G: 0.0117\n",
            "PRETRAIN [5/25][140/157] Loss_G: 0.0103\n",
            "PRETRAIN [5/25][141/157] Loss_G: 0.0102\n",
            "PRETRAIN [5/25][142/157] Loss_G: 0.0121\n",
            "PRETRAIN [5/25][143/157] Loss_G: 0.0126\n",
            "PRETRAIN [5/25][144/157] Loss_G: 0.0171\n",
            "PRETRAIN [5/25][145/157] Loss_G: 0.0145\n",
            "PRETRAIN [5/25][146/157] Loss_G: 0.0116\n",
            "PRETRAIN [5/25][147/157] Loss_G: 0.0126\n",
            "PRETRAIN [5/25][148/157] Loss_G: 0.0117\n",
            "PRETRAIN [5/25][149/157] Loss_G: 0.0130\n",
            "PRETRAIN [5/25][150/157] Loss_G: 0.0115\n",
            "PRETRAIN [5/25][151/157] Loss_G: 0.0137\n",
            "PRETRAIN [5/25][152/157] Loss_G: 0.0127\n",
            "PRETRAIN [5/25][153/157] Loss_G: 0.0113\n",
            "PRETRAIN [5/25][154/157] Loss_G: 0.0135\n",
            "PRETRAIN [5/25][155/157] Loss_G: 0.0118\n",
            "PRETRAIN [5/25][156/157] Loss_G: 0.0124\n",
            "Time elapsed Epoch 5: 116 seconds\n",
            "PRETRAIN [6/25][0/157] Loss_G: 0.0123\n",
            "PRETRAIN [6/25][1/157] Loss_G: 0.0123\n",
            "PRETRAIN [6/25][2/157] Loss_G: 0.0138\n",
            "PRETRAIN [6/25][3/157] Loss_G: 0.0106\n",
            "PRETRAIN [6/25][4/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][5/157] Loss_G: 0.0112\n",
            "PRETRAIN [6/25][6/157] Loss_G: 0.0115\n",
            "PRETRAIN [6/25][7/157] Loss_G: 0.0107\n",
            "PRETRAIN [6/25][8/157] Loss_G: 0.0122\n",
            "PRETRAIN [6/25][9/157] Loss_G: 0.0102\n",
            "PRETRAIN [6/25][10/157] Loss_G: 0.0096\n",
            "PRETRAIN [6/25][11/157] Loss_G: 0.0128\n",
            "PRETRAIN [6/25][12/157] Loss_G: 0.0100\n",
            "PRETRAIN [6/25][13/157] Loss_G: 0.0100\n",
            "PRETRAIN [6/25][14/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][15/157] Loss_G: 0.0103\n",
            "PRETRAIN [6/25][16/157] Loss_G: 0.0108\n",
            "PRETRAIN [6/25][17/157] Loss_G: 0.0105\n",
            "PRETRAIN [6/25][18/157] Loss_G: 0.0128\n",
            "PRETRAIN [6/25][19/157] Loss_G: 0.0127\n",
            "PRETRAIN [6/25][20/157] Loss_G: 0.0105\n",
            "PRETRAIN [6/25][21/157] Loss_G: 0.0123\n",
            "PRETRAIN [6/25][22/157] Loss_G: 0.0122\n",
            "PRETRAIN [6/25][23/157] Loss_G: 0.0134\n",
            "PRETRAIN [6/25][24/157] Loss_G: 0.0136\n",
            "PRETRAIN [6/25][25/157] Loss_G: 0.0123\n",
            "PRETRAIN [6/25][26/157] Loss_G: 0.0107\n",
            "PRETRAIN [6/25][27/157] Loss_G: 0.0111\n",
            "PRETRAIN [6/25][28/157] Loss_G: 0.0124\n",
            "PRETRAIN [6/25][29/157] Loss_G: 0.0116\n",
            "PRETRAIN [6/25][30/157] Loss_G: 0.0134\n",
            "PRETRAIN [6/25][31/157] Loss_G: 0.0120\n",
            "PRETRAIN [6/25][32/157] Loss_G: 0.0113\n",
            "PRETRAIN [6/25][33/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][34/157] Loss_G: 0.0096\n",
            "PRETRAIN [6/25][35/157] Loss_G: 0.0101\n",
            "PRETRAIN [6/25][36/157] Loss_G: 0.0112\n",
            "PRETRAIN [6/25][37/157] Loss_G: 0.0131\n",
            "PRETRAIN [6/25][38/157] Loss_G: 0.0101\n",
            "PRETRAIN [6/25][39/157] Loss_G: 0.0113\n",
            "PRETRAIN [6/25][40/157] Loss_G: 0.0106\n",
            "PRETRAIN [6/25][41/157] Loss_G: 0.0135\n",
            "PRETRAIN [6/25][42/157] Loss_G: 0.0121\n",
            "PRETRAIN [6/25][43/157] Loss_G: 0.0104\n",
            "PRETRAIN [6/25][44/157] Loss_G: 0.0106\n",
            "PRETRAIN [6/25][45/157] Loss_G: 0.0121\n",
            "PRETRAIN [6/25][46/157] Loss_G: 0.0091\n",
            "PRETRAIN [6/25][47/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][48/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][49/157] Loss_G: 0.0104\n",
            "PRETRAIN [6/25][50/157] Loss_G: 0.0131\n",
            "PRETRAIN [6/25][51/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][52/157] Loss_G: 0.0130\n",
            "PRETRAIN [6/25][53/157] Loss_G: 0.0104\n",
            "PRETRAIN [6/25][54/157] Loss_G: 0.0126\n",
            "PRETRAIN [6/25][55/157] Loss_G: 0.0120\n",
            "PRETRAIN [6/25][56/157] Loss_G: 0.0099\n",
            "PRETRAIN [6/25][57/157] Loss_G: 0.0098\n",
            "PRETRAIN [6/25][58/157] Loss_G: 0.0128\n",
            "PRETRAIN [6/25][59/157] Loss_G: 0.0096\n",
            "PRETRAIN [6/25][60/157] Loss_G: 0.0102\n",
            "PRETRAIN [6/25][61/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][62/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][63/157] Loss_G: 0.0108\n",
            "PRETRAIN [6/25][64/157] Loss_G: 0.0117\n",
            "PRETRAIN [6/25][65/157] Loss_G: 0.0121\n",
            "PRETRAIN [6/25][66/157] Loss_G: 0.0107\n",
            "PRETRAIN [6/25][67/157] Loss_G: 0.0127\n",
            "PRETRAIN [6/25][68/157] Loss_G: 0.0142\n",
            "PRETRAIN [6/25][69/157] Loss_G: 0.0121\n",
            "PRETRAIN [6/25][70/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][71/157] Loss_G: 0.0136\n",
            "PRETRAIN [6/25][72/157] Loss_G: 0.0106\n",
            "PRETRAIN [6/25][73/157] Loss_G: 0.0130\n",
            "PRETRAIN [6/25][74/157] Loss_G: 0.0119\n",
            "PRETRAIN [6/25][75/157] Loss_G: 0.0191\n",
            "PRETRAIN [6/25][76/157] Loss_G: 0.0096\n",
            "PRETRAIN [6/25][77/157] Loss_G: 0.0112\n",
            "PRETRAIN [6/25][78/157] Loss_G: 0.0122\n",
            "PRETRAIN [6/25][79/157] Loss_G: 0.0115\n",
            "PRETRAIN [6/25][80/157] Loss_G: 0.0169\n",
            "PRETRAIN [6/25][81/157] Loss_G: 0.0134\n",
            "PRETRAIN [6/25][82/157] Loss_G: 0.0123\n",
            "PRETRAIN [6/25][83/157] Loss_G: 0.0121\n",
            "PRETRAIN [6/25][84/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][85/157] Loss_G: 0.0117\n",
            "PRETRAIN [6/25][86/157] Loss_G: 0.0138\n",
            "PRETRAIN [6/25][87/157] Loss_G: 0.0115\n",
            "PRETRAIN [6/25][88/157] Loss_G: 0.0098\n",
            "PRETRAIN [6/25][89/157] Loss_G: 0.0097\n",
            "PRETRAIN [6/25][90/157] Loss_G: 0.0107\n",
            "PRETRAIN [6/25][91/157] Loss_G: 0.0116\n",
            "PRETRAIN [6/25][92/157] Loss_G: 0.0101\n",
            "PRETRAIN [6/25][93/157] Loss_G: 0.0116\n",
            "PRETRAIN [6/25][94/157] Loss_G: 0.0116\n",
            "PRETRAIN [6/25][95/157] Loss_G: 0.0120\n",
            "PRETRAIN [6/25][96/157] Loss_G: 0.0119\n",
            "PRETRAIN [6/25][97/157] Loss_G: 0.0113\n",
            "PRETRAIN [6/25][98/157] Loss_G: 0.0085\n",
            "PRETRAIN [6/25][99/157] Loss_G: 0.0104\n",
            "PRETRAIN [6/25][100/157] Loss_G: 0.0097\n",
            "PRETRAIN [6/25][101/157] Loss_G: 0.0097\n",
            "PRETRAIN [6/25][102/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][103/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][104/157] Loss_G: 0.0101\n",
            "PRETRAIN [6/25][105/157] Loss_G: 0.0121\n",
            "PRETRAIN [6/25][106/157] Loss_G: 0.0108\n",
            "PRETRAIN [6/25][107/157] Loss_G: 0.0118\n",
            "PRETRAIN [6/25][108/157] Loss_G: 0.0116\n",
            "PRETRAIN [6/25][109/157] Loss_G: 0.0103\n",
            "PRETRAIN [6/25][110/157] Loss_G: 0.0134\n",
            "PRETRAIN [6/25][111/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][112/157] Loss_G: 0.0125\n",
            "PRETRAIN [6/25][113/157] Loss_G: 0.0104\n",
            "PRETRAIN [6/25][114/157] Loss_G: 0.0097\n",
            "PRETRAIN [6/25][115/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][116/157] Loss_G: 0.0134\n",
            "PRETRAIN [6/25][117/157] Loss_G: 0.0104\n",
            "PRETRAIN [6/25][118/157] Loss_G: 0.0100\n",
            "PRETRAIN [6/25][119/157] Loss_G: 0.0105\n",
            "PRETRAIN [6/25][120/157] Loss_G: 0.0121\n",
            "PRETRAIN [6/25][121/157] Loss_G: 0.0111\n",
            "PRETRAIN [6/25][122/157] Loss_G: 0.0100\n",
            "PRETRAIN [6/25][123/157] Loss_G: 0.0103\n",
            "PRETRAIN [6/25][124/157] Loss_G: 0.0138\n",
            "PRETRAIN [6/25][125/157] Loss_G: 0.0113\n",
            "PRETRAIN [6/25][126/157] Loss_G: 0.0109\n",
            "PRETRAIN [6/25][127/157] Loss_G: 0.0140\n",
            "PRETRAIN [6/25][128/157] Loss_G: 0.0131\n",
            "PRETRAIN [6/25][129/157] Loss_G: 0.0145\n",
            "PRETRAIN [6/25][130/157] Loss_G: 0.0115\n",
            "PRETRAIN [6/25][131/157] Loss_G: 0.0147\n",
            "PRETRAIN [6/25][132/157] Loss_G: 0.0135\n",
            "PRETRAIN [6/25][133/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][134/157] Loss_G: 0.0123\n",
            "PRETRAIN [6/25][135/157] Loss_G: 0.0124\n",
            "PRETRAIN [6/25][136/157] Loss_G: 0.0133\n",
            "PRETRAIN [6/25][137/157] Loss_G: 0.0122\n",
            "PRETRAIN [6/25][138/157] Loss_G: 0.0124\n",
            "PRETRAIN [6/25][139/157] Loss_G: 0.0098\n",
            "PRETRAIN [6/25][140/157] Loss_G: 0.0118\n",
            "PRETRAIN [6/25][141/157] Loss_G: 0.0118\n",
            "PRETRAIN [6/25][142/157] Loss_G: 0.0096\n",
            "PRETRAIN [6/25][143/157] Loss_G: 0.0107\n",
            "PRETRAIN [6/25][144/157] Loss_G: 0.0108\n",
            "PRETRAIN [6/25][145/157] Loss_G: 0.0129\n",
            "PRETRAIN [6/25][146/157] Loss_G: 0.0104\n",
            "PRETRAIN [6/25][147/157] Loss_G: 0.0094\n",
            "PRETRAIN [6/25][148/157] Loss_G: 0.0107\n",
            "PRETRAIN [6/25][149/157] Loss_G: 0.0116\n",
            "PRETRAIN [6/25][150/157] Loss_G: 0.0118\n",
            "PRETRAIN [6/25][151/157] Loss_G: 0.0105\n",
            "PRETRAIN [6/25][152/157] Loss_G: 0.0120\n",
            "PRETRAIN [6/25][153/157] Loss_G: 0.0110\n",
            "PRETRAIN [6/25][154/157] Loss_G: 0.0125\n",
            "PRETRAIN [6/25][155/157] Loss_G: 0.0116\n",
            "PRETRAIN [6/25][156/157] Loss_G: 0.0157\n",
            "Time elapsed Epoch 6: 115 seconds\n",
            "PRETRAIN [7/25][0/157] Loss_G: 0.0117\n",
            "PRETRAIN [7/25][1/157] Loss_G: 0.0120\n",
            "PRETRAIN [7/25][2/157] Loss_G: 0.0096\n",
            "PRETRAIN [7/25][3/157] Loss_G: 0.0128\n",
            "PRETRAIN [7/25][4/157] Loss_G: 0.0133\n",
            "PRETRAIN [7/25][5/157] Loss_G: 0.0150\n",
            "PRETRAIN [7/25][6/157] Loss_G: 0.0122\n",
            "PRETRAIN [7/25][7/157] Loss_G: 0.0119\n",
            "PRETRAIN [7/25][8/157] Loss_G: 0.0104\n",
            "PRETRAIN [7/25][9/157] Loss_G: 0.0110\n",
            "PRETRAIN [7/25][10/157] Loss_G: 0.0100\n",
            "PRETRAIN [7/25][11/157] Loss_G: 0.0098\n",
            "PRETRAIN [7/25][12/157] Loss_G: 0.0102\n",
            "PRETRAIN [7/25][13/157] Loss_G: 0.0107\n",
            "PRETRAIN [7/25][14/157] Loss_G: 0.0088\n",
            "PRETRAIN [7/25][15/157] Loss_G: 0.0095\n",
            "PRETRAIN [7/25][16/157] Loss_G: 0.0115\n",
            "PRETRAIN [7/25][17/157] Loss_G: 0.0091\n",
            "PRETRAIN [7/25][18/157] Loss_G: 0.0100\n",
            "PRETRAIN [7/25][19/157] Loss_G: 0.0106\n",
            "PRETRAIN [7/25][20/157] Loss_G: 0.0092\n",
            "PRETRAIN [7/25][21/157] Loss_G: 0.0119\n",
            "PRETRAIN [7/25][22/157] Loss_G: 0.0095\n",
            "PRETRAIN [7/25][23/157] Loss_G: 0.0093\n",
            "PRETRAIN [7/25][24/157] Loss_G: 0.0102\n",
            "PRETRAIN [7/25][25/157] Loss_G: 0.0100\n",
            "PRETRAIN [7/25][26/157] Loss_G: 0.0099\n",
            "PRETRAIN [7/25][27/157] Loss_G: 0.0102\n",
            "PRETRAIN [7/25][28/157] Loss_G: 0.0097\n",
            "PRETRAIN [7/25][29/157] Loss_G: 0.0115\n",
            "PRETRAIN [7/25][30/157] Loss_G: 0.0112\n",
            "PRETRAIN [7/25][31/157] Loss_G: 0.0171\n",
            "PRETRAIN [7/25][32/157] Loss_G: 0.0405\n",
            "PRETRAIN [7/25][33/157] Loss_G: 0.0389\n",
            "PRETRAIN [7/25][34/157] Loss_G: 0.0308\n",
            "PRETRAIN [7/25][35/157] Loss_G: 0.0286\n",
            "PRETRAIN [7/25][36/157] Loss_G: 0.0246\n",
            "PRETRAIN [7/25][37/157] Loss_G: 0.0254\n",
            "PRETRAIN [7/25][38/157] Loss_G: 0.0246\n",
            "PRETRAIN [7/25][39/157] Loss_G: 0.0329\n",
            "PRETRAIN [7/25][40/157] Loss_G: 0.0280\n",
            "PRETRAIN [7/25][41/157] Loss_G: 0.0225\n",
            "PRETRAIN [7/25][42/157] Loss_G: 0.0195\n",
            "PRETRAIN [7/25][43/157] Loss_G: 0.0189\n",
            "PRETRAIN [7/25][44/157] Loss_G: 0.0162\n",
            "PRETRAIN [7/25][45/157] Loss_G: 0.0181\n",
            "PRETRAIN [7/25][46/157] Loss_G: 0.0132\n",
            "PRETRAIN [7/25][47/157] Loss_G: 0.0132\n",
            "PRETRAIN [7/25][48/157] Loss_G: 0.0153\n",
            "PRETRAIN [7/25][49/157] Loss_G: 0.0128\n",
            "PRETRAIN [7/25][50/157] Loss_G: 0.0121\n",
            "PRETRAIN [7/25][51/157] Loss_G: 0.0136\n",
            "PRETRAIN [7/25][52/157] Loss_G: 0.0125\n",
            "PRETRAIN [7/25][53/157] Loss_G: 0.0119\n",
            "PRETRAIN [7/25][54/157] Loss_G: 0.0146\n",
            "PRETRAIN [7/25][55/157] Loss_G: 0.0111\n",
            "PRETRAIN [7/25][56/157] Loss_G: 0.0124\n",
            "PRETRAIN [7/25][57/157] Loss_G: 0.0125\n",
            "PRETRAIN [7/25][58/157] Loss_G: 0.0130\n",
            "PRETRAIN [7/25][59/157] Loss_G: 0.0118\n",
            "PRETRAIN [7/25][60/157] Loss_G: 0.0113\n",
            "PRETRAIN [7/25][61/157] Loss_G: 0.0116\n",
            "PRETRAIN [7/25][62/157] Loss_G: 0.0110\n",
            "PRETRAIN [7/25][63/157] Loss_G: 0.0119\n",
            "PRETRAIN [7/25][64/157] Loss_G: 0.0116\n",
            "PRETRAIN [7/25][65/157] Loss_G: 0.0123\n",
            "PRETRAIN [7/25][66/157] Loss_G: 0.0109\n",
            "PRETRAIN [7/25][67/157] Loss_G: 0.0119\n",
            "PRETRAIN [7/25][68/157] Loss_G: 0.0119\n",
            "PRETRAIN [7/25][69/157] Loss_G: 0.0129\n",
            "PRETRAIN [7/25][70/157] Loss_G: 0.0106\n",
            "PRETRAIN [7/25][71/157] Loss_G: 0.0132\n",
            "PRETRAIN [7/25][72/157] Loss_G: 0.0091\n",
            "PRETRAIN [7/25][73/157] Loss_G: 0.0126\n",
            "PRETRAIN [7/25][74/157] Loss_G: 0.0105\n",
            "PRETRAIN [7/25][75/157] Loss_G: 0.0118\n",
            "PRETRAIN [7/25][76/157] Loss_G: 0.0124\n",
            "PRETRAIN [7/25][77/157] Loss_G: 0.0135\n",
            "PRETRAIN [7/25][78/157] Loss_G: 0.0121\n",
            "PRETRAIN [7/25][79/157] Loss_G: 0.0141\n",
            "PRETRAIN [7/25][80/157] Loss_G: 0.0134\n",
            "PRETRAIN [7/25][81/157] Loss_G: 0.0127\n",
            "PRETRAIN [7/25][82/157] Loss_G: 0.0136\n",
            "PRETRAIN [7/25][83/157] Loss_G: 0.0104\n",
            "PRETRAIN [7/25][84/157] Loss_G: 0.0124\n",
            "PRETRAIN [7/25][85/157] Loss_G: 0.0114\n",
            "PRETRAIN [7/25][86/157] Loss_G: 0.0123\n",
            "PRETRAIN [7/25][87/157] Loss_G: 0.0142\n",
            "PRETRAIN [7/25][88/157] Loss_G: 0.0114\n",
            "PRETRAIN [7/25][89/157] Loss_G: 0.0152\n",
            "PRETRAIN [7/25][90/157] Loss_G: 0.0122\n",
            "PRETRAIN [7/25][91/157] Loss_G: 0.0108\n",
            "PRETRAIN [7/25][92/157] Loss_G: 0.0104\n",
            "PRETRAIN [7/25][93/157] Loss_G: 0.0119\n",
            "PRETRAIN [7/25][94/157] Loss_G: 0.0113\n",
            "PRETRAIN [7/25][95/157] Loss_G: 0.0115\n",
            "PRETRAIN [7/25][96/157] Loss_G: 0.0133\n",
            "PRETRAIN [7/25][97/157] Loss_G: 0.0117\n",
            "PRETRAIN [7/25][98/157] Loss_G: 0.0141\n",
            "PRETRAIN [7/25][99/157] Loss_G: 0.0103\n",
            "PRETRAIN [7/25][100/157] Loss_G: 0.0115\n",
            "PRETRAIN [7/25][101/157] Loss_G: 0.0125\n",
            "PRETRAIN [7/25][102/157] Loss_G: 0.0118\n",
            "PRETRAIN [7/25][103/157] Loss_G: 0.0111\n",
            "PRETRAIN [7/25][104/157] Loss_G: 0.0091\n",
            "PRETRAIN [7/25][105/157] Loss_G: 0.0100\n",
            "PRETRAIN [7/25][106/157] Loss_G: 0.0092\n",
            "PRETRAIN [7/25][107/157] Loss_G: 0.0115\n",
            "PRETRAIN [7/25][108/157] Loss_G: 0.0113\n",
            "PRETRAIN [7/25][109/157] Loss_G: 0.0138\n",
            "PRETRAIN [7/25][110/157] Loss_G: 0.0120\n",
            "PRETRAIN [7/25][111/157] Loss_G: 0.0111\n",
            "PRETRAIN [7/25][112/157] Loss_G: 0.0131\n",
            "PRETRAIN [7/25][113/157] Loss_G: 0.0110\n",
            "PRETRAIN [7/25][114/157] Loss_G: 0.0117\n",
            "PRETRAIN [7/25][115/157] Loss_G: 0.0107\n",
            "PRETRAIN [7/25][116/157] Loss_G: 0.0111\n",
            "PRETRAIN [7/25][117/157] Loss_G: 0.0100\n",
            "PRETRAIN [7/25][118/157] Loss_G: 0.0090\n",
            "PRETRAIN [7/25][119/157] Loss_G: 0.0118\n",
            "PRETRAIN [7/25][120/157] Loss_G: 0.0095\n",
            "PRETRAIN [7/25][121/157] Loss_G: 0.0118\n",
            "PRETRAIN [7/25][122/157] Loss_G: 0.0103\n",
            "PRETRAIN [7/25][123/157] Loss_G: 0.0093\n",
            "PRETRAIN [7/25][124/157] Loss_G: 0.0114\n",
            "PRETRAIN [7/25][125/157] Loss_G: 0.0133\n",
            "PRETRAIN [7/25][126/157] Loss_G: 0.0120\n",
            "PRETRAIN [7/25][127/157] Loss_G: 0.0108\n",
            "PRETRAIN [7/25][128/157] Loss_G: 0.0120\n",
            "PRETRAIN [7/25][129/157] Loss_G: 0.0143\n",
            "PRETRAIN [7/25][130/157] Loss_G: 0.0135\n",
            "PRETRAIN [7/25][131/157] Loss_G: 0.0187\n",
            "PRETRAIN [7/25][132/157] Loss_G: 0.0110\n",
            "PRETRAIN [7/25][133/157] Loss_G: 0.0096\n",
            "PRETRAIN [7/25][134/157] Loss_G: 0.0098\n",
            "PRETRAIN [7/25][135/157] Loss_G: 0.0124\n",
            "PRETRAIN [7/25][136/157] Loss_G: 0.0102\n",
            "PRETRAIN [7/25][137/157] Loss_G: 0.0114\n",
            "PRETRAIN [7/25][138/157] Loss_G: 0.0130\n",
            "PRETRAIN [7/25][139/157] Loss_G: 0.0117\n",
            "PRETRAIN [7/25][140/157] Loss_G: 0.0104\n",
            "PRETRAIN [7/25][141/157] Loss_G: 0.0091\n",
            "PRETRAIN [7/25][142/157] Loss_G: 0.0095\n",
            "PRETRAIN [7/25][143/157] Loss_G: 0.0108\n",
            "PRETRAIN [7/25][144/157] Loss_G: 0.0120\n",
            "PRETRAIN [7/25][145/157] Loss_G: 0.0102\n",
            "PRETRAIN [7/25][146/157] Loss_G: 0.0110\n",
            "PRETRAIN [7/25][147/157] Loss_G: 0.0098\n",
            "PRETRAIN [7/25][148/157] Loss_G: 0.0126\n",
            "PRETRAIN [7/25][149/157] Loss_G: 0.0149\n",
            "PRETRAIN [7/25][150/157] Loss_G: 0.0094\n",
            "PRETRAIN [7/25][151/157] Loss_G: 0.0118\n",
            "PRETRAIN [7/25][152/157] Loss_G: 0.0107\n",
            "PRETRAIN [7/25][153/157] Loss_G: 0.0093\n",
            "PRETRAIN [7/25][154/157] Loss_G: 0.0109\n",
            "PRETRAIN [7/25][155/157] Loss_G: 0.0100\n",
            "PRETRAIN [7/25][156/157] Loss_G: 0.0151\n",
            "Time elapsed Epoch 7: 115 seconds\n",
            "PRETRAIN [8/25][0/157] Loss_G: 0.0129\n",
            "PRETRAIN [8/25][1/157] Loss_G: 0.0116\n",
            "PRETRAIN [8/25][2/157] Loss_G: 0.0119\n",
            "PRETRAIN [8/25][3/157] Loss_G: 0.0094\n",
            "PRETRAIN [8/25][4/157] Loss_G: 0.0109\n",
            "PRETRAIN [8/25][5/157] Loss_G: 0.0103\n",
            "PRETRAIN [8/25][6/157] Loss_G: 0.0107\n",
            "PRETRAIN [8/25][7/157] Loss_G: 0.0121\n",
            "PRETRAIN [8/25][8/157] Loss_G: 0.0123\n",
            "PRETRAIN [8/25][9/157] Loss_G: 0.0116\n",
            "PRETRAIN [8/25][10/157] Loss_G: 0.0114\n",
            "PRETRAIN [8/25][11/157] Loss_G: 0.0093\n",
            "PRETRAIN [8/25][12/157] Loss_G: 0.0107\n",
            "PRETRAIN [8/25][13/157] Loss_G: 0.0104\n",
            "PRETRAIN [8/25][14/157] Loss_G: 0.0121\n",
            "PRETRAIN [8/25][15/157] Loss_G: 0.0110\n",
            "PRETRAIN [8/25][16/157] Loss_G: 0.0102\n",
            "PRETRAIN [8/25][17/157] Loss_G: 0.0090\n",
            "PRETRAIN [8/25][18/157] Loss_G: 0.0105\n",
            "PRETRAIN [8/25][19/157] Loss_G: 0.0093\n",
            "PRETRAIN [8/25][20/157] Loss_G: 0.0087\n",
            "PRETRAIN [8/25][21/157] Loss_G: 0.0110\n",
            "PRETRAIN [8/25][22/157] Loss_G: 0.0118\n",
            "PRETRAIN [8/25][23/157] Loss_G: 0.0098\n",
            "PRETRAIN [8/25][24/157] Loss_G: 0.0116\n",
            "PRETRAIN [8/25][25/157] Loss_G: 0.0103\n",
            "PRETRAIN [8/25][26/157] Loss_G: 0.0098\n",
            "PRETRAIN [8/25][27/157] Loss_G: 0.0104\n",
            "PRETRAIN [8/25][28/157] Loss_G: 0.0097\n",
            "PRETRAIN [8/25][29/157] Loss_G: 0.0093\n",
            "PRETRAIN [8/25][30/157] Loss_G: 0.0103\n",
            "PRETRAIN [8/25][31/157] Loss_G: 0.0099\n",
            "PRETRAIN [8/25][32/157] Loss_G: 0.0092\n",
            "PRETRAIN [8/25][33/157] Loss_G: 0.0093\n",
            "PRETRAIN [8/25][34/157] Loss_G: 0.0089\n",
            "PRETRAIN [8/25][35/157] Loss_G: 0.0100\n",
            "PRETRAIN [8/25][36/157] Loss_G: 0.0103\n",
            "PRETRAIN [8/25][37/157] Loss_G: 0.0110\n",
            "PRETRAIN [8/25][38/157] Loss_G: 0.0119\n",
            "PRETRAIN [8/25][39/157] Loss_G: 0.0115\n",
            "PRETRAIN [8/25][40/157] Loss_G: 0.0107\n",
            "PRETRAIN [8/25][41/157] Loss_G: 0.0096\n",
            "PRETRAIN [8/25][42/157] Loss_G: 0.0095\n",
            "PRETRAIN [8/25][43/157] Loss_G: 0.0099\n",
            "PRETRAIN [8/25][44/157] Loss_G: 0.0181\n",
            "PRETRAIN [8/25][45/157] Loss_G: 0.0137\n",
            "PRETRAIN [8/25][46/157] Loss_G: 0.0130\n",
            "PRETRAIN [8/25][47/157] Loss_G: 0.0109\n",
            "PRETRAIN [8/25][48/157] Loss_G: 0.0114\n",
            "PRETRAIN [8/25][49/157] Loss_G: 0.0092\n",
            "PRETRAIN [8/25][50/157] Loss_G: 0.0120\n",
            "PRETRAIN [8/25][51/157] Loss_G: 0.0111\n",
            "PRETRAIN [8/25][52/157] Loss_G: 0.0099\n",
            "PRETRAIN [8/25][53/157] Loss_G: 0.0112\n",
            "PRETRAIN [8/25][54/157] Loss_G: 0.0096\n",
            "PRETRAIN [8/25][55/157] Loss_G: 0.0102\n",
            "PRETRAIN [8/25][56/157] Loss_G: 0.0111\n",
            "PRETRAIN [8/25][57/157] Loss_G: 0.0099\n",
            "PRETRAIN [8/25][58/157] Loss_G: 0.0109\n",
            "PRETRAIN [8/25][59/157] Loss_G: 0.0098\n",
            "PRETRAIN [8/25][60/157] Loss_G: 0.0098\n",
            "PRETRAIN [8/25][61/157] Loss_G: 0.0093\n",
            "PRETRAIN [8/25][62/157] Loss_G: 0.0106\n",
            "PRETRAIN [8/25][63/157] Loss_G: 0.0091\n",
            "PRETRAIN [8/25][64/157] Loss_G: 0.0112\n",
            "PRETRAIN [8/25][65/157] Loss_G: 0.0122\n",
            "PRETRAIN [8/25][66/157] Loss_G: 0.0106\n",
            "PRETRAIN [8/25][67/157] Loss_G: 0.0108\n",
            "PRETRAIN [8/25][68/157] Loss_G: 0.0123\n",
            "PRETRAIN [8/25][69/157] Loss_G: 0.0114\n",
            "PRETRAIN [8/25][70/157] Loss_G: 0.0120\n",
            "PRETRAIN [8/25][71/157] Loss_G: 0.0214\n",
            "PRETRAIN [8/25][72/157] Loss_G: 0.0210\n",
            "PRETRAIN [8/25][73/157] Loss_G: 0.0238\n",
            "PRETRAIN [8/25][74/157] Loss_G: 0.0248\n",
            "PRETRAIN [8/25][75/157] Loss_G: 0.0182\n",
            "PRETRAIN [8/25][76/157] Loss_G: 0.0164\n",
            "PRETRAIN [8/25][77/157] Loss_G: 0.0169\n",
            "PRETRAIN [8/25][78/157] Loss_G: 0.0157\n",
            "PRETRAIN [8/25][79/157] Loss_G: 0.0129\n",
            "PRETRAIN [8/25][80/157] Loss_G: 0.0160\n",
            "PRETRAIN [8/25][81/157] Loss_G: 0.0129\n",
            "PRETRAIN [8/25][82/157] Loss_G: 0.0146\n",
            "PRETRAIN [8/25][83/157] Loss_G: 0.0135\n",
            "PRETRAIN [8/25][84/157] Loss_G: 0.0135\n",
            "PRETRAIN [8/25][85/157] Loss_G: 0.0111\n",
            "PRETRAIN [8/25][86/157] Loss_G: 0.0109\n",
            "PRETRAIN [8/25][87/157] Loss_G: 0.0104\n",
            "PRETRAIN [8/25][88/157] Loss_G: 0.0108\n",
            "PRETRAIN [8/25][89/157] Loss_G: 0.0124\n",
            "PRETRAIN [8/25][90/157] Loss_G: 0.0105\n",
            "PRETRAIN [8/25][91/157] Loss_G: 0.0119\n",
            "PRETRAIN [8/25][92/157] Loss_G: 0.0123\n",
            "PRETRAIN [8/25][93/157] Loss_G: 0.0125\n",
            "PRETRAIN [8/25][94/157] Loss_G: 0.0116\n",
            "PRETRAIN [8/25][95/157] Loss_G: 0.0113\n",
            "PRETRAIN [8/25][96/157] Loss_G: 0.0103\n",
            "PRETRAIN [8/25][97/157] Loss_G: 0.0109\n",
            "PRETRAIN [8/25][98/157] Loss_G: 0.0136\n",
            "PRETRAIN [8/25][99/157] Loss_G: 0.0100\n",
            "PRETRAIN [8/25][100/157] Loss_G: 0.0093\n",
            "PRETRAIN [8/25][101/157] Loss_G: 0.0142\n",
            "PRETRAIN [8/25][102/157] Loss_G: 0.0091\n",
            "PRETRAIN [8/25][103/157] Loss_G: 0.0102\n",
            "PRETRAIN [8/25][104/157] Loss_G: 0.0102\n",
            "PRETRAIN [8/25][105/157] Loss_G: 0.0128\n",
            "PRETRAIN [8/25][106/157] Loss_G: 0.0100\n",
            "PRETRAIN [8/25][107/157] Loss_G: 0.0109\n",
            "PRETRAIN [8/25][108/157] Loss_G: 0.0119\n",
            "PRETRAIN [8/25][109/157] Loss_G: 0.0120\n",
            "PRETRAIN [8/25][110/157] Loss_G: 0.0108\n",
            "PRETRAIN [8/25][111/157] Loss_G: 0.0106\n",
            "PRETRAIN [8/25][112/157] Loss_G: 0.0089\n",
            "PRETRAIN [8/25][113/157] Loss_G: 0.0089\n",
            "PRETRAIN [8/25][114/157] Loss_G: 0.0095\n",
            "PRETRAIN [8/25][115/157] Loss_G: 0.0101\n",
            "PRETRAIN [8/25][116/157] Loss_G: 0.0127\n",
            "PRETRAIN [8/25][117/157] Loss_G: 0.0094\n",
            "PRETRAIN [8/25][118/157] Loss_G: 0.0170\n",
            "PRETRAIN [8/25][119/157] Loss_G: 0.0110\n",
            "PRETRAIN [8/25][120/157] Loss_G: 0.0111\n",
            "PRETRAIN [8/25][121/157] Loss_G: 0.0100\n",
            "PRETRAIN [8/25][122/157] Loss_G: 0.0100\n",
            "PRETRAIN [8/25][123/157] Loss_G: 0.0123\n",
            "PRETRAIN [8/25][124/157] Loss_G: 0.0083\n",
            "PRETRAIN [8/25][125/157] Loss_G: 0.0093\n",
            "PRETRAIN [8/25][126/157] Loss_G: 0.0106\n",
            "PRETRAIN [8/25][127/157] Loss_G: 0.0087\n",
            "PRETRAIN [8/25][128/157] Loss_G: 0.0088\n",
            "PRETRAIN [8/25][129/157] Loss_G: 0.0095\n",
            "PRETRAIN [8/25][130/157] Loss_G: 0.0102\n",
            "PRETRAIN [8/25][131/157] Loss_G: 0.0086\n",
            "PRETRAIN [8/25][132/157] Loss_G: 0.0104\n",
            "PRETRAIN [8/25][133/157] Loss_G: 0.0090\n",
            "PRETRAIN [8/25][134/157] Loss_G: 0.0123\n",
            "PRETRAIN [8/25][135/157] Loss_G: 0.0122\n",
            "PRETRAIN [8/25][136/157] Loss_G: 0.0150\n",
            "PRETRAIN [8/25][137/157] Loss_G: 0.0144\n",
            "PRETRAIN [8/25][138/157] Loss_G: 0.0114\n",
            "PRETRAIN [8/25][139/157] Loss_G: 0.0107\n",
            "PRETRAIN [8/25][140/157] Loss_G: 0.0102\n",
            "PRETRAIN [8/25][141/157] Loss_G: 0.0107\n",
            "PRETRAIN [8/25][142/157] Loss_G: 0.0122\n",
            "PRETRAIN [8/25][143/157] Loss_G: 0.0115\n",
            "PRETRAIN [8/25][144/157] Loss_G: 0.0122\n",
            "PRETRAIN [8/25][145/157] Loss_G: 0.0097\n",
            "PRETRAIN [8/25][146/157] Loss_G: 0.0106\n",
            "PRETRAIN [8/25][147/157] Loss_G: 0.0103\n",
            "PRETRAIN [8/25][148/157] Loss_G: 0.0107\n",
            "PRETRAIN [8/25][149/157] Loss_G: 0.0099\n",
            "PRETRAIN [8/25][150/157] Loss_G: 0.0109\n",
            "PRETRAIN [8/25][151/157] Loss_G: 0.0090\n",
            "PRETRAIN [8/25][152/157] Loss_G: 0.0142\n",
            "PRETRAIN [8/25][153/157] Loss_G: 0.0129\n",
            "PRETRAIN [8/25][154/157] Loss_G: 0.0092\n",
            "PRETRAIN [8/25][155/157] Loss_G: 0.0091\n",
            "PRETRAIN [8/25][156/157] Loss_G: 0.0163\n",
            "Time elapsed Epoch 8: 115 seconds\n",
            "PRETRAIN [9/25][0/157] Loss_G: 0.0182\n",
            "PRETRAIN [9/25][1/157] Loss_G: 0.0113\n",
            "PRETRAIN [9/25][2/157] Loss_G: 0.0093\n",
            "PRETRAIN [9/25][3/157] Loss_G: 0.0110\n",
            "PRETRAIN [9/25][4/157] Loss_G: 0.0106\n",
            "PRETRAIN [9/25][5/157] Loss_G: 0.0109\n",
            "PRETRAIN [9/25][6/157] Loss_G: 0.0088\n",
            "PRETRAIN [9/25][7/157] Loss_G: 0.0090\n",
            "PRETRAIN [9/25][8/157] Loss_G: 0.0101\n",
            "PRETRAIN [9/25][9/157] Loss_G: 0.0094\n",
            "PRETRAIN [9/25][10/157] Loss_G: 0.0108\n",
            "PRETRAIN [9/25][11/157] Loss_G: 0.0093\n",
            "PRETRAIN [9/25][12/157] Loss_G: 0.0075\n",
            "PRETRAIN [9/25][13/157] Loss_G: 0.0114\n",
            "PRETRAIN [9/25][14/157] Loss_G: 0.0094\n",
            "PRETRAIN [9/25][15/157] Loss_G: 0.0099\n",
            "PRETRAIN [9/25][16/157] Loss_G: 0.0142\n",
            "PRETRAIN [9/25][17/157] Loss_G: 0.0114\n",
            "PRETRAIN [9/25][18/157] Loss_G: 0.0111\n",
            "PRETRAIN [9/25][19/157] Loss_G: 0.0160\n",
            "PRETRAIN [9/25][20/157] Loss_G: 0.0096\n",
            "PRETRAIN [9/25][21/157] Loss_G: 0.0126\n",
            "PRETRAIN [9/25][22/157] Loss_G: 0.0100\n",
            "PRETRAIN [9/25][23/157] Loss_G: 0.0106\n",
            "PRETRAIN [9/25][24/157] Loss_G: 0.0091\n",
            "PRETRAIN [9/25][25/157] Loss_G: 0.0093\n",
            "PRETRAIN [9/25][26/157] Loss_G: 0.0097\n",
            "PRETRAIN [9/25][27/157] Loss_G: 0.0101\n",
            "PRETRAIN [9/25][28/157] Loss_G: 0.0088\n",
            "PRETRAIN [9/25][29/157] Loss_G: 0.0112\n",
            "PRETRAIN [9/25][30/157] Loss_G: 0.0089\n",
            "PRETRAIN [9/25][31/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][32/157] Loss_G: 0.0093\n",
            "PRETRAIN [9/25][33/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][34/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][35/157] Loss_G: 0.0081\n",
            "PRETRAIN [9/25][36/157] Loss_G: 0.0079\n",
            "PRETRAIN [9/25][37/157] Loss_G: 0.0086\n",
            "PRETRAIN [9/25][38/157] Loss_G: 0.0109\n",
            "PRETRAIN [9/25][39/157] Loss_G: 0.0107\n",
            "PRETRAIN [9/25][40/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][41/157] Loss_G: 0.0113\n",
            "PRETRAIN [9/25][42/157] Loss_G: 0.0118\n",
            "PRETRAIN [9/25][43/157] Loss_G: 0.0098\n",
            "PRETRAIN [9/25][44/157] Loss_G: 0.0112\n",
            "PRETRAIN [9/25][45/157] Loss_G: 0.0105\n",
            "PRETRAIN [9/25][46/157] Loss_G: 0.0095\n",
            "PRETRAIN [9/25][47/157] Loss_G: 0.0101\n",
            "PRETRAIN [9/25][48/157] Loss_G: 0.0125\n",
            "PRETRAIN [9/25][49/157] Loss_G: 0.0090\n",
            "PRETRAIN [9/25][50/157] Loss_G: 0.0108\n",
            "PRETRAIN [9/25][51/157] Loss_G: 0.0109\n",
            "PRETRAIN [9/25][52/157] Loss_G: 0.0122\n",
            "PRETRAIN [9/25][53/157] Loss_G: 0.0112\n",
            "PRETRAIN [9/25][54/157] Loss_G: 0.0101\n",
            "PRETRAIN [9/25][55/157] Loss_G: 0.0114\n",
            "PRETRAIN [9/25][56/157] Loss_G: 0.0118\n",
            "PRETRAIN [9/25][57/157] Loss_G: 0.0092\n",
            "PRETRAIN [9/25][58/157] Loss_G: 0.0107\n",
            "PRETRAIN [9/25][59/157] Loss_G: 0.0094\n",
            "PRETRAIN [9/25][60/157] Loss_G: 0.0099\n",
            "PRETRAIN [9/25][61/157] Loss_G: 0.0105\n",
            "PRETRAIN [9/25][62/157] Loss_G: 0.0095\n",
            "PRETRAIN [9/25][63/157] Loss_G: 0.0079\n",
            "PRETRAIN [9/25][64/157] Loss_G: 0.0091\n",
            "PRETRAIN [9/25][65/157] Loss_G: 0.0100\n",
            "PRETRAIN [9/25][66/157] Loss_G: 0.0101\n",
            "PRETRAIN [9/25][67/157] Loss_G: 0.0108\n",
            "PRETRAIN [9/25][68/157] Loss_G: 0.0119\n",
            "PRETRAIN [9/25][69/157] Loss_G: 0.0090\n",
            "PRETRAIN [9/25][70/157] Loss_G: 0.0136\n",
            "PRETRAIN [9/25][71/157] Loss_G: 0.0167\n",
            "PRETRAIN [9/25][72/157] Loss_G: 0.0106\n",
            "PRETRAIN [9/25][73/157] Loss_G: 0.0100\n",
            "PRETRAIN [9/25][74/157] Loss_G: 0.0132\n",
            "PRETRAIN [9/25][75/157] Loss_G: 0.0103\n",
            "PRETRAIN [9/25][76/157] Loss_G: 0.0108\n",
            "PRETRAIN [9/25][77/157] Loss_G: 0.0095\n",
            "PRETRAIN [9/25][78/157] Loss_G: 0.0109\n",
            "PRETRAIN [9/25][79/157] Loss_G: 0.0135\n",
            "PRETRAIN [9/25][80/157] Loss_G: 0.0119\n",
            "PRETRAIN [9/25][81/157] Loss_G: 0.0137\n",
            "PRETRAIN [9/25][82/157] Loss_G: 0.0134\n",
            "PRETRAIN [9/25][83/157] Loss_G: 0.0103\n",
            "PRETRAIN [9/25][84/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][85/157] Loss_G: 0.0101\n",
            "PRETRAIN [9/25][86/157] Loss_G: 0.0097\n",
            "PRETRAIN [9/25][87/157] Loss_G: 0.0096\n",
            "PRETRAIN [9/25][88/157] Loss_G: 0.0090\n",
            "PRETRAIN [9/25][89/157] Loss_G: 0.0094\n",
            "PRETRAIN [9/25][90/157] Loss_G: 0.0095\n",
            "PRETRAIN [9/25][91/157] Loss_G: 0.0089\n",
            "PRETRAIN [9/25][92/157] Loss_G: 0.0092\n",
            "PRETRAIN [9/25][93/157] Loss_G: 0.0094\n",
            "PRETRAIN [9/25][94/157] Loss_G: 0.0081\n",
            "PRETRAIN [9/25][95/157] Loss_G: 0.0099\n",
            "PRETRAIN [9/25][96/157] Loss_G: 0.0105\n",
            "PRETRAIN [9/25][97/157] Loss_G: 0.0081\n",
            "PRETRAIN [9/25][98/157] Loss_G: 0.0086\n",
            "PRETRAIN [9/25][99/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][100/157] Loss_G: 0.0092\n",
            "PRETRAIN [9/25][101/157] Loss_G: 0.0100\n",
            "PRETRAIN [9/25][102/157] Loss_G: 0.0091\n",
            "PRETRAIN [9/25][103/157] Loss_G: 0.0091\n",
            "PRETRAIN [9/25][104/157] Loss_G: 0.0098\n",
            "PRETRAIN [9/25][105/157] Loss_G: 0.0084\n",
            "PRETRAIN [9/25][106/157] Loss_G: 0.0108\n",
            "PRETRAIN [9/25][107/157] Loss_G: 0.0098\n",
            "PRETRAIN [9/25][108/157] Loss_G: 0.0103\n",
            "PRETRAIN [9/25][109/157] Loss_G: 0.0094\n",
            "PRETRAIN [9/25][110/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][111/157] Loss_G: 0.0109\n",
            "PRETRAIN [9/25][112/157] Loss_G: 0.0112\n",
            "PRETRAIN [9/25][113/157] Loss_G: 0.0110\n",
            "PRETRAIN [9/25][114/157] Loss_G: 0.0089\n",
            "PRETRAIN [9/25][115/157] Loss_G: 0.0088\n",
            "PRETRAIN [9/25][116/157] Loss_G: 0.0098\n",
            "PRETRAIN [9/25][117/157] Loss_G: 0.0086\n",
            "PRETRAIN [9/25][118/157] Loss_G: 0.0080\n",
            "PRETRAIN [9/25][119/157] Loss_G: 0.0082\n",
            "PRETRAIN [9/25][120/157] Loss_G: 0.0093\n",
            "PRETRAIN [9/25][121/157] Loss_G: 0.0099\n",
            "PRETRAIN [9/25][122/157] Loss_G: 0.0095\n",
            "PRETRAIN [9/25][123/157] Loss_G: 0.0105\n",
            "PRETRAIN [9/25][124/157] Loss_G: 0.0115\n",
            "PRETRAIN [9/25][125/157] Loss_G: 0.0088\n",
            "PRETRAIN [9/25][126/157] Loss_G: 0.0100\n",
            "PRETRAIN [9/25][127/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][128/157] Loss_G: 0.0096\n",
            "PRETRAIN [9/25][129/157] Loss_G: 0.0087\n",
            "PRETRAIN [9/25][130/157] Loss_G: 0.0102\n",
            "PRETRAIN [9/25][131/157] Loss_G: 0.0105\n",
            "PRETRAIN [9/25][132/157] Loss_G: 0.0107\n",
            "PRETRAIN [9/25][133/157] Loss_G: 0.0121\n",
            "PRETRAIN [9/25][134/157] Loss_G: 0.0086\n",
            "PRETRAIN [9/25][135/157] Loss_G: 0.0096\n",
            "PRETRAIN [9/25][136/157] Loss_G: 0.0097\n",
            "PRETRAIN [9/25][137/157] Loss_G: 0.0089\n",
            "PRETRAIN [9/25][138/157] Loss_G: 0.0080\n",
            "PRETRAIN [9/25][139/157] Loss_G: 0.0084\n",
            "PRETRAIN [9/25][140/157] Loss_G: 0.0087\n",
            "PRETRAIN [9/25][141/157] Loss_G: 0.0092\n",
            "PRETRAIN [9/25][142/157] Loss_G: 0.0104\n",
            "PRETRAIN [9/25][143/157] Loss_G: 0.0124\n",
            "PRETRAIN [9/25][144/157] Loss_G: 0.0126\n",
            "PRETRAIN [9/25][145/157] Loss_G: 0.0098\n",
            "PRETRAIN [9/25][146/157] Loss_G: 0.0090\n",
            "PRETRAIN [9/25][147/157] Loss_G: 0.0115\n",
            "PRETRAIN [9/25][148/157] Loss_G: 0.0079\n",
            "PRETRAIN [9/25][149/157] Loss_G: 0.0098\n",
            "PRETRAIN [9/25][150/157] Loss_G: 0.0097\n",
            "PRETRAIN [9/25][151/157] Loss_G: 0.0089\n",
            "PRETRAIN [9/25][152/157] Loss_G: 0.0097\n",
            "PRETRAIN [9/25][153/157] Loss_G: 0.0100\n",
            "PRETRAIN [9/25][154/157] Loss_G: 0.0112\n",
            "PRETRAIN [9/25][155/157] Loss_G: 0.0118\n",
            "PRETRAIN [9/25][156/157] Loss_G: 0.0144\n",
            "Time elapsed Epoch 9: 115 seconds\n",
            "PRETRAIN [10/25][0/157] Loss_G: 0.0112\n",
            "PRETRAIN [10/25][1/157] Loss_G: 0.0099\n",
            "PRETRAIN [10/25][2/157] Loss_G: 0.0105\n",
            "PRETRAIN [10/25][3/157] Loss_G: 0.0086\n",
            "PRETRAIN [10/25][4/157] Loss_G: 0.0088\n",
            "PRETRAIN [10/25][5/157] Loss_G: 0.0102\n",
            "PRETRAIN [10/25][6/157] Loss_G: 0.0102\n",
            "PRETRAIN [10/25][7/157] Loss_G: 0.0111\n",
            "PRETRAIN [10/25][8/157] Loss_G: 0.0100\n",
            "PRETRAIN [10/25][9/157] Loss_G: 0.0126\n",
            "PRETRAIN [10/25][10/157] Loss_G: 0.0092\n",
            "PRETRAIN [10/25][11/157] Loss_G: 0.0090\n",
            "PRETRAIN [10/25][12/157] Loss_G: 0.0088\n",
            "PRETRAIN [10/25][13/157] Loss_G: 0.0095\n",
            "PRETRAIN [10/25][14/157] Loss_G: 0.0080\n",
            "PRETRAIN [10/25][15/157] Loss_G: 0.0109\n",
            "PRETRAIN [10/25][16/157] Loss_G: 0.0101\n",
            "PRETRAIN [10/25][17/157] Loss_G: 0.0102\n",
            "PRETRAIN [10/25][18/157] Loss_G: 0.0100\n",
            "PRETRAIN [10/25][19/157] Loss_G: 0.0080\n",
            "PRETRAIN [10/25][20/157] Loss_G: 0.0129\n",
            "PRETRAIN [10/25][21/157] Loss_G: 0.0103\n",
            "PRETRAIN [10/25][22/157] Loss_G: 0.0119\n",
            "PRETRAIN [10/25][23/157] Loss_G: 0.0126\n",
            "PRETRAIN [10/25][24/157] Loss_G: 0.0109\n",
            "PRETRAIN [10/25][25/157] Loss_G: 0.0123\n",
            "PRETRAIN [10/25][26/157] Loss_G: 0.0097\n",
            "PRETRAIN [10/25][27/157] Loss_G: 0.0129\n",
            "PRETRAIN [10/25][28/157] Loss_G: 0.0128\n",
            "PRETRAIN [10/25][29/157] Loss_G: 0.0099\n",
            "PRETRAIN [10/25][30/157] Loss_G: 0.0104\n",
            "PRETRAIN [10/25][31/157] Loss_G: 0.0093\n",
            "PRETRAIN [10/25][32/157] Loss_G: 0.0093\n",
            "PRETRAIN [10/25][33/157] Loss_G: 0.0090\n",
            "PRETRAIN [10/25][34/157] Loss_G: 0.0098\n",
            "PRETRAIN [10/25][35/157] Loss_G: 0.0083\n",
            "PRETRAIN [10/25][36/157] Loss_G: 0.0111\n",
            "PRETRAIN [10/25][37/157] Loss_G: 0.0099\n",
            "PRETRAIN [10/25][38/157] Loss_G: 0.0110\n",
            "PRETRAIN [10/25][39/157] Loss_G: 0.0081\n",
            "PRETRAIN [10/25][40/157] Loss_G: 0.0088\n",
            "PRETRAIN [10/25][41/157] Loss_G: 0.0086\n",
            "PRETRAIN [10/25][42/157] Loss_G: 0.0088\n",
            "PRETRAIN [10/25][43/157] Loss_G: 0.0111\n",
            "PRETRAIN [10/25][44/157] Loss_G: 0.0087\n",
            "PRETRAIN [10/25][45/157] Loss_G: 0.0123\n",
            "PRETRAIN [10/25][46/157] Loss_G: 0.0105\n",
            "PRETRAIN [10/25][47/157] Loss_G: 0.0095\n",
            "PRETRAIN [10/25][48/157] Loss_G: 0.0118\n",
            "PRETRAIN [10/25][49/157] Loss_G: 0.0101\n",
            "PRETRAIN [10/25][50/157] Loss_G: 0.0108\n",
            "PRETRAIN [10/25][51/157] Loss_G: 0.0123\n",
            "PRETRAIN [10/25][52/157] Loss_G: 0.0093\n",
            "PRETRAIN [10/25][53/157] Loss_G: 0.0084\n",
            "PRETRAIN [10/25][54/157] Loss_G: 0.0092\n",
            "PRETRAIN [10/25][55/157] Loss_G: 0.0080\n",
            "PRETRAIN [10/25][56/157] Loss_G: 0.0115\n",
            "PRETRAIN [10/25][57/157] Loss_G: 0.0098\n",
            "PRETRAIN [10/25][58/157] Loss_G: 0.0098\n",
            "PRETRAIN [10/25][59/157] Loss_G: 0.0092\n",
            "PRETRAIN [10/25][60/157] Loss_G: 0.0102\n",
            "PRETRAIN [10/25][61/157] Loss_G: 0.0111\n",
            "PRETRAIN [10/25][62/157] Loss_G: 0.0088\n",
            "PRETRAIN [10/25][63/157] Loss_G: 0.0112\n",
            "PRETRAIN [10/25][64/157] Loss_G: 0.0097\n",
            "PRETRAIN [10/25][65/157] Loss_G: 0.0101\n",
            "PRETRAIN [10/25][66/157] Loss_G: 0.0113\n",
            "PRETRAIN [10/25][67/157] Loss_G: 0.0094\n",
            "PRETRAIN [10/25][68/157] Loss_G: 0.0091\n",
            "PRETRAIN [10/25][69/157] Loss_G: 0.0203\n",
            "PRETRAIN [10/25][70/157] Loss_G: 0.0418\n",
            "PRETRAIN [10/25][71/157] Loss_G: 0.0469\n",
            "PRETRAIN [10/25][72/157] Loss_G: 0.0256\n",
            "PRETRAIN [10/25][73/157] Loss_G: 0.0232\n",
            "PRETRAIN [10/25][74/157] Loss_G: 0.0221\n",
            "PRETRAIN [10/25][75/157] Loss_G: 0.0245\n",
            "PRETRAIN [10/25][76/157] Loss_G: 0.0231\n",
            "PRETRAIN [10/25][77/157] Loss_G: 0.0141\n",
            "PRETRAIN [10/25][78/157] Loss_G: 0.0130\n",
            "PRETRAIN [10/25][79/157] Loss_G: 0.0132\n",
            "PRETRAIN [10/25][80/157] Loss_G: 0.0122\n",
            "PRETRAIN [10/25][81/157] Loss_G: 0.0133\n",
            "PRETRAIN [10/25][82/157] Loss_G: 0.0142\n",
            "PRETRAIN [10/25][83/157] Loss_G: 0.0122\n",
            "PRETRAIN [10/25][84/157] Loss_G: 0.0126\n",
            "PRETRAIN [10/25][85/157] Loss_G: 0.0107\n",
            "PRETRAIN [10/25][86/157] Loss_G: 0.0108\n",
            "PRETRAIN [10/25][87/157] Loss_G: 0.0095\n",
            "PRETRAIN [10/25][88/157] Loss_G: 0.0099\n",
            "PRETRAIN [10/25][89/157] Loss_G: 0.0092\n",
            "PRETRAIN [10/25][90/157] Loss_G: 0.0124\n",
            "PRETRAIN [10/25][91/157] Loss_G: 0.0123\n",
            "PRETRAIN [10/25][92/157] Loss_G: 0.0108\n",
            "PRETRAIN [10/25][93/157] Loss_G: 0.0119\n",
            "PRETRAIN [10/25][94/157] Loss_G: 0.0096\n",
            "PRETRAIN [10/25][95/157] Loss_G: 0.0103\n",
            "PRETRAIN [10/25][96/157] Loss_G: 0.0095\n",
            "PRETRAIN [10/25][97/157] Loss_G: 0.0109\n",
            "PRETRAIN [10/25][98/157] Loss_G: 0.0117\n",
            "PRETRAIN [10/25][99/157] Loss_G: 0.0100\n",
            "PRETRAIN [10/25][100/157] Loss_G: 0.0104\n",
            "PRETRAIN [10/25][101/157] Loss_G: 0.0103\n",
            "PRETRAIN [10/25][102/157] Loss_G: 0.0088\n",
            "PRETRAIN [10/25][103/157] Loss_G: 0.0116\n",
            "PRETRAIN [10/25][104/157] Loss_G: 0.0094\n",
            "PRETRAIN [10/25][105/157] Loss_G: 0.0078\n",
            "PRETRAIN [10/25][106/157] Loss_G: 0.0106\n",
            "PRETRAIN [10/25][107/157] Loss_G: 0.0129\n",
            "PRETRAIN [10/25][108/157] Loss_G: 0.0122\n",
            "PRETRAIN [10/25][109/157] Loss_G: 0.0110\n",
            "PRETRAIN [10/25][110/157] Loss_G: 0.0118\n",
            "PRETRAIN [10/25][111/157] Loss_G: 0.0119\n",
            "PRETRAIN [10/25][112/157] Loss_G: 0.0115\n",
            "PRETRAIN [10/25][113/157] Loss_G: 0.0103\n",
            "PRETRAIN [10/25][114/157] Loss_G: 0.0122\n",
            "PRETRAIN [10/25][115/157] Loss_G: 0.0106\n",
            "PRETRAIN [10/25][116/157] Loss_G: 0.0117\n",
            "PRETRAIN [10/25][117/157] Loss_G: 0.0127\n",
            "PRETRAIN [10/25][118/157] Loss_G: 0.0130\n",
            "PRETRAIN [10/25][119/157] Loss_G: 0.0108\n",
            "PRETRAIN [10/25][120/157] Loss_G: 0.0093\n",
            "PRETRAIN [10/25][121/157] Loss_G: 0.0099\n",
            "PRETRAIN [10/25][122/157] Loss_G: 0.0117\n",
            "PRETRAIN [10/25][123/157] Loss_G: 0.0083\n",
            "PRETRAIN [10/25][124/157] Loss_G: 0.0090\n",
            "PRETRAIN [10/25][125/157] Loss_G: 0.0094\n",
            "PRETRAIN [10/25][126/157] Loss_G: 0.0100\n",
            "PRETRAIN [10/25][127/157] Loss_G: 0.0175\n",
            "PRETRAIN [10/25][128/157] Loss_G: 0.0171\n",
            "PRETRAIN [10/25][129/157] Loss_G: 0.0138\n",
            "PRETRAIN [10/25][130/157] Loss_G: 0.0152\n",
            "PRETRAIN [10/25][131/157] Loss_G: 0.0152\n",
            "PRETRAIN [10/25][132/157] Loss_G: 0.0134\n",
            "PRETRAIN [10/25][133/157] Loss_G: 0.0143\n",
            "PRETRAIN [10/25][134/157] Loss_G: 0.0122\n",
            "PRETRAIN [10/25][135/157] Loss_G: 0.0137\n",
            "PRETRAIN [10/25][136/157] Loss_G: 0.0125\n",
            "PRETRAIN [10/25][137/157] Loss_G: 0.0107\n",
            "PRETRAIN [10/25][138/157] Loss_G: 0.0117\n",
            "PRETRAIN [10/25][139/157] Loss_G: 0.0131\n",
            "PRETRAIN [10/25][140/157] Loss_G: 0.0107\n",
            "PRETRAIN [10/25][141/157] Loss_G: 0.0113\n",
            "PRETRAIN [10/25][142/157] Loss_G: 0.0115\n",
            "PRETRAIN [10/25][143/157] Loss_G: 0.0109\n",
            "PRETRAIN [10/25][144/157] Loss_G: 0.0123\n",
            "PRETRAIN [10/25][145/157] Loss_G: 0.0125\n",
            "PRETRAIN [10/25][146/157] Loss_G: 0.0121\n",
            "PRETRAIN [10/25][147/157] Loss_G: 0.0119\n",
            "PRETRAIN [10/25][148/157] Loss_G: 0.0101\n",
            "PRETRAIN [10/25][149/157] Loss_G: 0.0103\n",
            "PRETRAIN [10/25][150/157] Loss_G: 0.0163\n",
            "PRETRAIN [10/25][151/157] Loss_G: 0.0158\n",
            "PRETRAIN [10/25][152/157] Loss_G: 0.0128\n",
            "PRETRAIN [10/25][153/157] Loss_G: 0.0144\n",
            "PRETRAIN [10/25][154/157] Loss_G: 0.0132\n",
            "PRETRAIN [10/25][155/157] Loss_G: 0.0134\n",
            "PRETRAIN [10/25][156/157] Loss_G: 0.0130\n",
            "Time elapsed Epoch 10: 115 seconds\n",
            "PRETRAIN [11/25][0/157] Loss_G: 0.0124\n",
            "PRETRAIN [11/25][1/157] Loss_G: 0.0133\n",
            "PRETRAIN [11/25][2/157] Loss_G: 0.0145\n",
            "PRETRAIN [11/25][3/157] Loss_G: 0.0135\n",
            "PRETRAIN [11/25][4/157] Loss_G: 0.0107\n",
            "PRETRAIN [11/25][5/157] Loss_G: 0.0083\n",
            "PRETRAIN [11/25][6/157] Loss_G: 0.0105\n",
            "PRETRAIN [11/25][7/157] Loss_G: 0.0119\n",
            "PRETRAIN [11/25][8/157] Loss_G: 0.0106\n",
            "PRETRAIN [11/25][9/157] Loss_G: 0.0102\n",
            "PRETRAIN [11/25][10/157] Loss_G: 0.0113\n",
            "PRETRAIN [11/25][11/157] Loss_G: 0.0095\n",
            "PRETRAIN [11/25][12/157] Loss_G: 0.0110\n",
            "PRETRAIN [11/25][13/157] Loss_G: 0.0097\n",
            "PRETRAIN [11/25][14/157] Loss_G: 0.0122\n",
            "PRETRAIN [11/25][15/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][16/157] Loss_G: 0.0093\n",
            "PRETRAIN [11/25][17/157] Loss_G: 0.0118\n",
            "PRETRAIN [11/25][18/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][19/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][20/157] Loss_G: 0.0088\n",
            "PRETRAIN [11/25][21/157] Loss_G: 0.0086\n",
            "PRETRAIN [11/25][22/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][23/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][24/157] Loss_G: 0.0095\n",
            "PRETRAIN [11/25][25/157] Loss_G: 0.0103\n",
            "PRETRAIN [11/25][26/157] Loss_G: 0.0081\n",
            "PRETRAIN [11/25][27/157] Loss_G: 0.0103\n",
            "PRETRAIN [11/25][28/157] Loss_G: 0.0100\n",
            "PRETRAIN [11/25][29/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][30/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][31/157] Loss_G: 0.0114\n",
            "PRETRAIN [11/25][32/157] Loss_G: 0.0196\n",
            "PRETRAIN [11/25][33/157] Loss_G: 0.0130\n",
            "PRETRAIN [11/25][34/157] Loss_G: 0.0107\n",
            "PRETRAIN [11/25][35/157] Loss_G: 0.0109\n",
            "PRETRAIN [11/25][36/157] Loss_G: 0.0102\n",
            "PRETRAIN [11/25][37/157] Loss_G: 0.0094\n",
            "PRETRAIN [11/25][38/157] Loss_G: 0.0103\n",
            "PRETRAIN [11/25][39/157] Loss_G: 0.0093\n",
            "PRETRAIN [11/25][40/157] Loss_G: 0.0097\n",
            "PRETRAIN [11/25][41/157] Loss_G: 0.0102\n",
            "PRETRAIN [11/25][42/157] Loss_G: 0.0099\n",
            "PRETRAIN [11/25][43/157] Loss_G: 0.0119\n",
            "PRETRAIN [11/25][44/157] Loss_G: 0.0093\n",
            "PRETRAIN [11/25][45/157] Loss_G: 0.0095\n",
            "PRETRAIN [11/25][46/157] Loss_G: 0.0091\n",
            "PRETRAIN [11/25][47/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][48/157] Loss_G: 0.0102\n",
            "PRETRAIN [11/25][49/157] Loss_G: 0.0094\n",
            "PRETRAIN [11/25][50/157] Loss_G: 0.0098\n",
            "PRETRAIN [11/25][51/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][52/157] Loss_G: 0.0098\n",
            "PRETRAIN [11/25][53/157] Loss_G: 0.0088\n",
            "PRETRAIN [11/25][54/157] Loss_G: 0.0112\n",
            "PRETRAIN [11/25][55/157] Loss_G: 0.0086\n",
            "PRETRAIN [11/25][56/157] Loss_G: 0.0096\n",
            "PRETRAIN [11/25][57/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][58/157] Loss_G: 0.0088\n",
            "PRETRAIN [11/25][59/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][60/157] Loss_G: 0.0081\n",
            "PRETRAIN [11/25][61/157] Loss_G: 0.0107\n",
            "PRETRAIN [11/25][62/157] Loss_G: 0.0079\n",
            "PRETRAIN [11/25][63/157] Loss_G: 0.0104\n",
            "PRETRAIN [11/25][64/157] Loss_G: 0.0077\n",
            "PRETRAIN [11/25][65/157] Loss_G: 0.0098\n",
            "PRETRAIN [11/25][66/157] Loss_G: 0.0094\n",
            "PRETRAIN [11/25][67/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][68/157] Loss_G: 0.0083\n",
            "PRETRAIN [11/25][69/157] Loss_G: 0.0102\n",
            "PRETRAIN [11/25][70/157] Loss_G: 0.0079\n",
            "PRETRAIN [11/25][71/157] Loss_G: 0.0091\n",
            "PRETRAIN [11/25][72/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][73/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][74/157] Loss_G: 0.0086\n",
            "PRETRAIN [11/25][75/157] Loss_G: 0.0091\n",
            "PRETRAIN [11/25][76/157] Loss_G: 0.0095\n",
            "PRETRAIN [11/25][77/157] Loss_G: 0.0088\n",
            "PRETRAIN [11/25][78/157] Loss_G: 0.0089\n",
            "PRETRAIN [11/25][79/157] Loss_G: 0.0084\n",
            "PRETRAIN [11/25][80/157] Loss_G: 0.0078\n",
            "PRETRAIN [11/25][81/157] Loss_G: 0.0083\n",
            "PRETRAIN [11/25][82/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][83/157] Loss_G: 0.0091\n",
            "PRETRAIN [11/25][84/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][85/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][86/157] Loss_G: 0.0096\n",
            "PRETRAIN [11/25][87/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][88/157] Loss_G: 0.0160\n",
            "PRETRAIN [11/25][89/157] Loss_G: 0.0119\n",
            "PRETRAIN [11/25][90/157] Loss_G: 0.0115\n",
            "PRETRAIN [11/25][91/157] Loss_G: 0.0108\n",
            "PRETRAIN [11/25][92/157] Loss_G: 0.0102\n",
            "PRETRAIN [11/25][93/157] Loss_G: 0.0094\n",
            "PRETRAIN [11/25][94/157] Loss_G: 0.0083\n",
            "PRETRAIN [11/25][95/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][96/157] Loss_G: 0.0097\n",
            "PRETRAIN [11/25][97/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][98/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][99/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][100/157] Loss_G: 0.0091\n",
            "PRETRAIN [11/25][101/157] Loss_G: 0.0102\n",
            "PRETRAIN [11/25][102/157] Loss_G: 0.0078\n",
            "PRETRAIN [11/25][103/157] Loss_G: 0.0093\n",
            "PRETRAIN [11/25][104/157] Loss_G: 0.0100\n",
            "PRETRAIN [11/25][105/157] Loss_G: 0.0096\n",
            "PRETRAIN [11/25][106/157] Loss_G: 0.0089\n",
            "PRETRAIN [11/25][107/157] Loss_G: 0.0104\n",
            "PRETRAIN [11/25][108/157] Loss_G: 0.0094\n",
            "PRETRAIN [11/25][109/157] Loss_G: 0.0097\n",
            "PRETRAIN [11/25][110/157] Loss_G: 0.0107\n",
            "PRETRAIN [11/25][111/157] Loss_G: 0.0093\n",
            "PRETRAIN [11/25][112/157] Loss_G: 0.0099\n",
            "PRETRAIN [11/25][113/157] Loss_G: 0.0099\n",
            "PRETRAIN [11/25][114/157] Loss_G: 0.0077\n",
            "PRETRAIN [11/25][115/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][116/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][117/157] Loss_G: 0.0096\n",
            "PRETRAIN [11/25][118/157] Loss_G: 0.0111\n",
            "PRETRAIN [11/25][119/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][120/157] Loss_G: 0.0084\n",
            "PRETRAIN [11/25][121/157] Loss_G: 0.0101\n",
            "PRETRAIN [11/25][122/157] Loss_G: 0.0081\n",
            "PRETRAIN [11/25][123/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][124/157] Loss_G: 0.0091\n",
            "PRETRAIN [11/25][125/157] Loss_G: 0.0076\n",
            "PRETRAIN [11/25][126/157] Loss_G: 0.0096\n",
            "PRETRAIN [11/25][127/157] Loss_G: 0.0114\n",
            "PRETRAIN [11/25][128/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][129/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][130/157] Loss_G: 0.0081\n",
            "PRETRAIN [11/25][131/157] Loss_G: 0.0090\n",
            "PRETRAIN [11/25][132/157] Loss_G: 0.0086\n",
            "PRETRAIN [11/25][133/157] Loss_G: 0.0098\n",
            "PRETRAIN [11/25][134/157] Loss_G: 0.0079\n",
            "PRETRAIN [11/25][135/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][136/157] Loss_G: 0.0098\n",
            "PRETRAIN [11/25][137/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][138/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][139/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][140/157] Loss_G: 0.0097\n",
            "PRETRAIN [11/25][141/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][142/157] Loss_G: 0.0106\n",
            "PRETRAIN [11/25][143/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][144/157] Loss_G: 0.0113\n",
            "PRETRAIN [11/25][145/157] Loss_G: 0.0113\n",
            "PRETRAIN [11/25][146/157] Loss_G: 0.0085\n",
            "PRETRAIN [11/25][147/157] Loss_G: 0.0083\n",
            "PRETRAIN [11/25][148/157] Loss_G: 0.0092\n",
            "PRETRAIN [11/25][149/157] Loss_G: 0.0098\n",
            "PRETRAIN [11/25][150/157] Loss_G: 0.0083\n",
            "PRETRAIN [11/25][151/157] Loss_G: 0.0087\n",
            "PRETRAIN [11/25][152/157] Loss_G: 0.0091\n",
            "PRETRAIN [11/25][153/157] Loss_G: 0.0094\n",
            "PRETRAIN [11/25][154/157] Loss_G: 0.0094\n",
            "PRETRAIN [11/25][155/157] Loss_G: 0.0095\n",
            "PRETRAIN [11/25][156/157] Loss_G: 0.0140\n",
            "Time elapsed Epoch 11: 115 seconds\n",
            "PRETRAIN [12/25][0/157] Loss_G: 0.0090\n",
            "PRETRAIN [12/25][1/157] Loss_G: 0.0088\n",
            "PRETRAIN [12/25][2/157] Loss_G: 0.0106\n",
            "PRETRAIN [12/25][3/157] Loss_G: 0.0079\n",
            "PRETRAIN [12/25][4/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][5/157] Loss_G: 0.0100\n",
            "PRETRAIN [12/25][6/157] Loss_G: 0.0081\n",
            "PRETRAIN [12/25][7/157] Loss_G: 0.0088\n",
            "PRETRAIN [12/25][8/157] Loss_G: 0.0100\n",
            "PRETRAIN [12/25][9/157] Loss_G: 0.0108\n",
            "PRETRAIN [12/25][10/157] Loss_G: 0.0101\n",
            "PRETRAIN [12/25][11/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][12/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][13/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][14/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][15/157] Loss_G: 0.0090\n",
            "PRETRAIN [12/25][16/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][17/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][18/157] Loss_G: 0.0084\n",
            "PRETRAIN [12/25][19/157] Loss_G: 0.0075\n",
            "PRETRAIN [12/25][20/157] Loss_G: 0.0086\n",
            "PRETRAIN [12/25][21/157] Loss_G: 0.0084\n",
            "PRETRAIN [12/25][22/157] Loss_G: 0.0084\n",
            "PRETRAIN [12/25][23/157] Loss_G: 0.0071\n",
            "PRETRAIN [12/25][24/157] Loss_G: 0.0082\n",
            "PRETRAIN [12/25][25/157] Loss_G: 0.0092\n",
            "PRETRAIN [12/25][26/157] Loss_G: 0.0092\n",
            "PRETRAIN [12/25][27/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][28/157] Loss_G: 0.0077\n",
            "PRETRAIN [12/25][29/157] Loss_G: 0.0073\n",
            "PRETRAIN [12/25][30/157] Loss_G: 0.0081\n",
            "PRETRAIN [12/25][31/157] Loss_G: 0.0095\n",
            "PRETRAIN [12/25][32/157] Loss_G: 0.0105\n",
            "PRETRAIN [12/25][33/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][34/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][35/157] Loss_G: 0.0109\n",
            "PRETRAIN [12/25][36/157] Loss_G: 0.0131\n",
            "PRETRAIN [12/25][37/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][38/157] Loss_G: 0.0100\n",
            "PRETRAIN [12/25][39/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][40/157] Loss_G: 0.0101\n",
            "PRETRAIN [12/25][41/157] Loss_G: 0.0081\n",
            "PRETRAIN [12/25][42/157] Loss_G: 0.0095\n",
            "PRETRAIN [12/25][43/157] Loss_G: 0.0079\n",
            "PRETRAIN [12/25][44/157] Loss_G: 0.0093\n",
            "PRETRAIN [12/25][45/157] Loss_G: 0.0101\n",
            "PRETRAIN [12/25][46/157] Loss_G: 0.0102\n",
            "PRETRAIN [12/25][47/157] Loss_G: 0.0097\n",
            "PRETRAIN [12/25][48/157] Loss_G: 0.0105\n",
            "PRETRAIN [12/25][49/157] Loss_G: 0.0089\n",
            "PRETRAIN [12/25][50/157] Loss_G: 0.0083\n",
            "PRETRAIN [12/25][51/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][52/157] Loss_G: 0.0080\n",
            "PRETRAIN [12/25][53/157] Loss_G: 0.0100\n",
            "PRETRAIN [12/25][54/157] Loss_G: 0.0086\n",
            "PRETRAIN [12/25][55/157] Loss_G: 0.0092\n",
            "PRETRAIN [12/25][56/157] Loss_G: 0.0096\n",
            "PRETRAIN [12/25][57/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][58/157] Loss_G: 0.0099\n",
            "PRETRAIN [12/25][59/157] Loss_G: 0.0117\n",
            "PRETRAIN [12/25][60/157] Loss_G: 0.0083\n",
            "PRETRAIN [12/25][61/157] Loss_G: 0.0103\n",
            "PRETRAIN [12/25][62/157] Loss_G: 0.0095\n",
            "PRETRAIN [12/25][63/157] Loss_G: 0.0081\n",
            "PRETRAIN [12/25][64/157] Loss_G: 0.0084\n",
            "PRETRAIN [12/25][65/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][66/157] Loss_G: 0.0070\n",
            "PRETRAIN [12/25][67/157] Loss_G: 0.0089\n",
            "PRETRAIN [12/25][68/157] Loss_G: 0.0082\n",
            "PRETRAIN [12/25][69/157] Loss_G: 0.0074\n",
            "PRETRAIN [12/25][70/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][71/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][72/157] Loss_G: 0.0077\n",
            "PRETRAIN [12/25][73/157] Loss_G: 0.0098\n",
            "PRETRAIN [12/25][74/157] Loss_G: 0.0086\n",
            "PRETRAIN [12/25][75/157] Loss_G: 0.0116\n",
            "PRETRAIN [12/25][76/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][77/157] Loss_G: 0.0081\n",
            "PRETRAIN [12/25][78/157] Loss_G: 0.0113\n",
            "PRETRAIN [12/25][79/157] Loss_G: 0.0096\n",
            "PRETRAIN [12/25][80/157] Loss_G: 0.0098\n",
            "PRETRAIN [12/25][81/157] Loss_G: 0.0084\n",
            "PRETRAIN [12/25][82/157] Loss_G: 0.0102\n",
            "PRETRAIN [12/25][83/157] Loss_G: 0.0098\n",
            "PRETRAIN [12/25][84/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][85/157] Loss_G: 0.0089\n",
            "PRETRAIN [12/25][86/157] Loss_G: 0.0101\n",
            "PRETRAIN [12/25][87/157] Loss_G: 0.0082\n",
            "PRETRAIN [12/25][88/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][89/157] Loss_G: 0.0086\n",
            "PRETRAIN [12/25][90/157] Loss_G: 0.0089\n",
            "PRETRAIN [12/25][91/157] Loss_G: 0.0086\n",
            "PRETRAIN [12/25][92/157] Loss_G: 0.0107\n",
            "PRETRAIN [12/25][93/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][94/157] Loss_G: 0.0084\n",
            "PRETRAIN [12/25][95/157] Loss_G: 0.0098\n",
            "PRETRAIN [12/25][96/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][97/157] Loss_G: 0.0115\n",
            "PRETRAIN [12/25][98/157] Loss_G: 0.0088\n",
            "PRETRAIN [12/25][99/157] Loss_G: 0.0113\n",
            "PRETRAIN [12/25][100/157] Loss_G: 0.0081\n",
            "PRETRAIN [12/25][101/157] Loss_G: 0.0077\n",
            "PRETRAIN [12/25][102/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][103/157] Loss_G: 0.0086\n",
            "PRETRAIN [12/25][104/157] Loss_G: 0.0066\n",
            "PRETRAIN [12/25][105/157] Loss_G: 0.0079\n",
            "PRETRAIN [12/25][106/157] Loss_G: 0.0097\n",
            "PRETRAIN [12/25][107/157] Loss_G: 0.0098\n",
            "PRETRAIN [12/25][108/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][109/157] Loss_G: 0.0084\n",
            "PRETRAIN [12/25][110/157] Loss_G: 0.0099\n",
            "PRETRAIN [12/25][111/157] Loss_G: 0.0076\n",
            "PRETRAIN [12/25][112/157] Loss_G: 0.0096\n",
            "PRETRAIN [12/25][113/157] Loss_G: 0.0096\n",
            "PRETRAIN [12/25][114/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][115/157] Loss_G: 0.0100\n",
            "PRETRAIN [12/25][116/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][117/157] Loss_G: 0.0072\n",
            "PRETRAIN [12/25][118/157] Loss_G: 0.0069\n",
            "PRETRAIN [12/25][119/157] Loss_G: 0.0109\n",
            "PRETRAIN [12/25][120/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][121/157] Loss_G: 0.0083\n",
            "PRETRAIN [12/25][122/157] Loss_G: 0.0082\n",
            "PRETRAIN [12/25][123/157] Loss_G: 0.0092\n",
            "PRETRAIN [12/25][124/157] Loss_G: 0.0098\n",
            "PRETRAIN [12/25][125/157] Loss_G: 0.0090\n",
            "PRETRAIN [12/25][126/157] Loss_G: 0.0075\n",
            "PRETRAIN [12/25][127/157] Loss_G: 0.0080\n",
            "PRETRAIN [12/25][128/157] Loss_G: 0.0082\n",
            "PRETRAIN [12/25][129/157] Loss_G: 0.0110\n",
            "PRETRAIN [12/25][130/157] Loss_G: 0.0081\n",
            "PRETRAIN [12/25][131/157] Loss_G: 0.0108\n",
            "PRETRAIN [12/25][132/157] Loss_G: 0.0086\n",
            "PRETRAIN [12/25][133/157] Loss_G: 0.0083\n",
            "PRETRAIN [12/25][134/157] Loss_G: 0.0098\n",
            "PRETRAIN [12/25][135/157] Loss_G: 0.0093\n",
            "PRETRAIN [12/25][136/157] Loss_G: 0.0083\n",
            "PRETRAIN [12/25][137/157] Loss_G: 0.0099\n",
            "PRETRAIN [12/25][138/157] Loss_G: 0.0091\n",
            "PRETRAIN [12/25][139/157] Loss_G: 0.0105\n",
            "PRETRAIN [12/25][140/157] Loss_G: 0.0094\n",
            "PRETRAIN [12/25][141/157] Loss_G: 0.0103\n",
            "PRETRAIN [12/25][142/157] Loss_G: 0.0082\n",
            "PRETRAIN [12/25][143/157] Loss_G: 0.0128\n",
            "PRETRAIN [12/25][144/157] Loss_G: 0.0089\n",
            "PRETRAIN [12/25][145/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][146/157] Loss_G: 0.0078\n",
            "PRETRAIN [12/25][147/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][148/157] Loss_G: 0.0085\n",
            "PRETRAIN [12/25][149/157] Loss_G: 0.0096\n",
            "PRETRAIN [12/25][150/157] Loss_G: 0.0088\n",
            "PRETRAIN [12/25][151/157] Loss_G: 0.0080\n",
            "PRETRAIN [12/25][152/157] Loss_G: 0.0073\n",
            "PRETRAIN [12/25][153/157] Loss_G: 0.0095\n",
            "PRETRAIN [12/25][154/157] Loss_G: 0.0087\n",
            "PRETRAIN [12/25][155/157] Loss_G: 0.0097\n",
            "PRETRAIN [12/25][156/157] Loss_G: 0.0103\n",
            "Time elapsed Epoch 12: 115 seconds\n",
            "PRETRAIN [13/25][0/157] Loss_G: 0.0094\n",
            "PRETRAIN [13/25][1/157] Loss_G: 0.0093\n",
            "PRETRAIN [13/25][2/157] Loss_G: 0.0103\n",
            "PRETRAIN [13/25][3/157] Loss_G: 0.0122\n",
            "PRETRAIN [13/25][4/157] Loss_G: 0.0105\n",
            "PRETRAIN [13/25][5/157] Loss_G: 0.0091\n",
            "PRETRAIN [13/25][6/157] Loss_G: 0.0072\n",
            "PRETRAIN [13/25][7/157] Loss_G: 0.0101\n",
            "PRETRAIN [13/25][8/157] Loss_G: 0.0104\n",
            "PRETRAIN [13/25][9/157] Loss_G: 0.0079\n",
            "PRETRAIN [13/25][10/157] Loss_G: 0.0070\n",
            "PRETRAIN [13/25][11/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][12/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][13/157] Loss_G: 0.0079\n",
            "PRETRAIN [13/25][14/157] Loss_G: 0.0085\n",
            "PRETRAIN [13/25][15/157] Loss_G: 0.0095\n",
            "PRETRAIN [13/25][16/157] Loss_G: 0.0095\n",
            "PRETRAIN [13/25][17/157] Loss_G: 0.0091\n",
            "PRETRAIN [13/25][18/157] Loss_G: 0.0081\n",
            "PRETRAIN [13/25][19/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][20/157] Loss_G: 0.0087\n",
            "PRETRAIN [13/25][21/157] Loss_G: 0.0087\n",
            "PRETRAIN [13/25][22/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][23/157] Loss_G: 0.0082\n",
            "PRETRAIN [13/25][24/157] Loss_G: 0.0095\n",
            "PRETRAIN [13/25][25/157] Loss_G: 0.0115\n",
            "PRETRAIN [13/25][26/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][27/157] Loss_G: 0.0085\n",
            "PRETRAIN [13/25][28/157] Loss_G: 0.0073\n",
            "PRETRAIN [13/25][29/157] Loss_G: 0.0079\n",
            "PRETRAIN [13/25][30/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][31/157] Loss_G: 0.0097\n",
            "PRETRAIN [13/25][32/157] Loss_G: 0.0079\n",
            "PRETRAIN [13/25][33/157] Loss_G: 0.0089\n",
            "PRETRAIN [13/25][34/157] Loss_G: 0.0078\n",
            "PRETRAIN [13/25][35/157] Loss_G: 0.0087\n",
            "PRETRAIN [13/25][36/157] Loss_G: 0.0085\n",
            "PRETRAIN [13/25][37/157] Loss_G: 0.0081\n",
            "PRETRAIN [13/25][38/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][39/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][40/157] Loss_G: 0.0080\n",
            "PRETRAIN [13/25][41/157] Loss_G: 0.0091\n",
            "PRETRAIN [13/25][42/157] Loss_G: 0.0097\n",
            "PRETRAIN [13/25][43/157] Loss_G: 0.0119\n",
            "PRETRAIN [13/25][44/157] Loss_G: 0.0109\n",
            "PRETRAIN [13/25][45/157] Loss_G: 0.0089\n",
            "PRETRAIN [13/25][46/157] Loss_G: 0.0079\n",
            "PRETRAIN [13/25][47/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][48/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][49/157] Loss_G: 0.0078\n",
            "PRETRAIN [13/25][50/157] Loss_G: 0.0082\n",
            "PRETRAIN [13/25][51/157] Loss_G: 0.0078\n",
            "PRETRAIN [13/25][52/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][53/157] Loss_G: 0.0090\n",
            "PRETRAIN [13/25][54/157] Loss_G: 0.0103\n",
            "PRETRAIN [13/25][55/157] Loss_G: 0.0089\n",
            "PRETRAIN [13/25][56/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][57/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][58/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][59/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][60/157] Loss_G: 0.0084\n",
            "PRETRAIN [13/25][61/157] Loss_G: 0.0079\n",
            "PRETRAIN [13/25][62/157] Loss_G: 0.0126\n",
            "PRETRAIN [13/25][63/157] Loss_G: 0.0091\n",
            "PRETRAIN [13/25][64/157] Loss_G: 0.0104\n",
            "PRETRAIN [13/25][65/157] Loss_G: 0.0108\n",
            "PRETRAIN [13/25][66/157] Loss_G: 0.0076\n",
            "PRETRAIN [13/25][67/157] Loss_G: 0.0089\n",
            "PRETRAIN [13/25][68/157] Loss_G: 0.0102\n",
            "PRETRAIN [13/25][69/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][70/157] Loss_G: 0.0111\n",
            "PRETRAIN [13/25][71/157] Loss_G: 0.0102\n",
            "PRETRAIN [13/25][72/157] Loss_G: 0.0083\n",
            "PRETRAIN [13/25][73/157] Loss_G: 0.0115\n",
            "PRETRAIN [13/25][74/157] Loss_G: 0.0079\n",
            "PRETRAIN [13/25][75/157] Loss_G: 0.0098\n",
            "PRETRAIN [13/25][76/157] Loss_G: 0.0096\n",
            "PRETRAIN [13/25][77/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][78/157] Loss_G: 0.0087\n",
            "PRETRAIN [13/25][79/157] Loss_G: 0.0092\n",
            "PRETRAIN [13/25][80/157] Loss_G: 0.0083\n",
            "PRETRAIN [13/25][81/157] Loss_G: 0.0080\n",
            "PRETRAIN [13/25][82/157] Loss_G: 0.0089\n",
            "PRETRAIN [13/25][83/157] Loss_G: 0.0091\n",
            "PRETRAIN [13/25][84/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][85/157] Loss_G: 0.0092\n",
            "PRETRAIN [13/25][86/157] Loss_G: 0.0081\n",
            "PRETRAIN [13/25][87/157] Loss_G: 0.0097\n",
            "PRETRAIN [13/25][88/157] Loss_G: 0.0074\n",
            "PRETRAIN [13/25][89/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][90/157] Loss_G: 0.0083\n",
            "PRETRAIN [13/25][91/157] Loss_G: 0.0104\n",
            "PRETRAIN [13/25][92/157] Loss_G: 0.0098\n",
            "PRETRAIN [13/25][93/157] Loss_G: 0.0087\n",
            "PRETRAIN [13/25][94/157] Loss_G: 0.0090\n",
            "PRETRAIN [13/25][95/157] Loss_G: 0.0074\n",
            "PRETRAIN [13/25][96/157] Loss_G: 0.0074\n",
            "PRETRAIN [13/25][97/157] Loss_G: 0.0168\n",
            "PRETRAIN [13/25][98/157] Loss_G: 0.0162\n",
            "PRETRAIN [13/25][99/157] Loss_G: 0.0174\n",
            "PRETRAIN [13/25][100/157] Loss_G: 0.0191\n",
            "PRETRAIN [13/25][101/157] Loss_G: 0.0173\n",
            "PRETRAIN [13/25][102/157] Loss_G: 0.0155\n",
            "PRETRAIN [13/25][103/157] Loss_G: 0.0168\n",
            "PRETRAIN [13/25][104/157] Loss_G: 0.0121\n",
            "PRETRAIN [13/25][105/157] Loss_G: 0.0128\n",
            "PRETRAIN [13/25][106/157] Loss_G: 0.0114\n",
            "PRETRAIN [13/25][107/157] Loss_G: 0.0115\n",
            "PRETRAIN [13/25][108/157] Loss_G: 0.0111\n",
            "PRETRAIN [13/25][109/157] Loss_G: 0.0081\n",
            "PRETRAIN [13/25][110/157] Loss_G: 0.0112\n",
            "PRETRAIN [13/25][111/157] Loss_G: 0.0110\n",
            "PRETRAIN [13/25][112/157] Loss_G: 0.0097\n",
            "PRETRAIN [13/25][113/157] Loss_G: 0.0131\n",
            "PRETRAIN [13/25][114/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][115/157] Loss_G: 0.0088\n",
            "PRETRAIN [13/25][116/157] Loss_G: 0.0098\n",
            "PRETRAIN [13/25][117/157] Loss_G: 0.0083\n",
            "PRETRAIN [13/25][118/157] Loss_G: 0.0099\n",
            "PRETRAIN [13/25][119/157] Loss_G: 0.0118\n",
            "PRETRAIN [13/25][120/157] Loss_G: 0.0092\n",
            "PRETRAIN [13/25][121/157] Loss_G: 0.0080\n",
            "PRETRAIN [13/25][122/157] Loss_G: 0.0080\n",
            "PRETRAIN [13/25][123/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][124/157] Loss_G: 0.0080\n",
            "PRETRAIN [13/25][125/157] Loss_G: 0.0089\n",
            "PRETRAIN [13/25][126/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][127/157] Loss_G: 0.0111\n",
            "PRETRAIN [13/25][128/157] Loss_G: 0.0126\n",
            "PRETRAIN [13/25][129/157] Loss_G: 0.0113\n",
            "PRETRAIN [13/25][130/157] Loss_G: 0.0103\n",
            "PRETRAIN [13/25][131/157] Loss_G: 0.0141\n",
            "PRETRAIN [13/25][132/157] Loss_G: 0.0105\n",
            "PRETRAIN [13/25][133/157] Loss_G: 0.0095\n",
            "PRETRAIN [13/25][134/157] Loss_G: 0.0087\n",
            "PRETRAIN [13/25][135/157] Loss_G: 0.0109\n",
            "PRETRAIN [13/25][136/157] Loss_G: 0.0086\n",
            "PRETRAIN [13/25][137/157] Loss_G: 0.0109\n",
            "PRETRAIN [13/25][138/157] Loss_G: 0.0095\n",
            "PRETRAIN [13/25][139/157] Loss_G: 0.0077\n",
            "PRETRAIN [13/25][140/157] Loss_G: 0.0078\n",
            "PRETRAIN [13/25][141/157] Loss_G: 0.0099\n",
            "PRETRAIN [13/25][142/157] Loss_G: 0.0080\n",
            "PRETRAIN [13/25][143/157] Loss_G: 0.0089\n",
            "PRETRAIN [13/25][144/157] Loss_G: 0.0085\n",
            "PRETRAIN [13/25][145/157] Loss_G: 0.0082\n",
            "PRETRAIN [13/25][146/157] Loss_G: 0.0094\n",
            "PRETRAIN [13/25][147/157] Loss_G: 0.0078\n",
            "PRETRAIN [13/25][148/157] Loss_G: 0.0099\n",
            "PRETRAIN [13/25][149/157] Loss_G: 0.0104\n",
            "PRETRAIN [13/25][150/157] Loss_G: 0.0103\n",
            "PRETRAIN [13/25][151/157] Loss_G: 0.0101\n",
            "PRETRAIN [13/25][152/157] Loss_G: 0.0102\n",
            "PRETRAIN [13/25][153/157] Loss_G: 0.0097\n",
            "PRETRAIN [13/25][154/157] Loss_G: 0.0076\n",
            "PRETRAIN [13/25][155/157] Loss_G: 0.0106\n",
            "PRETRAIN [13/25][156/157] Loss_G: 0.0096\n",
            "Time elapsed Epoch 13: 115 seconds\n",
            "PRETRAIN [14/25][0/157] Loss_G: 0.0091\n",
            "PRETRAIN [14/25][1/157] Loss_G: 0.0094\n",
            "PRETRAIN [14/25][2/157] Loss_G: 0.0074\n",
            "PRETRAIN [14/25][3/157] Loss_G: 0.0091\n",
            "PRETRAIN [14/25][4/157] Loss_G: 0.0075\n",
            "PRETRAIN [14/25][5/157] Loss_G: 0.0095\n",
            "PRETRAIN [14/25][6/157] Loss_G: 0.0093\n",
            "PRETRAIN [14/25][7/157] Loss_G: 0.0092\n",
            "PRETRAIN [14/25][8/157] Loss_G: 0.0090\n",
            "PRETRAIN [14/25][9/157] Loss_G: 0.0078\n",
            "PRETRAIN [14/25][10/157] Loss_G: 0.0091\n",
            "PRETRAIN [14/25][11/157] Loss_G: 0.0079\n",
            "PRETRAIN [14/25][12/157] Loss_G: 0.0096\n",
            "PRETRAIN [14/25][13/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][14/157] Loss_G: 0.0097\n",
            "PRETRAIN [14/25][15/157] Loss_G: 0.0096\n",
            "PRETRAIN [14/25][16/157] Loss_G: 0.0153\n",
            "PRETRAIN [14/25][17/157] Loss_G: 0.0115\n",
            "PRETRAIN [14/25][18/157] Loss_G: 0.0087\n",
            "PRETRAIN [14/25][19/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][20/157] Loss_G: 0.0086\n",
            "PRETRAIN [14/25][21/157] Loss_G: 0.0087\n",
            "PRETRAIN [14/25][22/157] Loss_G: 0.0086\n",
            "PRETRAIN [14/25][23/157] Loss_G: 0.0085\n",
            "PRETRAIN [14/25][24/157] Loss_G: 0.0084\n",
            "PRETRAIN [14/25][25/157] Loss_G: 0.0094\n",
            "PRETRAIN [14/25][26/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][27/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][28/157] Loss_G: 0.0107\n",
            "PRETRAIN [14/25][29/157] Loss_G: 0.0091\n",
            "PRETRAIN [14/25][30/157] Loss_G: 0.0089\n",
            "PRETRAIN [14/25][31/157] Loss_G: 0.0080\n",
            "PRETRAIN [14/25][32/157] Loss_G: 0.0098\n",
            "PRETRAIN [14/25][33/157] Loss_G: 0.0068\n",
            "PRETRAIN [14/25][34/157] Loss_G: 0.0078\n",
            "PRETRAIN [14/25][35/157] Loss_G: 0.0092\n",
            "PRETRAIN [14/25][36/157] Loss_G: 0.0077\n",
            "PRETRAIN [14/25][37/157] Loss_G: 0.0090\n",
            "PRETRAIN [14/25][38/157] Loss_G: 0.0082\n",
            "PRETRAIN [14/25][39/157] Loss_G: 0.0080\n",
            "PRETRAIN [14/25][40/157] Loss_G: 0.0078\n",
            "PRETRAIN [14/25][41/157] Loss_G: 0.0079\n",
            "PRETRAIN [14/25][42/157] Loss_G: 0.0088\n",
            "PRETRAIN [14/25][43/157] Loss_G: 0.0101\n",
            "PRETRAIN [14/25][44/157] Loss_G: 0.0111\n",
            "PRETRAIN [14/25][45/157] Loss_G: 0.0093\n",
            "PRETRAIN [14/25][46/157] Loss_G: 0.0087\n",
            "PRETRAIN [14/25][47/157] Loss_G: 0.0068\n",
            "PRETRAIN [14/25][48/157] Loss_G: 0.0103\n",
            "PRETRAIN [14/25][49/157] Loss_G: 0.0092\n",
            "PRETRAIN [14/25][50/157] Loss_G: 0.0113\n",
            "PRETRAIN [14/25][51/157] Loss_G: 0.0089\n",
            "PRETRAIN [14/25][52/157] Loss_G: 0.0086\n",
            "PRETRAIN [14/25][53/157] Loss_G: 0.0094\n",
            "PRETRAIN [14/25][54/157] Loss_G: 0.0078\n",
            "PRETRAIN [14/25][55/157] Loss_G: 0.0086\n",
            "PRETRAIN [14/25][56/157] Loss_G: 0.0102\n",
            "PRETRAIN [14/25][57/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][58/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][59/157] Loss_G: 0.0070\n",
            "PRETRAIN [14/25][60/157] Loss_G: 0.0108\n",
            "PRETRAIN [14/25][61/157] Loss_G: 0.0103\n",
            "PRETRAIN [14/25][62/157] Loss_G: 0.0067\n",
            "PRETRAIN [14/25][63/157] Loss_G: 0.0084\n",
            "PRETRAIN [14/25][64/157] Loss_G: 0.0089\n",
            "PRETRAIN [14/25][65/157] Loss_G: 0.0091\n",
            "PRETRAIN [14/25][66/157] Loss_G: 0.0074\n",
            "PRETRAIN [14/25][67/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][68/157] Loss_G: 0.0097\n",
            "PRETRAIN [14/25][69/157] Loss_G: 0.0082\n",
            "PRETRAIN [14/25][70/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][71/157] Loss_G: 0.0096\n",
            "PRETRAIN [14/25][72/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][73/157] Loss_G: 0.0107\n",
            "PRETRAIN [14/25][74/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][75/157] Loss_G: 0.0088\n",
            "PRETRAIN [14/25][76/157] Loss_G: 0.0072\n",
            "PRETRAIN [14/25][77/157] Loss_G: 0.0067\n",
            "PRETRAIN [14/25][78/157] Loss_G: 0.0098\n",
            "PRETRAIN [14/25][79/157] Loss_G: 0.0085\n",
            "PRETRAIN [14/25][80/157] Loss_G: 0.0074\n",
            "PRETRAIN [14/25][81/157] Loss_G: 0.0092\n",
            "PRETRAIN [14/25][82/157] Loss_G: 0.0105\n",
            "PRETRAIN [14/25][83/157] Loss_G: 0.0080\n",
            "PRETRAIN [14/25][84/157] Loss_G: 0.0095\n",
            "PRETRAIN [14/25][85/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][86/157] Loss_G: 0.0097\n",
            "PRETRAIN [14/25][87/157] Loss_G: 0.0075\n",
            "PRETRAIN [14/25][88/157] Loss_G: 0.0108\n",
            "PRETRAIN [14/25][89/157] Loss_G: 0.0093\n",
            "PRETRAIN [14/25][90/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][91/157] Loss_G: 0.0088\n",
            "PRETRAIN [14/25][92/157] Loss_G: 0.0099\n",
            "PRETRAIN [14/25][93/157] Loss_G: 0.0090\n",
            "PRETRAIN [14/25][94/157] Loss_G: 0.0109\n",
            "PRETRAIN [14/25][95/157] Loss_G: 0.0089\n",
            "PRETRAIN [14/25][96/157] Loss_G: 0.0080\n",
            "PRETRAIN [14/25][97/157] Loss_G: 0.0091\n",
            "PRETRAIN [14/25][98/157] Loss_G: 0.0078\n",
            "PRETRAIN [14/25][99/157] Loss_G: 0.0096\n",
            "PRETRAIN [14/25][100/157] Loss_G: 0.0075\n",
            "PRETRAIN [14/25][101/157] Loss_G: 0.0077\n",
            "PRETRAIN [14/25][102/157] Loss_G: 0.0085\n",
            "PRETRAIN [14/25][103/157] Loss_G: 0.0101\n",
            "PRETRAIN [14/25][104/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][105/157] Loss_G: 0.0073\n",
            "PRETRAIN [14/25][106/157] Loss_G: 0.0088\n",
            "PRETRAIN [14/25][107/157] Loss_G: 0.0102\n",
            "PRETRAIN [14/25][108/157] Loss_G: 0.0078\n",
            "PRETRAIN [14/25][109/157] Loss_G: 0.0076\n",
            "PRETRAIN [14/25][110/157] Loss_G: 0.0110\n",
            "PRETRAIN [14/25][111/157] Loss_G: 0.0099\n",
            "PRETRAIN [14/25][112/157] Loss_G: 0.0090\n",
            "PRETRAIN [14/25][113/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][114/157] Loss_G: 0.0076\n",
            "PRETRAIN [14/25][115/157] Loss_G: 0.0072\n",
            "PRETRAIN [14/25][116/157] Loss_G: 0.0082\n",
            "PRETRAIN [14/25][117/157] Loss_G: 0.0077\n",
            "PRETRAIN [14/25][118/157] Loss_G: 0.0088\n",
            "PRETRAIN [14/25][119/157] Loss_G: 0.0097\n",
            "PRETRAIN [14/25][120/157] Loss_G: 0.0082\n",
            "PRETRAIN [14/25][121/157] Loss_G: 0.0081\n",
            "PRETRAIN [14/25][122/157] Loss_G: 0.0074\n",
            "PRETRAIN [14/25][123/157] Loss_G: 0.0101\n",
            "PRETRAIN [14/25][124/157] Loss_G: 0.0094\n",
            "PRETRAIN [14/25][125/157] Loss_G: 0.0090\n",
            "PRETRAIN [14/25][126/157] Loss_G: 0.0092\n",
            "PRETRAIN [14/25][127/157] Loss_G: 0.0102\n",
            "PRETRAIN [14/25][128/157] Loss_G: 0.0077\n",
            "PRETRAIN [14/25][129/157] Loss_G: 0.0082\n",
            "PRETRAIN [14/25][130/157] Loss_G: 0.0091\n",
            "PRETRAIN [14/25][131/157] Loss_G: 0.0092\n",
            "PRETRAIN [14/25][132/157] Loss_G: 0.0085\n",
            "PRETRAIN [14/25][133/157] Loss_G: 0.0084\n",
            "PRETRAIN [14/25][134/157] Loss_G: 0.0102\n",
            "PRETRAIN [14/25][135/157] Loss_G: 0.0093\n",
            "PRETRAIN [14/25][136/157] Loss_G: 0.0098\n",
            "PRETRAIN [14/25][137/157] Loss_G: 0.0074\n",
            "PRETRAIN [14/25][138/157] Loss_G: 0.0087\n",
            "PRETRAIN [14/25][139/157] Loss_G: 0.0068\n",
            "PRETRAIN [14/25][140/157] Loss_G: 0.0105\n",
            "PRETRAIN [14/25][141/157] Loss_G: 0.0084\n",
            "PRETRAIN [14/25][142/157] Loss_G: 0.0082\n",
            "PRETRAIN [14/25][143/157] Loss_G: 0.0082\n",
            "PRETRAIN [14/25][144/157] Loss_G: 0.0089\n",
            "PRETRAIN [14/25][145/157] Loss_G: 0.0083\n",
            "PRETRAIN [14/25][146/157] Loss_G: 0.0071\n",
            "PRETRAIN [14/25][147/157] Loss_G: 0.0084\n",
            "PRETRAIN [14/25][148/157] Loss_G: 0.0079\n",
            "PRETRAIN [14/25][149/157] Loss_G: 0.0101\n",
            "PRETRAIN [14/25][150/157] Loss_G: 0.0094\n",
            "PRETRAIN [14/25][151/157] Loss_G: 0.0094\n",
            "PRETRAIN [14/25][152/157] Loss_G: 0.0079\n",
            "PRETRAIN [14/25][153/157] Loss_G: 0.0084\n",
            "PRETRAIN [14/25][154/157] Loss_G: 0.0125\n",
            "PRETRAIN [14/25][155/157] Loss_G: 0.0094\n",
            "PRETRAIN [14/25][156/157] Loss_G: 0.0095\n",
            "Time elapsed Epoch 14: 115 seconds\n",
            "PRETRAIN [15/25][0/157] Loss_G: 0.0113\n",
            "PRETRAIN [15/25][1/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][2/157] Loss_G: 0.0088\n",
            "PRETRAIN [15/25][3/157] Loss_G: 0.0092\n",
            "PRETRAIN [15/25][4/157] Loss_G: 0.0090\n",
            "PRETRAIN [15/25][5/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][6/157] Loss_G: 0.0100\n",
            "PRETRAIN [15/25][7/157] Loss_G: 0.0082\n",
            "PRETRAIN [15/25][8/157] Loss_G: 0.0076\n",
            "PRETRAIN [15/25][9/157] Loss_G: 0.0082\n",
            "PRETRAIN [15/25][10/157] Loss_G: 0.0084\n",
            "PRETRAIN [15/25][11/157] Loss_G: 0.0099\n",
            "PRETRAIN [15/25][12/157] Loss_G: 0.0097\n",
            "PRETRAIN [15/25][13/157] Loss_G: 0.0099\n",
            "PRETRAIN [15/25][14/157] Loss_G: 0.0123\n",
            "PRETRAIN [15/25][15/157] Loss_G: 0.0094\n",
            "PRETRAIN [15/25][16/157] Loss_G: 0.0077\n",
            "PRETRAIN [15/25][17/157] Loss_G: 0.0085\n",
            "PRETRAIN [15/25][18/157] Loss_G: 0.0133\n",
            "PRETRAIN [15/25][19/157] Loss_G: 0.0091\n",
            "PRETRAIN [15/25][20/157] Loss_G: 0.0084\n",
            "PRETRAIN [15/25][21/157] Loss_G: 0.0094\n",
            "PRETRAIN [15/25][22/157] Loss_G: 0.0083\n",
            "PRETRAIN [15/25][23/157] Loss_G: 0.0072\n",
            "PRETRAIN [15/25][24/157] Loss_G: 0.0080\n",
            "PRETRAIN [15/25][25/157] Loss_G: 0.0094\n",
            "PRETRAIN [15/25][26/157] Loss_G: 0.0080\n",
            "PRETRAIN [15/25][27/157] Loss_G: 0.0085\n",
            "PRETRAIN [15/25][28/157] Loss_G: 0.0072\n",
            "PRETRAIN [15/25][29/157] Loss_G: 0.0070\n",
            "PRETRAIN [15/25][30/157] Loss_G: 0.0076\n",
            "PRETRAIN [15/25][31/157] Loss_G: 0.0090\n",
            "PRETRAIN [15/25][32/157] Loss_G: 0.0067\n",
            "PRETRAIN [15/25][33/157] Loss_G: 0.0089\n",
            "PRETRAIN [15/25][34/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][35/157] Loss_G: 0.0069\n",
            "PRETRAIN [15/25][36/157] Loss_G: 0.0127\n",
            "PRETRAIN [15/25][37/157] Loss_G: 0.0089\n",
            "PRETRAIN [15/25][38/157] Loss_G: 0.0121\n",
            "PRETRAIN [15/25][39/157] Loss_G: 0.0079\n",
            "PRETRAIN [15/25][40/157] Loss_G: 0.0089\n",
            "PRETRAIN [15/25][41/157] Loss_G: 0.0104\n",
            "PRETRAIN [15/25][42/157] Loss_G: 0.0074\n",
            "PRETRAIN [15/25][43/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][44/157] Loss_G: 0.0093\n",
            "PRETRAIN [15/25][45/157] Loss_G: 0.0107\n",
            "PRETRAIN [15/25][46/157] Loss_G: 0.0089\n",
            "PRETRAIN [15/25][47/157] Loss_G: 0.0079\n",
            "PRETRAIN [15/25][48/157] Loss_G: 0.0077\n",
            "PRETRAIN [15/25][49/157] Loss_G: 0.0097\n",
            "PRETRAIN [15/25][50/157] Loss_G: 0.0083\n",
            "PRETRAIN [15/25][51/157] Loss_G: 0.0088\n",
            "PRETRAIN [15/25][52/157] Loss_G: 0.0084\n",
            "PRETRAIN [15/25][53/157] Loss_G: 0.0073\n",
            "PRETRAIN [15/25][54/157] Loss_G: 0.0085\n",
            "PRETRAIN [15/25][55/157] Loss_G: 0.0087\n",
            "PRETRAIN [15/25][56/157] Loss_G: 0.0095\n",
            "PRETRAIN [15/25][57/157] Loss_G: 0.0086\n",
            "PRETRAIN [15/25][58/157] Loss_G: 0.0089\n",
            "PRETRAIN [15/25][59/157] Loss_G: 0.0116\n",
            "PRETRAIN [15/25][60/157] Loss_G: 0.0101\n",
            "PRETRAIN [15/25][61/157] Loss_G: 0.0088\n",
            "PRETRAIN [15/25][62/157] Loss_G: 0.0084\n",
            "PRETRAIN [15/25][63/157] Loss_G: 0.0088\n",
            "PRETRAIN [15/25][64/157] Loss_G: 0.0082\n",
            "PRETRAIN [15/25][65/157] Loss_G: 0.0090\n",
            "PRETRAIN [15/25][66/157] Loss_G: 0.0074\n",
            "PRETRAIN [15/25][67/157] Loss_G: 0.0091\n",
            "PRETRAIN [15/25][68/157] Loss_G: 0.0098\n",
            "PRETRAIN [15/25][69/157] Loss_G: 0.0073\n",
            "PRETRAIN [15/25][70/157] Loss_G: 0.0083\n",
            "PRETRAIN [15/25][71/157] Loss_G: 0.0087\n",
            "PRETRAIN [15/25][72/157] Loss_G: 0.0094\n",
            "PRETRAIN [15/25][73/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][74/157] Loss_G: 0.0070\n",
            "PRETRAIN [15/25][75/157] Loss_G: 0.0070\n",
            "PRETRAIN [15/25][76/157] Loss_G: 0.0069\n",
            "PRETRAIN [15/25][77/157] Loss_G: 0.0101\n",
            "PRETRAIN [15/25][78/157] Loss_G: 0.0069\n",
            "PRETRAIN [15/25][79/157] Loss_G: 0.0076\n",
            "PRETRAIN [15/25][80/157] Loss_G: 0.0069\n",
            "PRETRAIN [15/25][81/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][82/157] Loss_G: 0.0088\n",
            "PRETRAIN [15/25][83/157] Loss_G: 0.0073\n",
            "PRETRAIN [15/25][84/157] Loss_G: 0.0066\n",
            "PRETRAIN [15/25][85/157] Loss_G: 0.0082\n",
            "PRETRAIN [15/25][86/157] Loss_G: 0.0073\n",
            "PRETRAIN [15/25][87/157] Loss_G: 0.0091\n",
            "PRETRAIN [15/25][88/157] Loss_G: 0.0087\n",
            "PRETRAIN [15/25][89/157] Loss_G: 0.0083\n",
            "PRETRAIN [15/25][90/157] Loss_G: 0.0070\n",
            "PRETRAIN [15/25][91/157] Loss_G: 0.0071\n",
            "PRETRAIN [15/25][92/157] Loss_G: 0.0069\n",
            "PRETRAIN [15/25][93/157] Loss_G: 0.0096\n",
            "PRETRAIN [15/25][94/157] Loss_G: 0.0082\n",
            "PRETRAIN [15/25][95/157] Loss_G: 0.0079\n",
            "PRETRAIN [15/25][96/157] Loss_G: 0.0077\n",
            "PRETRAIN [15/25][97/157] Loss_G: 0.0084\n",
            "PRETRAIN [15/25][98/157] Loss_G: 0.0098\n",
            "PRETRAIN [15/25][99/157] Loss_G: 0.0085\n",
            "PRETRAIN [15/25][100/157] Loss_G: 0.0103\n",
            "PRETRAIN [15/25][101/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][102/157] Loss_G: 0.0082\n",
            "PRETRAIN [15/25][103/157] Loss_G: 0.0079\n",
            "PRETRAIN [15/25][104/157] Loss_G: 0.0074\n",
            "PRETRAIN [15/25][105/157] Loss_G: 0.0069\n",
            "PRETRAIN [15/25][106/157] Loss_G: 0.0079\n",
            "PRETRAIN [15/25][107/157] Loss_G: 0.0088\n",
            "PRETRAIN [15/25][108/157] Loss_G: 0.0092\n",
            "PRETRAIN [15/25][109/157] Loss_G: 0.0076\n",
            "PRETRAIN [15/25][110/157] Loss_G: 0.0101\n",
            "PRETRAIN [15/25][111/157] Loss_G: 0.0074\n",
            "PRETRAIN [15/25][112/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][113/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][114/157] Loss_G: 0.0075\n",
            "PRETRAIN [15/25][115/157] Loss_G: 0.0082\n",
            "PRETRAIN [15/25][116/157] Loss_G: 0.0073\n",
            "PRETRAIN [15/25][117/157] Loss_G: 0.0077\n",
            "PRETRAIN [15/25][118/157] Loss_G: 0.0104\n",
            "PRETRAIN [15/25][119/157] Loss_G: 0.0074\n",
            "PRETRAIN [15/25][120/157] Loss_G: 0.0090\n",
            "PRETRAIN [15/25][121/157] Loss_G: 0.0080\n",
            "PRETRAIN [15/25][122/157] Loss_G: 0.0087\n",
            "PRETRAIN [15/25][123/157] Loss_G: 0.0079\n",
            "PRETRAIN [15/25][124/157] Loss_G: 0.0110\n",
            "PRETRAIN [15/25][125/157] Loss_G: 0.0085\n",
            "PRETRAIN [15/25][126/157] Loss_G: 0.0093\n",
            "PRETRAIN [15/25][127/157] Loss_G: 0.0072\n",
            "PRETRAIN [15/25][128/157] Loss_G: 0.0066\n",
            "PRETRAIN [15/25][129/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][130/157] Loss_G: 0.0074\n",
            "PRETRAIN [15/25][131/157] Loss_G: 0.0071\n",
            "PRETRAIN [15/25][132/157] Loss_G: 0.0093\n",
            "PRETRAIN [15/25][133/157] Loss_G: 0.0072\n",
            "PRETRAIN [15/25][134/157] Loss_G: 0.0129\n",
            "PRETRAIN [15/25][135/157] Loss_G: 0.0075\n",
            "PRETRAIN [15/25][136/157] Loss_G: 0.0096\n",
            "PRETRAIN [15/25][137/157] Loss_G: 0.0072\n",
            "PRETRAIN [15/25][138/157] Loss_G: 0.0088\n",
            "PRETRAIN [15/25][139/157] Loss_G: 0.0091\n",
            "PRETRAIN [15/25][140/157] Loss_G: 0.0074\n",
            "PRETRAIN [15/25][141/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][142/157] Loss_G: 0.0069\n",
            "PRETRAIN [15/25][143/157] Loss_G: 0.0080\n",
            "PRETRAIN [15/25][144/157] Loss_G: 0.0072\n",
            "PRETRAIN [15/25][145/157] Loss_G: 0.0087\n",
            "PRETRAIN [15/25][146/157] Loss_G: 0.0090\n",
            "PRETRAIN [15/25][147/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][148/157] Loss_G: 0.0097\n",
            "PRETRAIN [15/25][149/157] Loss_G: 0.0093\n",
            "PRETRAIN [15/25][150/157] Loss_G: 0.0080\n",
            "PRETRAIN [15/25][151/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][152/157] Loss_G: 0.0092\n",
            "PRETRAIN [15/25][153/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][154/157] Loss_G: 0.0078\n",
            "PRETRAIN [15/25][155/157] Loss_G: 0.0081\n",
            "PRETRAIN [15/25][156/157] Loss_G: 0.0086\n",
            "Time elapsed Epoch 15: 115 seconds\n",
            "PRETRAIN [16/25][0/157] Loss_G: 0.0074\n",
            "PRETRAIN [16/25][1/157] Loss_G: 0.0090\n",
            "PRETRAIN [16/25][2/157] Loss_G: 0.0091\n",
            "PRETRAIN [16/25][3/157] Loss_G: 0.0075\n",
            "PRETRAIN [16/25][4/157] Loss_G: 0.0100\n",
            "PRETRAIN [16/25][5/157] Loss_G: 0.0083\n",
            "PRETRAIN [16/25][6/157] Loss_G: 0.0064\n",
            "PRETRAIN [16/25][7/157] Loss_G: 0.0068\n",
            "PRETRAIN [16/25][8/157] Loss_G: 0.0080\n",
            "PRETRAIN [16/25][9/157] Loss_G: 0.0077\n",
            "PRETRAIN [16/25][10/157] Loss_G: 0.0084\n",
            "PRETRAIN [16/25][11/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][12/157] Loss_G: 0.0086\n",
            "PRETRAIN [16/25][13/157] Loss_G: 0.0087\n",
            "PRETRAIN [16/25][14/157] Loss_G: 0.0083\n",
            "PRETRAIN [16/25][15/157] Loss_G: 0.0095\n",
            "PRETRAIN [16/25][16/157] Loss_G: 0.0085\n",
            "PRETRAIN [16/25][17/157] Loss_G: 0.0083\n",
            "PRETRAIN [16/25][18/157] Loss_G: 0.0089\n",
            "PRETRAIN [16/25][19/157] Loss_G: 0.0090\n",
            "PRETRAIN [16/25][20/157] Loss_G: 0.0096\n",
            "PRETRAIN [16/25][21/157] Loss_G: 0.0104\n",
            "PRETRAIN [16/25][22/157] Loss_G: 0.0085\n",
            "PRETRAIN [16/25][23/157] Loss_G: 0.0088\n",
            "PRETRAIN [16/25][24/157] Loss_G: 0.0083\n",
            "PRETRAIN [16/25][25/157] Loss_G: 0.0139\n",
            "PRETRAIN [16/25][26/157] Loss_G: 0.0086\n",
            "PRETRAIN [16/25][27/157] Loss_G: 0.0107\n",
            "PRETRAIN [16/25][28/157] Loss_G: 0.0119\n",
            "PRETRAIN [16/25][29/157] Loss_G: 0.0081\n",
            "PRETRAIN [16/25][30/157] Loss_G: 0.0085\n",
            "PRETRAIN [16/25][31/157] Loss_G: 0.0128\n",
            "PRETRAIN [16/25][32/157] Loss_G: 0.0082\n",
            "PRETRAIN [16/25][33/157] Loss_G: 0.0080\n",
            "PRETRAIN [16/25][34/157] Loss_G: 0.0097\n",
            "PRETRAIN [16/25][35/157] Loss_G: 0.0102\n",
            "PRETRAIN [16/25][36/157] Loss_G: 0.0089\n",
            "PRETRAIN [16/25][37/157] Loss_G: 0.0075\n",
            "PRETRAIN [16/25][38/157] Loss_G: 0.0080\n",
            "PRETRAIN [16/25][39/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][40/157] Loss_G: 0.0062\n",
            "PRETRAIN [16/25][41/157] Loss_G: 0.0086\n",
            "PRETRAIN [16/25][42/157] Loss_G: 0.0073\n",
            "PRETRAIN [16/25][43/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][44/157] Loss_G: 0.0086\n",
            "PRETRAIN [16/25][45/157] Loss_G: 0.0081\n",
            "PRETRAIN [16/25][46/157] Loss_G: 0.0082\n",
            "PRETRAIN [16/25][47/157] Loss_G: 0.0070\n",
            "PRETRAIN [16/25][48/157] Loss_G: 0.0081\n",
            "PRETRAIN [16/25][49/157] Loss_G: 0.0065\n",
            "PRETRAIN [16/25][50/157] Loss_G: 0.0082\n",
            "PRETRAIN [16/25][51/157] Loss_G: 0.0065\n",
            "PRETRAIN [16/25][52/157] Loss_G: 0.0070\n",
            "PRETRAIN [16/25][53/157] Loss_G: 0.0069\n",
            "PRETRAIN [16/25][54/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][55/157] Loss_G: 0.0080\n",
            "PRETRAIN [16/25][56/157] Loss_G: 0.0099\n",
            "PRETRAIN [16/25][57/157] Loss_G: 0.0103\n",
            "PRETRAIN [16/25][58/157] Loss_G: 0.0087\n",
            "PRETRAIN [16/25][59/157] Loss_G: 0.0067\n",
            "PRETRAIN [16/25][60/157] Loss_G: 0.0079\n",
            "PRETRAIN [16/25][61/157] Loss_G: 0.0083\n",
            "PRETRAIN [16/25][62/157] Loss_G: 0.0076\n",
            "PRETRAIN [16/25][63/157] Loss_G: 0.0067\n",
            "PRETRAIN [16/25][64/157] Loss_G: 0.0072\n",
            "PRETRAIN [16/25][65/157] Loss_G: 0.0088\n",
            "PRETRAIN [16/25][66/157] Loss_G: 0.0069\n",
            "PRETRAIN [16/25][67/157] Loss_G: 0.0070\n",
            "PRETRAIN [16/25][68/157] Loss_G: 0.0074\n",
            "PRETRAIN [16/25][69/157] Loss_G: 0.0073\n",
            "PRETRAIN [16/25][70/157] Loss_G: 0.0077\n",
            "PRETRAIN [16/25][71/157] Loss_G: 0.0079\n",
            "PRETRAIN [16/25][72/157] Loss_G: 0.0085\n",
            "PRETRAIN [16/25][73/157] Loss_G: 0.0070\n",
            "PRETRAIN [16/25][74/157] Loss_G: 0.0085\n",
            "PRETRAIN [16/25][75/157] Loss_G: 0.0081\n",
            "PRETRAIN [16/25][76/157] Loss_G: 0.0079\n",
            "PRETRAIN [16/25][77/157] Loss_G: 0.0081\n",
            "PRETRAIN [16/25][78/157] Loss_G: 0.0088\n",
            "PRETRAIN [16/25][79/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][80/157] Loss_G: 0.0092\n",
            "PRETRAIN [16/25][81/157] Loss_G: 0.0071\n",
            "PRETRAIN [16/25][82/157] Loss_G: 0.0096\n",
            "PRETRAIN [16/25][83/157] Loss_G: 0.0095\n",
            "PRETRAIN [16/25][84/157] Loss_G: 0.0084\n",
            "PRETRAIN [16/25][85/157] Loss_G: 0.0077\n",
            "PRETRAIN [16/25][86/157] Loss_G: 0.0076\n",
            "PRETRAIN [16/25][87/157] Loss_G: 0.0073\n",
            "PRETRAIN [16/25][88/157] Loss_G: 0.0065\n",
            "PRETRAIN [16/25][89/157] Loss_G: 0.0073\n",
            "PRETRAIN [16/25][90/157] Loss_G: 0.0070\n",
            "PRETRAIN [16/25][91/157] Loss_G: 0.0090\n",
            "PRETRAIN [16/25][92/157] Loss_G: 0.0082\n",
            "PRETRAIN [16/25][93/157] Loss_G: 0.0084\n",
            "PRETRAIN [16/25][94/157] Loss_G: 0.0098\n",
            "PRETRAIN [16/25][95/157] Loss_G: 0.0069\n",
            "PRETRAIN [16/25][96/157] Loss_G: 0.0068\n",
            "PRETRAIN [16/25][97/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][98/157] Loss_G: 0.0071\n",
            "PRETRAIN [16/25][99/157] Loss_G: 0.0093\n",
            "PRETRAIN [16/25][100/157] Loss_G: 0.0079\n",
            "PRETRAIN [16/25][101/157] Loss_G: 0.0092\n",
            "PRETRAIN [16/25][102/157] Loss_G: 0.0071\n",
            "PRETRAIN [16/25][103/157] Loss_G: 0.0087\n",
            "PRETRAIN [16/25][104/157] Loss_G: 0.0093\n",
            "PRETRAIN [16/25][105/157] Loss_G: 0.0062\n",
            "PRETRAIN [16/25][106/157] Loss_G: 0.0079\n",
            "PRETRAIN [16/25][107/157] Loss_G: 0.0073\n",
            "PRETRAIN [16/25][108/157] Loss_G: 0.0092\n",
            "PRETRAIN [16/25][109/157] Loss_G: 0.0092\n",
            "PRETRAIN [16/25][110/157] Loss_G: 0.0090\n",
            "PRETRAIN [16/25][111/157] Loss_G: 0.0067\n",
            "PRETRAIN [16/25][112/157] Loss_G: 0.0066\n",
            "PRETRAIN [16/25][113/157] Loss_G: 0.0072\n",
            "PRETRAIN [16/25][114/157] Loss_G: 0.0069\n",
            "PRETRAIN [16/25][115/157] Loss_G: 0.0105\n",
            "PRETRAIN [16/25][116/157] Loss_G: 0.0075\n",
            "PRETRAIN [16/25][117/157] Loss_G: 0.0072\n",
            "PRETRAIN [16/25][118/157] Loss_G: 0.0074\n",
            "PRETRAIN [16/25][119/157] Loss_G: 0.0096\n",
            "PRETRAIN [16/25][120/157] Loss_G: 0.0123\n",
            "PRETRAIN [16/25][121/157] Loss_G: 0.0130\n",
            "PRETRAIN [16/25][122/157] Loss_G: 0.0122\n",
            "PRETRAIN [16/25][123/157] Loss_G: 0.0114\n",
            "PRETRAIN [16/25][124/157] Loss_G: 0.0106\n",
            "PRETRAIN [16/25][125/157] Loss_G: 0.0096\n",
            "PRETRAIN [16/25][126/157] Loss_G: 0.0099\n",
            "PRETRAIN [16/25][127/157] Loss_G: 0.0092\n",
            "PRETRAIN [16/25][128/157] Loss_G: 0.0076\n",
            "PRETRAIN [16/25][129/157] Loss_G: 0.0099\n",
            "PRETRAIN [16/25][130/157] Loss_G: 0.0090\n",
            "PRETRAIN [16/25][131/157] Loss_G: 0.0104\n",
            "PRETRAIN [16/25][132/157] Loss_G: 0.0094\n",
            "PRETRAIN [16/25][133/157] Loss_G: 0.0085\n",
            "PRETRAIN [16/25][134/157] Loss_G: 0.0090\n",
            "PRETRAIN [16/25][135/157] Loss_G: 0.0101\n",
            "PRETRAIN [16/25][136/157] Loss_G: 0.0083\n",
            "PRETRAIN [16/25][137/157] Loss_G: 0.0093\n",
            "PRETRAIN [16/25][138/157] Loss_G: 0.0117\n",
            "PRETRAIN [16/25][139/157] Loss_G: 0.0085\n",
            "PRETRAIN [16/25][140/157] Loss_G: 0.0090\n",
            "PRETRAIN [16/25][141/157] Loss_G: 0.0084\n",
            "PRETRAIN [16/25][142/157] Loss_G: 0.0083\n",
            "PRETRAIN [16/25][143/157] Loss_G: 0.0098\n",
            "PRETRAIN [16/25][144/157] Loss_G: 0.0095\n",
            "PRETRAIN [16/25][145/157] Loss_G: 0.0087\n",
            "PRETRAIN [16/25][146/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][147/157] Loss_G: 0.0072\n",
            "PRETRAIN [16/25][148/157] Loss_G: 0.0079\n",
            "PRETRAIN [16/25][149/157] Loss_G: 0.0092\n",
            "PRETRAIN [16/25][150/157] Loss_G: 0.0101\n",
            "PRETRAIN [16/25][151/157] Loss_G: 0.0079\n",
            "PRETRAIN [16/25][152/157] Loss_G: 0.0067\n",
            "PRETRAIN [16/25][153/157] Loss_G: 0.0080\n",
            "PRETRAIN [16/25][154/157] Loss_G: 0.0084\n",
            "PRETRAIN [16/25][155/157] Loss_G: 0.0078\n",
            "PRETRAIN [16/25][156/157] Loss_G: 0.0092\n",
            "Time elapsed Epoch 16: 115 seconds\n",
            "PRETRAIN [17/25][0/157] Loss_G: 0.0092\n",
            "PRETRAIN [17/25][1/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][2/157] Loss_G: 0.0086\n",
            "PRETRAIN [17/25][3/157] Loss_G: 0.0084\n",
            "PRETRAIN [17/25][4/157] Loss_G: 0.0074\n",
            "PRETRAIN [17/25][5/157] Loss_G: 0.0070\n",
            "PRETRAIN [17/25][6/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][7/157] Loss_G: 0.0064\n",
            "PRETRAIN [17/25][8/157] Loss_G: 0.0093\n",
            "PRETRAIN [17/25][9/157] Loss_G: 0.0060\n",
            "PRETRAIN [17/25][10/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][11/157] Loss_G: 0.0081\n",
            "PRETRAIN [17/25][12/157] Loss_G: 0.0078\n",
            "PRETRAIN [17/25][13/157] Loss_G: 0.0079\n",
            "PRETRAIN [17/25][14/157] Loss_G: 0.0074\n",
            "PRETRAIN [17/25][15/157] Loss_G: 0.0092\n",
            "PRETRAIN [17/25][16/157] Loss_G: 0.0099\n",
            "PRETRAIN [17/25][17/157] Loss_G: 0.0091\n",
            "PRETRAIN [17/25][18/157] Loss_G: 0.0067\n",
            "PRETRAIN [17/25][19/157] Loss_G: 0.0095\n",
            "PRETRAIN [17/25][20/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][21/157] Loss_G: 0.0080\n",
            "PRETRAIN [17/25][22/157] Loss_G: 0.0079\n",
            "PRETRAIN [17/25][23/157] Loss_G: 0.0077\n",
            "PRETRAIN [17/25][24/157] Loss_G: 0.0103\n",
            "PRETRAIN [17/25][25/157] Loss_G: 0.0079\n",
            "PRETRAIN [17/25][26/157] Loss_G: 0.0070\n",
            "PRETRAIN [17/25][27/157] Loss_G: 0.0079\n",
            "PRETRAIN [17/25][28/157] Loss_G: 0.0074\n",
            "PRETRAIN [17/25][29/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][30/157] Loss_G: 0.0079\n",
            "PRETRAIN [17/25][31/157] Loss_G: 0.0102\n",
            "PRETRAIN [17/25][32/157] Loss_G: 0.0082\n",
            "PRETRAIN [17/25][33/157] Loss_G: 0.0074\n",
            "PRETRAIN [17/25][34/157] Loss_G: 0.0082\n",
            "PRETRAIN [17/25][35/157] Loss_G: 0.0084\n",
            "PRETRAIN [17/25][36/157] Loss_G: 0.0064\n",
            "PRETRAIN [17/25][37/157] Loss_G: 0.0073\n",
            "PRETRAIN [17/25][38/157] Loss_G: 0.0078\n",
            "PRETRAIN [17/25][39/157] Loss_G: 0.0087\n",
            "PRETRAIN [17/25][40/157] Loss_G: 0.0071\n",
            "PRETRAIN [17/25][41/157] Loss_G: 0.0070\n",
            "PRETRAIN [17/25][42/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][43/157] Loss_G: 0.0083\n",
            "PRETRAIN [17/25][44/157] Loss_G: 0.0080\n",
            "PRETRAIN [17/25][45/157] Loss_G: 0.0073\n",
            "PRETRAIN [17/25][46/157] Loss_G: 0.0084\n",
            "PRETRAIN [17/25][47/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][48/157] Loss_G: 0.0079\n",
            "PRETRAIN [17/25][49/157] Loss_G: 0.0080\n",
            "PRETRAIN [17/25][50/157] Loss_G: 0.0083\n",
            "PRETRAIN [17/25][51/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][52/157] Loss_G: 0.0085\n",
            "PRETRAIN [17/25][53/157] Loss_G: 0.0112\n",
            "PRETRAIN [17/25][54/157] Loss_G: 0.0077\n",
            "PRETRAIN [17/25][55/157] Loss_G: 0.0073\n",
            "PRETRAIN [17/25][56/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][57/157] Loss_G: 0.0072\n",
            "PRETRAIN [17/25][58/157] Loss_G: 0.0078\n",
            "PRETRAIN [17/25][59/157] Loss_G: 0.0079\n",
            "PRETRAIN [17/25][60/157] Loss_G: 0.0092\n",
            "PRETRAIN [17/25][61/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][62/157] Loss_G: 0.0083\n",
            "PRETRAIN [17/25][63/157] Loss_G: 0.0096\n",
            "PRETRAIN [17/25][64/157] Loss_G: 0.0092\n",
            "PRETRAIN [17/25][65/157] Loss_G: 0.0078\n",
            "PRETRAIN [17/25][66/157] Loss_G: 0.0072\n",
            "PRETRAIN [17/25][67/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][68/157] Loss_G: 0.0085\n",
            "PRETRAIN [17/25][69/157] Loss_G: 0.0072\n",
            "PRETRAIN [17/25][70/157] Loss_G: 0.0073\n",
            "PRETRAIN [17/25][71/157] Loss_G: 0.0084\n",
            "PRETRAIN [17/25][72/157] Loss_G: 0.0102\n",
            "PRETRAIN [17/25][73/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][74/157] Loss_G: 0.0088\n",
            "PRETRAIN [17/25][75/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][76/157] Loss_G: 0.0063\n",
            "PRETRAIN [17/25][77/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][78/157] Loss_G: 0.0071\n",
            "PRETRAIN [17/25][79/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][80/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][81/157] Loss_G: 0.0097\n",
            "PRETRAIN [17/25][82/157] Loss_G: 0.0095\n",
            "PRETRAIN [17/25][83/157] Loss_G: 0.0093\n",
            "PRETRAIN [17/25][84/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][85/157] Loss_G: 0.0085\n",
            "PRETRAIN [17/25][86/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][87/157] Loss_G: 0.0070\n",
            "PRETRAIN [17/25][88/157] Loss_G: 0.0066\n",
            "PRETRAIN [17/25][89/157] Loss_G: 0.0072\n",
            "PRETRAIN [17/25][90/157] Loss_G: 0.0072\n",
            "PRETRAIN [17/25][91/157] Loss_G: 0.0068\n",
            "PRETRAIN [17/25][92/157] Loss_G: 0.0066\n",
            "PRETRAIN [17/25][93/157] Loss_G: 0.0092\n",
            "PRETRAIN [17/25][94/157] Loss_G: 0.0074\n",
            "PRETRAIN [17/25][95/157] Loss_G: 0.0081\n",
            "PRETRAIN [17/25][96/157] Loss_G: 0.0087\n",
            "PRETRAIN [17/25][97/157] Loss_G: 0.0086\n",
            "PRETRAIN [17/25][98/157] Loss_G: 0.0083\n",
            "PRETRAIN [17/25][99/157] Loss_G: 0.0101\n",
            "PRETRAIN [17/25][100/157] Loss_G: 0.0098\n",
            "PRETRAIN [17/25][101/157] Loss_G: 0.0064\n",
            "PRETRAIN [17/25][102/157] Loss_G: 0.0069\n",
            "PRETRAIN [17/25][103/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][104/157] Loss_G: 0.0080\n",
            "PRETRAIN [17/25][105/157] Loss_G: 0.0078\n",
            "PRETRAIN [17/25][106/157] Loss_G: 0.0131\n",
            "PRETRAIN [17/25][107/157] Loss_G: 0.0094\n",
            "PRETRAIN [17/25][108/157] Loss_G: 0.0090\n",
            "PRETRAIN [17/25][109/157] Loss_G: 0.0077\n",
            "PRETRAIN [17/25][110/157] Loss_G: 0.0070\n",
            "PRETRAIN [17/25][111/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][112/157] Loss_G: 0.0068\n",
            "PRETRAIN [17/25][113/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][114/157] Loss_G: 0.0077\n",
            "PRETRAIN [17/25][115/157] Loss_G: 0.0066\n",
            "PRETRAIN [17/25][116/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][117/157] Loss_G: 0.0083\n",
            "PRETRAIN [17/25][118/157] Loss_G: 0.0102\n",
            "PRETRAIN [17/25][119/157] Loss_G: 0.0093\n",
            "PRETRAIN [17/25][120/157] Loss_G: 0.0084\n",
            "PRETRAIN [17/25][121/157] Loss_G: 0.0073\n",
            "PRETRAIN [17/25][122/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][123/157] Loss_G: 0.0077\n",
            "PRETRAIN [17/25][124/157] Loss_G: 0.0083\n",
            "PRETRAIN [17/25][125/157] Loss_G: 0.0083\n",
            "PRETRAIN [17/25][126/157] Loss_G: 0.0071\n",
            "PRETRAIN [17/25][127/157] Loss_G: 0.0077\n",
            "PRETRAIN [17/25][128/157] Loss_G: 0.0061\n",
            "PRETRAIN [17/25][129/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][130/157] Loss_G: 0.0082\n",
            "PRETRAIN [17/25][131/157] Loss_G: 0.0088\n",
            "PRETRAIN [17/25][132/157] Loss_G: 0.0077\n",
            "PRETRAIN [17/25][133/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][134/157] Loss_G: 0.0089\n",
            "PRETRAIN [17/25][135/157] Loss_G: 0.0078\n",
            "PRETRAIN [17/25][136/157] Loss_G: 0.0085\n",
            "PRETRAIN [17/25][137/157] Loss_G: 0.0073\n",
            "PRETRAIN [17/25][138/157] Loss_G: 0.0091\n",
            "PRETRAIN [17/25][139/157] Loss_G: 0.0096\n",
            "PRETRAIN [17/25][140/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][141/157] Loss_G: 0.0107\n",
            "PRETRAIN [17/25][142/157] Loss_G: 0.0088\n",
            "PRETRAIN [17/25][143/157] Loss_G: 0.0088\n",
            "PRETRAIN [17/25][144/157] Loss_G: 0.0076\n",
            "PRETRAIN [17/25][145/157] Loss_G: 0.0102\n",
            "PRETRAIN [17/25][146/157] Loss_G: 0.0081\n",
            "PRETRAIN [17/25][147/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][148/157] Loss_G: 0.0086\n",
            "PRETRAIN [17/25][149/157] Loss_G: 0.0072\n",
            "PRETRAIN [17/25][150/157] Loss_G: 0.0070\n",
            "PRETRAIN [17/25][151/157] Loss_G: 0.0082\n",
            "PRETRAIN [17/25][152/157] Loss_G: 0.0075\n",
            "PRETRAIN [17/25][153/157] Loss_G: 0.0071\n",
            "PRETRAIN [17/25][154/157] Loss_G: 0.0066\n",
            "PRETRAIN [17/25][155/157] Loss_G: 0.0071\n",
            "PRETRAIN [17/25][156/157] Loss_G: 0.0136\n",
            "Time elapsed Epoch 17: 115 seconds\n",
            "PRETRAIN [18/25][0/157] Loss_G: 0.0090\n",
            "PRETRAIN [18/25][1/157] Loss_G: 0.0079\n",
            "PRETRAIN [18/25][2/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][3/157] Loss_G: 0.0091\n",
            "PRETRAIN [18/25][4/157] Loss_G: 0.0090\n",
            "PRETRAIN [18/25][5/157] Loss_G: 0.0080\n",
            "PRETRAIN [18/25][6/157] Loss_G: 0.0089\n",
            "PRETRAIN [18/25][7/157] Loss_G: 0.0075\n",
            "PRETRAIN [18/25][8/157] Loss_G: 0.0089\n",
            "PRETRAIN [18/25][9/157] Loss_G: 0.0072\n",
            "PRETRAIN [18/25][10/157] Loss_G: 0.0082\n",
            "PRETRAIN [18/25][11/157] Loss_G: 0.0090\n",
            "PRETRAIN [18/25][12/157] Loss_G: 0.0065\n",
            "PRETRAIN [18/25][13/157] Loss_G: 0.0073\n",
            "PRETRAIN [18/25][14/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][15/157] Loss_G: 0.0084\n",
            "PRETRAIN [18/25][16/157] Loss_G: 0.0064\n",
            "PRETRAIN [18/25][17/157] Loss_G: 0.0074\n",
            "PRETRAIN [18/25][18/157] Loss_G: 0.0109\n",
            "PRETRAIN [18/25][19/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][20/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][21/157] Loss_G: 0.0073\n",
            "PRETRAIN [18/25][22/157] Loss_G: 0.0074\n",
            "PRETRAIN [18/25][23/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][24/157] Loss_G: 0.0090\n",
            "PRETRAIN [18/25][25/157] Loss_G: 0.0069\n",
            "PRETRAIN [18/25][26/157] Loss_G: 0.0072\n",
            "PRETRAIN [18/25][27/157] Loss_G: 0.0070\n",
            "PRETRAIN [18/25][28/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][29/157] Loss_G: 0.0066\n",
            "PRETRAIN [18/25][30/157] Loss_G: 0.0080\n",
            "PRETRAIN [18/25][31/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][32/157] Loss_G: 0.0101\n",
            "PRETRAIN [18/25][33/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][34/157] Loss_G: 0.0068\n",
            "PRETRAIN [18/25][35/157] Loss_G: 0.0072\n",
            "PRETRAIN [18/25][36/157] Loss_G: 0.0098\n",
            "PRETRAIN [18/25][37/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][38/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][39/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][40/157] Loss_G: 0.0085\n",
            "PRETRAIN [18/25][41/157] Loss_G: 0.0085\n",
            "PRETRAIN [18/25][42/157] Loss_G: 0.0096\n",
            "PRETRAIN [18/25][43/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][44/157] Loss_G: 0.0072\n",
            "PRETRAIN [18/25][45/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][46/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][47/157] Loss_G: 0.0111\n",
            "PRETRAIN [18/25][48/157] Loss_G: 0.0103\n",
            "PRETRAIN [18/25][49/157] Loss_G: 0.0092\n",
            "PRETRAIN [18/25][50/157] Loss_G: 0.0086\n",
            "PRETRAIN [18/25][51/157] Loss_G: 0.0082\n",
            "PRETRAIN [18/25][52/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][53/157] Loss_G: 0.0084\n",
            "PRETRAIN [18/25][54/157] Loss_G: 0.0061\n",
            "PRETRAIN [18/25][55/157] Loss_G: 0.0094\n",
            "PRETRAIN [18/25][56/157] Loss_G: 0.0082\n",
            "PRETRAIN [18/25][57/157] Loss_G: 0.0087\n",
            "PRETRAIN [18/25][58/157] Loss_G: 0.0095\n",
            "PRETRAIN [18/25][59/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][60/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][61/157] Loss_G: 0.0097\n",
            "PRETRAIN [18/25][62/157] Loss_G: 0.0069\n",
            "PRETRAIN [18/25][63/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][64/157] Loss_G: 0.0073\n",
            "PRETRAIN [18/25][65/157] Loss_G: 0.0070\n",
            "PRETRAIN [18/25][66/157] Loss_G: 0.0101\n",
            "PRETRAIN [18/25][67/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][68/157] Loss_G: 0.0097\n",
            "PRETRAIN [18/25][69/157] Loss_G: 0.0074\n",
            "PRETRAIN [18/25][70/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][71/157] Loss_G: 0.0103\n",
            "PRETRAIN [18/25][72/157] Loss_G: 0.0073\n",
            "PRETRAIN [18/25][73/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][74/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][75/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][76/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][77/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][78/157] Loss_G: 0.0090\n",
            "PRETRAIN [18/25][79/157] Loss_G: 0.0075\n",
            "PRETRAIN [18/25][80/157] Loss_G: 0.0067\n",
            "PRETRAIN [18/25][81/157] Loss_G: 0.0084\n",
            "PRETRAIN [18/25][82/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][83/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][84/157] Loss_G: 0.0067\n",
            "PRETRAIN [18/25][85/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][86/157] Loss_G: 0.0085\n",
            "PRETRAIN [18/25][87/157] Loss_G: 0.0090\n",
            "PRETRAIN [18/25][88/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][89/157] Loss_G: 0.0073\n",
            "PRETRAIN [18/25][90/157] Loss_G: 0.0075\n",
            "PRETRAIN [18/25][91/157] Loss_G: 0.0094\n",
            "PRETRAIN [18/25][92/157] Loss_G: 0.0086\n",
            "PRETRAIN [18/25][93/157] Loss_G: 0.0098\n",
            "PRETRAIN [18/25][94/157] Loss_G: 0.0072\n",
            "PRETRAIN [18/25][95/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][96/157] Loss_G: 0.0088\n",
            "PRETRAIN [18/25][97/157] Loss_G: 0.0074\n",
            "PRETRAIN [18/25][98/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][99/157] Loss_G: 0.0082\n",
            "PRETRAIN [18/25][100/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][101/157] Loss_G: 0.0063\n",
            "PRETRAIN [18/25][102/157] Loss_G: 0.0092\n",
            "PRETRAIN [18/25][103/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][104/157] Loss_G: 0.0067\n",
            "PRETRAIN [18/25][105/157] Loss_G: 0.0065\n",
            "PRETRAIN [18/25][106/157] Loss_G: 0.0080\n",
            "PRETRAIN [18/25][107/157] Loss_G: 0.0069\n",
            "PRETRAIN [18/25][108/157] Loss_G: 0.0065\n",
            "PRETRAIN [18/25][109/157] Loss_G: 0.0059\n",
            "PRETRAIN [18/25][110/157] Loss_G: 0.0081\n",
            "PRETRAIN [18/25][111/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][112/157] Loss_G: 0.0074\n",
            "PRETRAIN [18/25][113/157] Loss_G: 0.0063\n",
            "PRETRAIN [18/25][114/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][115/157] Loss_G: 0.0070\n",
            "PRETRAIN [18/25][116/157] Loss_G: 0.0069\n",
            "PRETRAIN [18/25][117/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][118/157] Loss_G: 0.0088\n",
            "PRETRAIN [18/25][119/157] Loss_G: 0.0079\n",
            "PRETRAIN [18/25][120/157] Loss_G: 0.0086\n",
            "PRETRAIN [18/25][121/157] Loss_G: 0.0090\n",
            "PRETRAIN [18/25][122/157] Loss_G: 0.0088\n",
            "PRETRAIN [18/25][123/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][124/157] Loss_G: 0.0103\n",
            "PRETRAIN [18/25][125/157] Loss_G: 0.0060\n",
            "PRETRAIN [18/25][126/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][127/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][128/157] Loss_G: 0.0075\n",
            "PRETRAIN [18/25][129/157] Loss_G: 0.0075\n",
            "PRETRAIN [18/25][130/157] Loss_G: 0.0072\n",
            "PRETRAIN [18/25][131/157] Loss_G: 0.0079\n",
            "PRETRAIN [18/25][132/157] Loss_G: 0.0071\n",
            "PRETRAIN [18/25][133/157] Loss_G: 0.0067\n",
            "PRETRAIN [18/25][134/157] Loss_G: 0.0074\n",
            "PRETRAIN [18/25][135/157] Loss_G: 0.0070\n",
            "PRETRAIN [18/25][136/157] Loss_G: 0.0074\n",
            "PRETRAIN [18/25][137/157] Loss_G: 0.0085\n",
            "PRETRAIN [18/25][138/157] Loss_G: 0.0078\n",
            "PRETRAIN [18/25][139/157] Loss_G: 0.0076\n",
            "PRETRAIN [18/25][140/157] Loss_G: 0.0083\n",
            "PRETRAIN [18/25][141/157] Loss_G: 0.0064\n",
            "PRETRAIN [18/25][142/157] Loss_G: 0.0058\n",
            "PRETRAIN [18/25][143/157] Loss_G: 0.0082\n",
            "PRETRAIN [18/25][144/157] Loss_G: 0.0086\n",
            "PRETRAIN [18/25][145/157] Loss_G: 0.0077\n",
            "PRETRAIN [18/25][146/157] Loss_G: 0.0079\n",
            "PRETRAIN [18/25][147/157] Loss_G: 0.0079\n",
            "PRETRAIN [18/25][148/157] Loss_G: 0.0071\n",
            "PRETRAIN [18/25][149/157] Loss_G: 0.0088\n",
            "PRETRAIN [18/25][150/157] Loss_G: 0.0082\n",
            "PRETRAIN [18/25][151/157] Loss_G: 0.0079\n",
            "PRETRAIN [18/25][152/157] Loss_G: 0.0107\n",
            "PRETRAIN [18/25][153/157] Loss_G: 0.0072\n",
            "PRETRAIN [18/25][154/157] Loss_G: 0.0068\n",
            "PRETRAIN [18/25][155/157] Loss_G: 0.0100\n",
            "PRETRAIN [18/25][156/157] Loss_G: 0.0102\n",
            "Time elapsed Epoch 18: 115 seconds\n",
            "PRETRAIN [19/25][0/157] Loss_G: 0.0112\n",
            "PRETRAIN [19/25][1/157] Loss_G: 0.0100\n",
            "PRETRAIN [19/25][2/157] Loss_G: 0.0117\n",
            "PRETRAIN [19/25][3/157] Loss_G: 0.0082\n",
            "PRETRAIN [19/25][4/157] Loss_G: 0.0095\n",
            "PRETRAIN [19/25][5/157] Loss_G: 0.0085\n",
            "PRETRAIN [19/25][6/157] Loss_G: 0.0101\n",
            "PRETRAIN [19/25][7/157] Loss_G: 0.0065\n",
            "PRETRAIN [19/25][8/157] Loss_G: 0.0083\n",
            "PRETRAIN [19/25][9/157] Loss_G: 0.0065\n",
            "PRETRAIN [19/25][10/157] Loss_G: 0.0102\n",
            "PRETRAIN [19/25][11/157] Loss_G: 0.0088\n",
            "PRETRAIN [19/25][12/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][13/157] Loss_G: 0.0088\n",
            "PRETRAIN [19/25][14/157] Loss_G: 0.0083\n",
            "PRETRAIN [19/25][15/157] Loss_G: 0.0069\n",
            "PRETRAIN [19/25][16/157] Loss_G: 0.0072\n",
            "PRETRAIN [19/25][17/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][18/157] Loss_G: 0.0087\n",
            "PRETRAIN [19/25][19/157] Loss_G: 0.0066\n",
            "PRETRAIN [19/25][20/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][21/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][22/157] Loss_G: 0.0087\n",
            "PRETRAIN [19/25][23/157] Loss_G: 0.0070\n",
            "PRETRAIN [19/25][24/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][25/157] Loss_G: 0.0079\n",
            "PRETRAIN [19/25][26/157] Loss_G: 0.0085\n",
            "PRETRAIN [19/25][27/157] Loss_G: 0.0078\n",
            "PRETRAIN [19/25][28/157] Loss_G: 0.0080\n",
            "PRETRAIN [19/25][29/157] Loss_G: 0.0071\n",
            "PRETRAIN [19/25][30/157] Loss_G: 0.0104\n",
            "PRETRAIN [19/25][31/157] Loss_G: 0.0103\n",
            "PRETRAIN [19/25][32/157] Loss_G: 0.0079\n",
            "PRETRAIN [19/25][33/157] Loss_G: 0.0075\n",
            "PRETRAIN [19/25][34/157] Loss_G: 0.0138\n",
            "PRETRAIN [19/25][35/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][36/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][37/157] Loss_G: 0.0104\n",
            "PRETRAIN [19/25][38/157] Loss_G: 0.0071\n",
            "PRETRAIN [19/25][39/157] Loss_G: 0.0094\n",
            "PRETRAIN [19/25][40/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][41/157] Loss_G: 0.0082\n",
            "PRETRAIN [19/25][42/157] Loss_G: 0.0073\n",
            "PRETRAIN [19/25][43/157] Loss_G: 0.0070\n",
            "PRETRAIN [19/25][44/157] Loss_G: 0.0070\n",
            "PRETRAIN [19/25][45/157] Loss_G: 0.0069\n",
            "PRETRAIN [19/25][46/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][47/157] Loss_G: 0.0074\n",
            "PRETRAIN [19/25][48/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][49/157] Loss_G: 0.0067\n",
            "PRETRAIN [19/25][50/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][51/157] Loss_G: 0.0116\n",
            "PRETRAIN [19/25][52/157] Loss_G: 0.0079\n",
            "PRETRAIN [19/25][53/157] Loss_G: 0.0070\n",
            "PRETRAIN [19/25][54/157] Loss_G: 0.0072\n",
            "PRETRAIN [19/25][55/157] Loss_G: 0.0096\n",
            "PRETRAIN [19/25][56/157] Loss_G: 0.0073\n",
            "PRETRAIN [19/25][57/157] Loss_G: 0.0085\n",
            "PRETRAIN [19/25][58/157] Loss_G: 0.0067\n",
            "PRETRAIN [19/25][59/157] Loss_G: 0.0069\n",
            "PRETRAIN [19/25][60/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][61/157] Loss_G: 0.0067\n",
            "PRETRAIN [19/25][62/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][63/157] Loss_G: 0.0085\n",
            "PRETRAIN [19/25][64/157] Loss_G: 0.0074\n",
            "PRETRAIN [19/25][65/157] Loss_G: 0.0061\n",
            "PRETRAIN [19/25][66/157] Loss_G: 0.0061\n",
            "PRETRAIN [19/25][67/157] Loss_G: 0.0067\n",
            "PRETRAIN [19/25][68/157] Loss_G: 0.0081\n",
            "PRETRAIN [19/25][69/157] Loss_G: 0.0071\n",
            "PRETRAIN [19/25][70/157] Loss_G: 0.0070\n",
            "PRETRAIN [19/25][71/157] Loss_G: 0.0065\n",
            "PRETRAIN [19/25][72/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][73/157] Loss_G: 0.0067\n",
            "PRETRAIN [19/25][74/157] Loss_G: 0.0063\n",
            "PRETRAIN [19/25][75/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][76/157] Loss_G: 0.0072\n",
            "PRETRAIN [19/25][77/157] Loss_G: 0.0065\n",
            "PRETRAIN [19/25][78/157] Loss_G: 0.0082\n",
            "PRETRAIN [19/25][79/157] Loss_G: 0.0084\n",
            "PRETRAIN [19/25][80/157] Loss_G: 0.0083\n",
            "PRETRAIN [19/25][81/157] Loss_G: 0.0073\n",
            "PRETRAIN [19/25][82/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][83/157] Loss_G: 0.0084\n",
            "PRETRAIN [19/25][84/157] Loss_G: 0.0085\n",
            "PRETRAIN [19/25][85/157] Loss_G: 0.0082\n",
            "PRETRAIN [19/25][86/157] Loss_G: 0.0074\n",
            "PRETRAIN [19/25][87/157] Loss_G: 0.0104\n",
            "PRETRAIN [19/25][88/157] Loss_G: 0.0075\n",
            "PRETRAIN [19/25][89/157] Loss_G: 0.0083\n",
            "PRETRAIN [19/25][90/157] Loss_G: 0.0093\n",
            "PRETRAIN [19/25][91/157] Loss_G: 0.0073\n",
            "PRETRAIN [19/25][92/157] Loss_G: 0.0065\n",
            "PRETRAIN [19/25][93/157] Loss_G: 0.0080\n",
            "PRETRAIN [19/25][94/157] Loss_G: 0.0061\n",
            "PRETRAIN [19/25][95/157] Loss_G: 0.0075\n",
            "PRETRAIN [19/25][96/157] Loss_G: 0.0064\n",
            "PRETRAIN [19/25][97/157] Loss_G: 0.0060\n",
            "PRETRAIN [19/25][98/157] Loss_G: 0.0074\n",
            "PRETRAIN [19/25][99/157] Loss_G: 0.0069\n",
            "PRETRAIN [19/25][100/157] Loss_G: 0.0076\n",
            "PRETRAIN [19/25][101/157] Loss_G: 0.0075\n",
            "PRETRAIN [19/25][102/157] Loss_G: 0.0074\n",
            "PRETRAIN [19/25][103/157] Loss_G: 0.0070\n",
            "PRETRAIN [19/25][104/157] Loss_G: 0.0061\n",
            "PRETRAIN [19/25][105/157] Loss_G: 0.0067\n",
            "PRETRAIN [19/25][106/157] Loss_G: 0.0089\n",
            "PRETRAIN [19/25][107/157] Loss_G: 0.0077\n",
            "PRETRAIN [19/25][108/157] Loss_G: 0.0088\n",
            "PRETRAIN [19/25][109/157] Loss_G: 0.0080\n",
            "PRETRAIN [19/25][110/157] Loss_G: 0.0100\n",
            "PRETRAIN [19/25][111/157] Loss_G: 0.0072\n",
            "PRETRAIN [19/25][112/157] Loss_G: 0.0076\n",
            "PRETRAIN [19/25][113/157] Loss_G: 0.0089\n",
            "PRETRAIN [19/25][114/157] Loss_G: 0.0103\n",
            "PRETRAIN [19/25][115/157] Loss_G: 0.0810\n",
            "PRETRAIN [19/25][116/157] Loss_G: 0.0286\n",
            "PRETRAIN [19/25][117/157] Loss_G: 0.0272\n",
            "PRETRAIN [19/25][118/157] Loss_G: 0.0259\n",
            "PRETRAIN [19/25][119/157] Loss_G: 0.0209\n",
            "PRETRAIN [19/25][120/157] Loss_G: 0.0163\n",
            "PRETRAIN [19/25][121/157] Loss_G: 0.0165\n",
            "PRETRAIN [19/25][122/157] Loss_G: 0.0131\n",
            "PRETRAIN [19/25][123/157] Loss_G: 0.0121\n",
            "PRETRAIN [19/25][124/157] Loss_G: 0.0143\n",
            "PRETRAIN [19/25][125/157] Loss_G: 0.0118\n",
            "PRETRAIN [19/25][126/157] Loss_G: 0.0111\n",
            "PRETRAIN [19/25][127/157] Loss_G: 0.0109\n",
            "PRETRAIN [19/25][128/157] Loss_G: 0.0110\n",
            "PRETRAIN [19/25][129/157] Loss_G: 0.0095\n",
            "PRETRAIN [19/25][130/157] Loss_G: 0.0094\n",
            "PRETRAIN [19/25][131/157] Loss_G: 0.0104\n",
            "PRETRAIN [19/25][132/157] Loss_G: 0.0095\n",
            "PRETRAIN [19/25][133/157] Loss_G: 0.0090\n",
            "PRETRAIN [19/25][134/157] Loss_G: 0.0108\n",
            "PRETRAIN [19/25][135/157] Loss_G: 0.0110\n",
            "PRETRAIN [19/25][136/157] Loss_G: 0.0089\n",
            "PRETRAIN [19/25][137/157] Loss_G: 0.0092\n",
            "PRETRAIN [19/25][138/157] Loss_G: 0.0089\n",
            "PRETRAIN [19/25][139/157] Loss_G: 0.0093\n",
            "PRETRAIN [19/25][140/157] Loss_G: 0.0100\n",
            "PRETRAIN [19/25][141/157] Loss_G: 0.0092\n",
            "PRETRAIN [19/25][142/157] Loss_G: 0.0114\n",
            "PRETRAIN [19/25][143/157] Loss_G: 0.0083\n",
            "PRETRAIN [19/25][144/157] Loss_G: 0.0090\n",
            "PRETRAIN [19/25][145/157] Loss_G: 0.0087\n",
            "PRETRAIN [19/25][146/157] Loss_G: 0.0083\n",
            "PRETRAIN [19/25][147/157] Loss_G: 0.0111\n",
            "PRETRAIN [19/25][148/157] Loss_G: 0.0090\n",
            "PRETRAIN [19/25][149/157] Loss_G: 0.0090\n",
            "PRETRAIN [19/25][150/157] Loss_G: 0.0075\n",
            "PRETRAIN [19/25][151/157] Loss_G: 0.0096\n",
            "PRETRAIN [19/25][152/157] Loss_G: 0.0091\n",
            "PRETRAIN [19/25][153/157] Loss_G: 0.0079\n",
            "PRETRAIN [19/25][154/157] Loss_G: 0.0093\n",
            "PRETRAIN [19/25][155/157] Loss_G: 0.0069\n",
            "PRETRAIN [19/25][156/157] Loss_G: 0.0165\n",
            "Time elapsed Epoch 19: 115 seconds\n",
            "PRETRAIN [20/25][0/157] Loss_G: 0.0186\n",
            "PRETRAIN [20/25][1/157] Loss_G: 0.0113\n",
            "PRETRAIN [20/25][2/157] Loss_G: 0.0097\n",
            "PRETRAIN [20/25][3/157] Loss_G: 0.0111\n",
            "PRETRAIN [20/25][4/157] Loss_G: 0.0096\n",
            "PRETRAIN [20/25][5/157] Loss_G: 0.0103\n",
            "PRETRAIN [20/25][6/157] Loss_G: 0.0095\n",
            "PRETRAIN [20/25][7/157] Loss_G: 0.0085\n",
            "PRETRAIN [20/25][8/157] Loss_G: 0.0107\n",
            "PRETRAIN [20/25][9/157] Loss_G: 0.0090\n",
            "PRETRAIN [20/25][10/157] Loss_G: 0.0075\n",
            "PRETRAIN [20/25][11/157] Loss_G: 0.0093\n",
            "PRETRAIN [20/25][12/157] Loss_G: 0.0085\n",
            "PRETRAIN [20/25][13/157] Loss_G: 0.0084\n",
            "PRETRAIN [20/25][14/157] Loss_G: 0.0093\n",
            "PRETRAIN [20/25][15/157] Loss_G: 0.0091\n",
            "PRETRAIN [20/25][16/157] Loss_G: 0.0084\n",
            "PRETRAIN [20/25][17/157] Loss_G: 0.0097\n",
            "PRETRAIN [20/25][18/157] Loss_G: 0.0089\n",
            "PRETRAIN [20/25][19/157] Loss_G: 0.0087\n",
            "PRETRAIN [20/25][20/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][21/157] Loss_G: 0.0104\n",
            "PRETRAIN [20/25][22/157] Loss_G: 0.0068\n",
            "PRETRAIN [20/25][23/157] Loss_G: 0.0084\n",
            "PRETRAIN [20/25][24/157] Loss_G: 0.0099\n",
            "PRETRAIN [20/25][25/157] Loss_G: 0.0082\n",
            "PRETRAIN [20/25][26/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][27/157] Loss_G: 0.0072\n",
            "PRETRAIN [20/25][28/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][29/157] Loss_G: 0.0079\n",
            "PRETRAIN [20/25][30/157] Loss_G: 0.0087\n",
            "PRETRAIN [20/25][31/157] Loss_G: 0.0079\n",
            "PRETRAIN [20/25][32/157] Loss_G: 0.0109\n",
            "PRETRAIN [20/25][33/157] Loss_G: 0.0079\n",
            "PRETRAIN [20/25][34/157] Loss_G: 0.0086\n",
            "PRETRAIN [20/25][35/157] Loss_G: 0.0083\n",
            "PRETRAIN [20/25][36/157] Loss_G: 0.0100\n",
            "PRETRAIN [20/25][37/157] Loss_G: 0.0072\n",
            "PRETRAIN [20/25][38/157] Loss_G: 0.0093\n",
            "PRETRAIN [20/25][39/157] Loss_G: 0.0075\n",
            "PRETRAIN [20/25][40/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][41/157] Loss_G: 0.0099\n",
            "PRETRAIN [20/25][42/157] Loss_G: 0.0078\n",
            "PRETRAIN [20/25][43/157] Loss_G: 0.0087\n",
            "PRETRAIN [20/25][44/157] Loss_G: 0.0082\n",
            "PRETRAIN [20/25][45/157] Loss_G: 0.0081\n",
            "PRETRAIN [20/25][46/157] Loss_G: 0.0078\n",
            "PRETRAIN [20/25][47/157] Loss_G: 0.0075\n",
            "PRETRAIN [20/25][48/157] Loss_G: 0.0067\n",
            "PRETRAIN [20/25][49/157] Loss_G: 0.0083\n",
            "PRETRAIN [20/25][50/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][51/157] Loss_G: 0.0087\n",
            "PRETRAIN [20/25][52/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][53/157] Loss_G: 0.0075\n",
            "PRETRAIN [20/25][54/157] Loss_G: 0.0084\n",
            "PRETRAIN [20/25][55/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][56/157] Loss_G: 0.0088\n",
            "PRETRAIN [20/25][57/157] Loss_G: 0.0097\n",
            "PRETRAIN [20/25][58/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][59/157] Loss_G: 0.0083\n",
            "PRETRAIN [20/25][60/157] Loss_G: 0.0074\n",
            "PRETRAIN [20/25][61/157] Loss_G: 0.0071\n",
            "PRETRAIN [20/25][62/157] Loss_G: 0.0067\n",
            "PRETRAIN [20/25][63/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][64/157] Loss_G: 0.0111\n",
            "PRETRAIN [20/25][65/157] Loss_G: 0.0093\n",
            "PRETRAIN [20/25][66/157] Loss_G: 0.0084\n",
            "PRETRAIN [20/25][67/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][68/157] Loss_G: 0.0073\n",
            "PRETRAIN [20/25][69/157] Loss_G: 0.0096\n",
            "PRETRAIN [20/25][70/157] Loss_G: 0.0088\n",
            "PRETRAIN [20/25][71/157] Loss_G: 0.0077\n",
            "PRETRAIN [20/25][72/157] Loss_G: 0.0058\n",
            "PRETRAIN [20/25][73/157] Loss_G: 0.0082\n",
            "PRETRAIN [20/25][74/157] Loss_G: 0.0081\n",
            "PRETRAIN [20/25][75/157] Loss_G: 0.0083\n",
            "PRETRAIN [20/25][76/157] Loss_G: 0.0079\n",
            "PRETRAIN [20/25][77/157] Loss_G: 0.0077\n",
            "PRETRAIN [20/25][78/157] Loss_G: 0.0085\n",
            "PRETRAIN [20/25][79/157] Loss_G: 0.0093\n",
            "PRETRAIN [20/25][80/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][81/157] Loss_G: 0.0089\n",
            "PRETRAIN [20/25][82/157] Loss_G: 0.0091\n",
            "PRETRAIN [20/25][83/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][84/157] Loss_G: 0.0084\n",
            "PRETRAIN [20/25][85/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][86/157] Loss_G: 0.0082\n",
            "PRETRAIN [20/25][87/157] Loss_G: 0.0143\n",
            "PRETRAIN [20/25][88/157] Loss_G: 0.0090\n",
            "PRETRAIN [20/25][89/157] Loss_G: 0.0086\n",
            "PRETRAIN [20/25][90/157] Loss_G: 0.0083\n",
            "PRETRAIN [20/25][91/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][92/157] Loss_G: 0.0067\n",
            "PRETRAIN [20/25][93/157] Loss_G: 0.0081\n",
            "PRETRAIN [20/25][94/157] Loss_G: 0.0093\n",
            "PRETRAIN [20/25][95/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][96/157] Loss_G: 0.0114\n",
            "PRETRAIN [20/25][97/157] Loss_G: 0.0071\n",
            "PRETRAIN [20/25][98/157] Loss_G: 0.0082\n",
            "PRETRAIN [20/25][99/157] Loss_G: 0.0087\n",
            "PRETRAIN [20/25][100/157] Loss_G: 0.0079\n",
            "PRETRAIN [20/25][101/157] Loss_G: 0.0077\n",
            "PRETRAIN [20/25][102/157] Loss_G: 0.0081\n",
            "PRETRAIN [20/25][103/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][104/157] Loss_G: 0.0091\n",
            "PRETRAIN [20/25][105/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][106/157] Loss_G: 0.0073\n",
            "PRETRAIN [20/25][107/157] Loss_G: 0.0078\n",
            "PRETRAIN [20/25][108/157] Loss_G: 0.0073\n",
            "PRETRAIN [20/25][109/157] Loss_G: 0.0091\n",
            "PRETRAIN [20/25][110/157] Loss_G: 0.0077\n",
            "PRETRAIN [20/25][111/157] Loss_G: 0.0084\n",
            "PRETRAIN [20/25][112/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][113/157] Loss_G: 0.0081\n",
            "PRETRAIN [20/25][114/157] Loss_G: 0.0063\n",
            "PRETRAIN [20/25][115/157] Loss_G: 0.0067\n",
            "PRETRAIN [20/25][116/157] Loss_G: 0.0074\n",
            "PRETRAIN [20/25][117/157] Loss_G: 0.0067\n",
            "PRETRAIN [20/25][118/157] Loss_G: 0.0069\n",
            "PRETRAIN [20/25][119/157] Loss_G: 0.0082\n",
            "PRETRAIN [20/25][120/157] Loss_G: 0.0069\n",
            "PRETRAIN [20/25][121/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][122/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][123/157] Loss_G: 0.0066\n",
            "PRETRAIN [20/25][124/157] Loss_G: 0.0071\n",
            "PRETRAIN [20/25][125/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][126/157] Loss_G: 0.0074\n",
            "PRETRAIN [20/25][127/157] Loss_G: 0.0077\n",
            "PRETRAIN [20/25][128/157] Loss_G: 0.0062\n",
            "PRETRAIN [20/25][129/157] Loss_G: 0.0071\n",
            "PRETRAIN [20/25][130/157] Loss_G: 0.0078\n",
            "PRETRAIN [20/25][131/157] Loss_G: 0.0071\n",
            "PRETRAIN [20/25][132/157] Loss_G: 0.0067\n",
            "PRETRAIN [20/25][133/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][134/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][135/157] Loss_G: 0.0071\n",
            "PRETRAIN [20/25][136/157] Loss_G: 0.0082\n",
            "PRETRAIN [20/25][137/157] Loss_G: 0.0070\n",
            "PRETRAIN [20/25][138/157] Loss_G: 0.0074\n",
            "PRETRAIN [20/25][139/157] Loss_G: 0.0060\n",
            "PRETRAIN [20/25][140/157] Loss_G: 0.0068\n",
            "PRETRAIN [20/25][141/157] Loss_G: 0.0069\n",
            "PRETRAIN [20/25][142/157] Loss_G: 0.0063\n",
            "PRETRAIN [20/25][143/157] Loss_G: 0.0074\n",
            "PRETRAIN [20/25][144/157] Loss_G: 0.0067\n",
            "PRETRAIN [20/25][145/157] Loss_G: 0.0086\n",
            "PRETRAIN [20/25][146/157] Loss_G: 0.0076\n",
            "PRETRAIN [20/25][147/157] Loss_G: 0.0074\n",
            "PRETRAIN [20/25][148/157] Loss_G: 0.0071\n",
            "PRETRAIN [20/25][149/157] Loss_G: 0.0087\n",
            "PRETRAIN [20/25][150/157] Loss_G: 0.0069\n",
            "PRETRAIN [20/25][151/157] Loss_G: 0.0078\n",
            "PRETRAIN [20/25][152/157] Loss_G: 0.0068\n",
            "PRETRAIN [20/25][153/157] Loss_G: 0.0073\n",
            "PRETRAIN [20/25][154/157] Loss_G: 0.0096\n",
            "PRETRAIN [20/25][155/157] Loss_G: 0.0080\n",
            "PRETRAIN [20/25][156/157] Loss_G: 0.0093\n",
            "Time elapsed Epoch 20: 115 seconds\n",
            "PRETRAIN [21/25][0/157] Loss_G: 0.0087\n",
            "PRETRAIN [21/25][1/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][2/157] Loss_G: 0.0088\n",
            "PRETRAIN [21/25][3/157] Loss_G: 0.0102\n",
            "PRETRAIN [21/25][4/157] Loss_G: 0.0068\n",
            "PRETRAIN [21/25][5/157] Loss_G: 0.0071\n",
            "PRETRAIN [21/25][6/157] Loss_G: 0.0068\n",
            "PRETRAIN [21/25][7/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][8/157] Loss_G: 0.0073\n",
            "PRETRAIN [21/25][9/157] Loss_G: 0.0087\n",
            "PRETRAIN [21/25][10/157] Loss_G: 0.0088\n",
            "PRETRAIN [21/25][11/157] Loss_G: 0.0082\n",
            "PRETRAIN [21/25][12/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][13/157] Loss_G: 0.0076\n",
            "PRETRAIN [21/25][14/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][15/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][16/157] Loss_G: 0.0094\n",
            "PRETRAIN [21/25][17/157] Loss_G: 0.0083\n",
            "PRETRAIN [21/25][18/157] Loss_G: 0.0062\n",
            "PRETRAIN [21/25][19/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][20/157] Loss_G: 0.0081\n",
            "PRETRAIN [21/25][21/157] Loss_G: 0.0069\n",
            "PRETRAIN [21/25][22/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][23/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][24/157] Loss_G: 0.0089\n",
            "PRETRAIN [21/25][25/157] Loss_G: 0.0072\n",
            "PRETRAIN [21/25][26/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][27/157] Loss_G: 0.0091\n",
            "PRETRAIN [21/25][28/157] Loss_G: 0.0079\n",
            "PRETRAIN [21/25][29/157] Loss_G: 0.0086\n",
            "PRETRAIN [21/25][30/157] Loss_G: 0.0083\n",
            "PRETRAIN [21/25][31/157] Loss_G: 0.0080\n",
            "PRETRAIN [21/25][32/157] Loss_G: 0.0065\n",
            "PRETRAIN [21/25][33/157] Loss_G: 0.0081\n",
            "PRETRAIN [21/25][34/157] Loss_G: 0.0056\n",
            "PRETRAIN [21/25][35/157] Loss_G: 0.0072\n",
            "PRETRAIN [21/25][36/157] Loss_G: 0.0070\n",
            "PRETRAIN [21/25][37/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][38/157] Loss_G: 0.0085\n",
            "PRETRAIN [21/25][39/157] Loss_G: 0.0071\n",
            "PRETRAIN [21/25][40/157] Loss_G: 0.0092\n",
            "PRETRAIN [21/25][41/157] Loss_G: 0.0082\n",
            "PRETRAIN [21/25][42/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][43/157] Loss_G: 0.0068\n",
            "PRETRAIN [21/25][44/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][45/157] Loss_G: 0.0079\n",
            "PRETRAIN [21/25][46/157] Loss_G: 0.0088\n",
            "PRETRAIN [21/25][47/157] Loss_G: 0.0143\n",
            "PRETRAIN [21/25][48/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][49/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][50/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][51/157] Loss_G: 0.0081\n",
            "PRETRAIN [21/25][52/157] Loss_G: 0.0091\n",
            "PRETRAIN [21/25][53/157] Loss_G: 0.0088\n",
            "PRETRAIN [21/25][54/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][55/157] Loss_G: 0.0085\n",
            "PRETRAIN [21/25][56/157] Loss_G: 0.0083\n",
            "PRETRAIN [21/25][57/157] Loss_G: 0.0070\n",
            "PRETRAIN [21/25][58/157] Loss_G: 0.0061\n",
            "PRETRAIN [21/25][59/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][60/157] Loss_G: 0.0089\n",
            "PRETRAIN [21/25][61/157] Loss_G: 0.0078\n",
            "PRETRAIN [21/25][62/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][63/157] Loss_G: 0.0075\n",
            "PRETRAIN [21/25][64/157] Loss_G: 0.0090\n",
            "PRETRAIN [21/25][65/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][66/157] Loss_G: 0.0070\n",
            "PRETRAIN [21/25][67/157] Loss_G: 0.0073\n",
            "PRETRAIN [21/25][68/157] Loss_G: 0.0080\n",
            "PRETRAIN [21/25][69/157] Loss_G: 0.0075\n",
            "PRETRAIN [21/25][70/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][71/157] Loss_G: 0.0081\n",
            "PRETRAIN [21/25][72/157] Loss_G: 0.0072\n",
            "PRETRAIN [21/25][73/157] Loss_G: 0.0114\n",
            "PRETRAIN [21/25][74/157] Loss_G: 0.0091\n",
            "PRETRAIN [21/25][75/157] Loss_G: 0.0076\n",
            "PRETRAIN [21/25][76/157] Loss_G: 0.0101\n",
            "PRETRAIN [21/25][77/157] Loss_G: 0.0078\n",
            "PRETRAIN [21/25][78/157] Loss_G: 0.0066\n",
            "PRETRAIN [21/25][79/157] Loss_G: 0.0069\n",
            "PRETRAIN [21/25][80/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][81/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][82/157] Loss_G: 0.0086\n",
            "PRETRAIN [21/25][83/157] Loss_G: 0.0079\n",
            "PRETRAIN [21/25][84/157] Loss_G: 0.0081\n",
            "PRETRAIN [21/25][85/157] Loss_G: 0.0081\n",
            "PRETRAIN [21/25][86/157] Loss_G: 0.0072\n",
            "PRETRAIN [21/25][87/157] Loss_G: 0.0075\n",
            "PRETRAIN [21/25][88/157] Loss_G: 0.0064\n",
            "PRETRAIN [21/25][89/157] Loss_G: 0.0063\n",
            "PRETRAIN [21/25][90/157] Loss_G: 0.0067\n",
            "PRETRAIN [21/25][91/157] Loss_G: 0.0090\n",
            "PRETRAIN [21/25][92/157] Loss_G: 0.0072\n",
            "PRETRAIN [21/25][93/157] Loss_G: 0.0075\n",
            "PRETRAIN [21/25][94/157] Loss_G: 0.0105\n",
            "PRETRAIN [21/25][95/157] Loss_G: 0.0072\n",
            "PRETRAIN [21/25][96/157] Loss_G: 0.0070\n",
            "PRETRAIN [21/25][97/157] Loss_G: 0.0067\n",
            "PRETRAIN [21/25][98/157] Loss_G: 0.0071\n",
            "PRETRAIN [21/25][99/157] Loss_G: 0.0092\n",
            "PRETRAIN [21/25][100/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][101/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][102/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][103/157] Loss_G: 0.0073\n",
            "PRETRAIN [21/25][104/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][105/157] Loss_G: 0.0066\n",
            "PRETRAIN [21/25][106/157] Loss_G: 0.0082\n",
            "PRETRAIN [21/25][107/157] Loss_G: 0.0078\n",
            "PRETRAIN [21/25][108/157] Loss_G: 0.0069\n",
            "PRETRAIN [21/25][109/157] Loss_G: 0.0061\n",
            "PRETRAIN [21/25][110/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][111/157] Loss_G: 0.0078\n",
            "PRETRAIN [21/25][112/157] Loss_G: 0.0079\n",
            "PRETRAIN [21/25][113/157] Loss_G: 0.0074\n",
            "PRETRAIN [21/25][114/157] Loss_G: 0.0080\n",
            "PRETRAIN [21/25][115/157] Loss_G: 0.0062\n",
            "PRETRAIN [21/25][116/157] Loss_G: 0.0101\n",
            "PRETRAIN [21/25][117/157] Loss_G: 0.0076\n",
            "PRETRAIN [21/25][118/157] Loss_G: 0.0137\n",
            "PRETRAIN [21/25][119/157] Loss_G: 0.0079\n",
            "PRETRAIN [21/25][120/157] Loss_G: 0.0109\n",
            "PRETRAIN [21/25][121/157] Loss_G: 0.0098\n",
            "PRETRAIN [21/25][122/157] Loss_G: 0.0092\n",
            "PRETRAIN [21/25][123/157] Loss_G: 0.0091\n",
            "PRETRAIN [21/25][124/157] Loss_G: 0.0096\n",
            "PRETRAIN [21/25][125/157] Loss_G: 0.0075\n",
            "PRETRAIN [21/25][126/157] Loss_G: 0.0097\n",
            "PRETRAIN [21/25][127/157] Loss_G: 0.0097\n",
            "PRETRAIN [21/25][128/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][129/157] Loss_G: 0.0098\n",
            "PRETRAIN [21/25][130/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][131/157] Loss_G: 0.0065\n",
            "PRETRAIN [21/25][132/157] Loss_G: 0.0065\n",
            "PRETRAIN [21/25][133/157] Loss_G: 0.0065\n",
            "PRETRAIN [21/25][134/157] Loss_G: 0.0081\n",
            "PRETRAIN [21/25][135/157] Loss_G: 0.0070\n",
            "PRETRAIN [21/25][136/157] Loss_G: 0.0085\n",
            "PRETRAIN [21/25][137/157] Loss_G: 0.0093\n",
            "PRETRAIN [21/25][138/157] Loss_G: 0.0125\n",
            "PRETRAIN [21/25][139/157] Loss_G: 0.0082\n",
            "PRETRAIN [21/25][140/157] Loss_G: 0.0098\n",
            "PRETRAIN [21/25][141/157] Loss_G: 0.0070\n",
            "PRETRAIN [21/25][142/157] Loss_G: 0.0088\n",
            "PRETRAIN [21/25][143/157] Loss_G: 0.0082\n",
            "PRETRAIN [21/25][144/157] Loss_G: 0.0073\n",
            "PRETRAIN [21/25][145/157] Loss_G: 0.0077\n",
            "PRETRAIN [21/25][146/157] Loss_G: 0.0082\n",
            "PRETRAIN [21/25][147/157] Loss_G: 0.0092\n",
            "PRETRAIN [21/25][148/157] Loss_G: 0.0084\n",
            "PRETRAIN [21/25][149/157] Loss_G: 0.0068\n",
            "PRETRAIN [21/25][150/157] Loss_G: 0.0100\n",
            "PRETRAIN [21/25][151/157] Loss_G: 0.0078\n",
            "PRETRAIN [21/25][152/157] Loss_G: 0.0080\n",
            "PRETRAIN [21/25][153/157] Loss_G: 0.0093\n",
            "PRETRAIN [21/25][154/157] Loss_G: 0.0076\n",
            "PRETRAIN [21/25][155/157] Loss_G: 0.0070\n",
            "PRETRAIN [21/25][156/157] Loss_G: 0.0075\n",
            "Time elapsed Epoch 21: 115 seconds\n",
            "PRETRAIN [22/25][0/157] Loss_G: 0.0084\n",
            "PRETRAIN [22/25][1/157] Loss_G: 0.0087\n",
            "PRETRAIN [22/25][2/157] Loss_G: 0.0088\n",
            "PRETRAIN [22/25][3/157] Loss_G: 0.0069\n",
            "PRETRAIN [22/25][4/157] Loss_G: 0.0092\n",
            "PRETRAIN [22/25][5/157] Loss_G: 0.0062\n",
            "PRETRAIN [22/25][6/157] Loss_G: 0.0080\n",
            "PRETRAIN [22/25][7/157] Loss_G: 0.0072\n",
            "PRETRAIN [22/25][8/157] Loss_G: 0.0078\n",
            "PRETRAIN [22/25][9/157] Loss_G: 0.0071\n",
            "PRETRAIN [22/25][10/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][11/157] Loss_G: 0.0082\n",
            "PRETRAIN [22/25][12/157] Loss_G: 0.0078\n",
            "PRETRAIN [22/25][13/157] Loss_G: 0.0081\n",
            "PRETRAIN [22/25][14/157] Loss_G: 0.0067\n",
            "PRETRAIN [22/25][15/157] Loss_G: 0.0067\n",
            "PRETRAIN [22/25][16/157] Loss_G: 0.0107\n",
            "PRETRAIN [22/25][17/157] Loss_G: 0.0072\n",
            "PRETRAIN [22/25][18/157] Loss_G: 0.0087\n",
            "PRETRAIN [22/25][19/157] Loss_G: 0.0090\n",
            "PRETRAIN [22/25][20/157] Loss_G: 0.0069\n",
            "PRETRAIN [22/25][21/157] Loss_G: 0.0082\n",
            "PRETRAIN [22/25][22/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][23/157] Loss_G: 0.0077\n",
            "PRETRAIN [22/25][24/157] Loss_G: 0.0092\n",
            "PRETRAIN [22/25][25/157] Loss_G: 0.0072\n",
            "PRETRAIN [22/25][26/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][27/157] Loss_G: 0.0063\n",
            "PRETRAIN [22/25][28/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][29/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][30/157] Loss_G: 0.0083\n",
            "PRETRAIN [22/25][31/157] Loss_G: 0.0080\n",
            "PRETRAIN [22/25][32/157] Loss_G: 0.0089\n",
            "PRETRAIN [22/25][33/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][34/157] Loss_G: 0.0060\n",
            "PRETRAIN [22/25][35/157] Loss_G: 0.0065\n",
            "PRETRAIN [22/25][36/157] Loss_G: 0.0068\n",
            "PRETRAIN [22/25][37/157] Loss_G: 0.0069\n",
            "PRETRAIN [22/25][38/157] Loss_G: 0.0060\n",
            "PRETRAIN [22/25][39/157] Loss_G: 0.0085\n",
            "PRETRAIN [22/25][40/157] Loss_G: 0.0063\n",
            "PRETRAIN [22/25][41/157] Loss_G: 0.0068\n",
            "PRETRAIN [22/25][42/157] Loss_G: 0.0097\n",
            "PRETRAIN [22/25][43/157] Loss_G: 0.0077\n",
            "PRETRAIN [22/25][44/157] Loss_G: 0.0103\n",
            "PRETRAIN [22/25][45/157] Loss_G: 0.0074\n",
            "PRETRAIN [22/25][46/157] Loss_G: 0.0084\n",
            "PRETRAIN [22/25][47/157] Loss_G: 0.0071\n",
            "PRETRAIN [22/25][48/157] Loss_G: 0.0079\n",
            "PRETRAIN [22/25][49/157] Loss_G: 0.0084\n",
            "PRETRAIN [22/25][50/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][51/157] Loss_G: 0.0069\n",
            "PRETRAIN [22/25][52/157] Loss_G: 0.0066\n",
            "PRETRAIN [22/25][53/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][54/157] Loss_G: 0.0081\n",
            "PRETRAIN [22/25][55/157] Loss_G: 0.0074\n",
            "PRETRAIN [22/25][56/157] Loss_G: 0.0085\n",
            "PRETRAIN [22/25][57/157] Loss_G: 0.0098\n",
            "PRETRAIN [22/25][58/157] Loss_G: 0.0068\n",
            "PRETRAIN [22/25][59/157] Loss_G: 0.0081\n",
            "PRETRAIN [22/25][60/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][61/157] Loss_G: 0.0072\n",
            "PRETRAIN [22/25][62/157] Loss_G: 0.0078\n",
            "PRETRAIN [22/25][63/157] Loss_G: 0.0087\n",
            "PRETRAIN [22/25][64/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][65/157] Loss_G: 0.0068\n",
            "PRETRAIN [22/25][66/157] Loss_G: 0.0087\n",
            "PRETRAIN [22/25][67/157] Loss_G: 0.0069\n",
            "PRETRAIN [22/25][68/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][69/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][70/157] Loss_G: 0.0079\n",
            "PRETRAIN [22/25][71/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][72/157] Loss_G: 0.0062\n",
            "PRETRAIN [22/25][73/157] Loss_G: 0.0059\n",
            "PRETRAIN [22/25][74/157] Loss_G: 0.0061\n",
            "PRETRAIN [22/25][75/157] Loss_G: 0.0059\n",
            "PRETRAIN [22/25][76/157] Loss_G: 0.0077\n",
            "PRETRAIN [22/25][77/157] Loss_G: 0.0080\n",
            "PRETRAIN [22/25][78/157] Loss_G: 0.0091\n",
            "PRETRAIN [22/25][79/157] Loss_G: 0.0074\n",
            "PRETRAIN [22/25][80/157] Loss_G: 0.0075\n",
            "PRETRAIN [22/25][81/157] Loss_G: 0.0075\n",
            "PRETRAIN [22/25][82/157] Loss_G: 0.0072\n",
            "PRETRAIN [22/25][83/157] Loss_G: 0.0085\n",
            "PRETRAIN [22/25][84/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][85/157] Loss_G: 0.0074\n",
            "PRETRAIN [22/25][86/157] Loss_G: 0.0090\n",
            "PRETRAIN [22/25][87/157] Loss_G: 0.0083\n",
            "PRETRAIN [22/25][88/157] Loss_G: 0.0084\n",
            "PRETRAIN [22/25][89/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][90/157] Loss_G: 0.0112\n",
            "PRETRAIN [22/25][91/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][92/157] Loss_G: 0.0071\n",
            "PRETRAIN [22/25][93/157] Loss_G: 0.0068\n",
            "PRETRAIN [22/25][94/157] Loss_G: 0.0067\n",
            "PRETRAIN [22/25][95/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][96/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][97/157] Loss_G: 0.0091\n",
            "PRETRAIN [22/25][98/157] Loss_G: 0.0094\n",
            "PRETRAIN [22/25][99/157] Loss_G: 0.0072\n",
            "PRETRAIN [22/25][100/157] Loss_G: 0.0077\n",
            "PRETRAIN [22/25][101/157] Loss_G: 0.0075\n",
            "PRETRAIN [22/25][102/157] Loss_G: 0.0077\n",
            "PRETRAIN [22/25][103/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][104/157] Loss_G: 0.0076\n",
            "PRETRAIN [22/25][105/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][106/157] Loss_G: 0.0084\n",
            "PRETRAIN [22/25][107/157] Loss_G: 0.0094\n",
            "PRETRAIN [22/25][108/157] Loss_G: 0.0079\n",
            "PRETRAIN [22/25][109/157] Loss_G: 0.0060\n",
            "PRETRAIN [22/25][110/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][111/157] Loss_G: 0.0054\n",
            "PRETRAIN [22/25][112/157] Loss_G: 0.0084\n",
            "PRETRAIN [22/25][113/157] Loss_G: 0.0082\n",
            "PRETRAIN [22/25][114/157] Loss_G: 0.0062\n",
            "PRETRAIN [22/25][115/157] Loss_G: 0.0097\n",
            "PRETRAIN [22/25][116/157] Loss_G: 0.0087\n",
            "PRETRAIN [22/25][117/157] Loss_G: 0.0071\n",
            "PRETRAIN [22/25][118/157] Loss_G: 0.0078\n",
            "PRETRAIN [22/25][119/157] Loss_G: 0.0075\n",
            "PRETRAIN [22/25][120/157] Loss_G: 0.0074\n",
            "PRETRAIN [22/25][121/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][122/157] Loss_G: 0.0093\n",
            "PRETRAIN [22/25][123/157] Loss_G: 0.0077\n",
            "PRETRAIN [22/25][124/157] Loss_G: 0.0063\n",
            "PRETRAIN [22/25][125/157] Loss_G: 0.0083\n",
            "PRETRAIN [22/25][126/157] Loss_G: 0.0068\n",
            "PRETRAIN [22/25][127/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][128/157] Loss_G: 0.0072\n",
            "PRETRAIN [22/25][129/157] Loss_G: 0.0076\n",
            "PRETRAIN [22/25][130/157] Loss_G: 0.0057\n",
            "PRETRAIN [22/25][131/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][132/157] Loss_G: 0.0086\n",
            "PRETRAIN [22/25][133/157] Loss_G: 0.0071\n",
            "PRETRAIN [22/25][134/157] Loss_G: 0.0079\n",
            "PRETRAIN [22/25][135/157] Loss_G: 0.0075\n",
            "PRETRAIN [22/25][136/157] Loss_G: 0.0093\n",
            "PRETRAIN [22/25][137/157] Loss_G: 0.0069\n",
            "PRETRAIN [22/25][138/157] Loss_G: 0.0081\n",
            "PRETRAIN [22/25][139/157] Loss_G: 0.0062\n",
            "PRETRAIN [22/25][140/157] Loss_G: 0.0070\n",
            "PRETRAIN [22/25][141/157] Loss_G: 0.0075\n",
            "PRETRAIN [22/25][142/157] Loss_G: 0.0067\n",
            "PRETRAIN [22/25][143/157] Loss_G: 0.0073\n",
            "PRETRAIN [22/25][144/157] Loss_G: 0.0071\n",
            "PRETRAIN [22/25][145/157] Loss_G: 0.0078\n",
            "PRETRAIN [22/25][146/157] Loss_G: 0.0078\n",
            "PRETRAIN [22/25][147/157] Loss_G: 0.0056\n",
            "PRETRAIN [22/25][148/157] Loss_G: 0.0066\n",
            "PRETRAIN [22/25][149/157] Loss_G: 0.0065\n",
            "PRETRAIN [22/25][150/157] Loss_G: 0.0067\n",
            "PRETRAIN [22/25][151/157] Loss_G: 0.0079\n",
            "PRETRAIN [22/25][152/157] Loss_G: 0.0064\n",
            "PRETRAIN [22/25][153/157] Loss_G: 0.0063\n",
            "PRETRAIN [22/25][154/157] Loss_G: 0.0074\n",
            "PRETRAIN [22/25][155/157] Loss_G: 0.0065\n",
            "PRETRAIN [22/25][156/157] Loss_G: 0.0142\n",
            "Time elapsed Epoch 22: 115 seconds\n",
            "PRETRAIN [23/25][0/157] Loss_G: 0.0078\n",
            "PRETRAIN [23/25][1/157] Loss_G: 0.0108\n",
            "PRETRAIN [23/25][2/157] Loss_G: 0.0115\n",
            "PRETRAIN [23/25][3/157] Loss_G: 0.0073\n",
            "PRETRAIN [23/25][4/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][5/157] Loss_G: 0.0085\n",
            "PRETRAIN [23/25][6/157] Loss_G: 0.0107\n",
            "PRETRAIN [23/25][7/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][8/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][9/157] Loss_G: 0.0087\n",
            "PRETRAIN [23/25][10/157] Loss_G: 0.0089\n",
            "PRETRAIN [23/25][11/157] Loss_G: 0.0085\n",
            "PRETRAIN [23/25][12/157] Loss_G: 0.0078\n",
            "PRETRAIN [23/25][13/157] Loss_G: 0.0088\n",
            "PRETRAIN [23/25][14/157] Loss_G: 0.0076\n",
            "PRETRAIN [23/25][15/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][16/157] Loss_G: 0.0070\n",
            "PRETRAIN [23/25][17/157] Loss_G: 0.0070\n",
            "PRETRAIN [23/25][18/157] Loss_G: 0.0073\n",
            "PRETRAIN [23/25][19/157] Loss_G: 0.0059\n",
            "PRETRAIN [23/25][20/157] Loss_G: 0.0060\n",
            "PRETRAIN [23/25][21/157] Loss_G: 0.0063\n",
            "PRETRAIN [23/25][22/157] Loss_G: 0.0085\n",
            "PRETRAIN [23/25][23/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][24/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][25/157] Loss_G: 0.0073\n",
            "PRETRAIN [23/25][26/157] Loss_G: 0.0061\n",
            "PRETRAIN [23/25][27/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][28/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][29/157] Loss_G: 0.0083\n",
            "PRETRAIN [23/25][30/157] Loss_G: 0.0073\n",
            "PRETRAIN [23/25][31/157] Loss_G: 0.0104\n",
            "PRETRAIN [23/25][32/157] Loss_G: 0.0087\n",
            "PRETRAIN [23/25][33/157] Loss_G: 0.0081\n",
            "PRETRAIN [23/25][34/157] Loss_G: 0.0069\n",
            "PRETRAIN [23/25][35/157] Loss_G: 0.0081\n",
            "PRETRAIN [23/25][36/157] Loss_G: 0.0061\n",
            "PRETRAIN [23/25][37/157] Loss_G: 0.0069\n",
            "PRETRAIN [23/25][38/157] Loss_G: 0.0078\n",
            "PRETRAIN [23/25][39/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][40/157] Loss_G: 0.0073\n",
            "PRETRAIN [23/25][41/157] Loss_G: 0.0087\n",
            "PRETRAIN [23/25][42/157] Loss_G: 0.0080\n",
            "PRETRAIN [23/25][43/157] Loss_G: 0.0100\n",
            "PRETRAIN [23/25][44/157] Loss_G: 0.0067\n",
            "PRETRAIN [23/25][45/157] Loss_G: 0.0082\n",
            "PRETRAIN [23/25][46/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][47/157] Loss_G: 0.0081\n",
            "PRETRAIN [23/25][48/157] Loss_G: 0.0061\n",
            "PRETRAIN [23/25][49/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][50/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][51/157] Loss_G: 0.0066\n",
            "PRETRAIN [23/25][52/157] Loss_G: 0.0079\n",
            "PRETRAIN [23/25][53/157] Loss_G: 0.0072\n",
            "PRETRAIN [23/25][54/157] Loss_G: 0.0059\n",
            "PRETRAIN [23/25][55/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][56/157] Loss_G: 0.0081\n",
            "PRETRAIN [23/25][57/157] Loss_G: 0.0070\n",
            "PRETRAIN [23/25][58/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][59/157] Loss_G: 0.0073\n",
            "PRETRAIN [23/25][60/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][61/157] Loss_G: 0.0072\n",
            "PRETRAIN [23/25][62/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][63/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][64/157] Loss_G: 0.0075\n",
            "PRETRAIN [23/25][65/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][66/157] Loss_G: 0.0101\n",
            "PRETRAIN [23/25][67/157] Loss_G: 0.0083\n",
            "PRETRAIN [23/25][68/157] Loss_G: 0.0075\n",
            "PRETRAIN [23/25][69/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][70/157] Loss_G: 0.0082\n",
            "PRETRAIN [23/25][71/157] Loss_G: 0.0061\n",
            "PRETRAIN [23/25][72/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][73/157] Loss_G: 0.0064\n",
            "PRETRAIN [23/25][74/157] Loss_G: 0.0075\n",
            "PRETRAIN [23/25][75/157] Loss_G: 0.0076\n",
            "PRETRAIN [23/25][76/157] Loss_G: 0.0070\n",
            "PRETRAIN [23/25][77/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][78/157] Loss_G: 0.0087\n",
            "PRETRAIN [23/25][79/157] Loss_G: 0.0084\n",
            "PRETRAIN [23/25][80/157] Loss_G: 0.0086\n",
            "PRETRAIN [23/25][81/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][82/157] Loss_G: 0.0075\n",
            "PRETRAIN [23/25][83/157] Loss_G: 0.0080\n",
            "PRETRAIN [23/25][84/157] Loss_G: 0.0066\n",
            "PRETRAIN [23/25][85/157] Loss_G: 0.0070\n",
            "PRETRAIN [23/25][86/157] Loss_G: 0.0067\n",
            "PRETRAIN [23/25][87/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][88/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][89/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][90/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][91/157] Loss_G: 0.0053\n",
            "PRETRAIN [23/25][92/157] Loss_G: 0.0064\n",
            "PRETRAIN [23/25][93/157] Loss_G: 0.0062\n",
            "PRETRAIN [23/25][94/157] Loss_G: 0.0061\n",
            "PRETRAIN [23/25][95/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][96/157] Loss_G: 0.0078\n",
            "PRETRAIN [23/25][97/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][98/157] Loss_G: 0.0066\n",
            "PRETRAIN [23/25][99/157] Loss_G: 0.0062\n",
            "PRETRAIN [23/25][100/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][101/157] Loss_G: 0.0090\n",
            "PRETRAIN [23/25][102/157] Loss_G: 0.0081\n",
            "PRETRAIN [23/25][103/157] Loss_G: 0.0079\n",
            "PRETRAIN [23/25][104/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][105/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][106/157] Loss_G: 0.0072\n",
            "PRETRAIN [23/25][107/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][108/157] Loss_G: 0.0079\n",
            "PRETRAIN [23/25][109/157] Loss_G: 0.0087\n",
            "PRETRAIN [23/25][110/157] Loss_G: 0.0079\n",
            "PRETRAIN [23/25][111/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][112/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][113/157] Loss_G: 0.0066\n",
            "PRETRAIN [23/25][114/157] Loss_G: 0.0089\n",
            "PRETRAIN [23/25][115/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][116/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][117/157] Loss_G: 0.0085\n",
            "PRETRAIN [23/25][118/157] Loss_G: 0.0087\n",
            "PRETRAIN [23/25][119/157] Loss_G: 0.0063\n",
            "PRETRAIN [23/25][120/157] Loss_G: 0.0069\n",
            "PRETRAIN [23/25][121/157] Loss_G: 0.0080\n",
            "PRETRAIN [23/25][122/157] Loss_G: 0.0067\n",
            "PRETRAIN [23/25][123/157] Loss_G: 0.0069\n",
            "PRETRAIN [23/25][124/157] Loss_G: 0.0072\n",
            "PRETRAIN [23/25][125/157] Loss_G: 0.0089\n",
            "PRETRAIN [23/25][126/157] Loss_G: 0.0082\n",
            "PRETRAIN [23/25][127/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][128/157] Loss_G: 0.0073\n",
            "PRETRAIN [23/25][129/157] Loss_G: 0.0079\n",
            "PRETRAIN [23/25][130/157] Loss_G: 0.0080\n",
            "PRETRAIN [23/25][131/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][132/157] Loss_G: 0.0069\n",
            "PRETRAIN [23/25][133/157] Loss_G: 0.0086\n",
            "PRETRAIN [23/25][134/157] Loss_G: 0.0082\n",
            "PRETRAIN [23/25][135/157] Loss_G: 0.0079\n",
            "PRETRAIN [23/25][136/157] Loss_G: 0.0067\n",
            "PRETRAIN [23/25][137/157] Loss_G: 0.0071\n",
            "PRETRAIN [23/25][138/157] Loss_G: 0.0105\n",
            "PRETRAIN [23/25][139/157] Loss_G: 0.0084\n",
            "PRETRAIN [23/25][140/157] Loss_G: 0.0083\n",
            "PRETRAIN [23/25][141/157] Loss_G: 0.0074\n",
            "PRETRAIN [23/25][142/157] Loss_G: 0.0107\n",
            "PRETRAIN [23/25][143/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][144/157] Loss_G: 0.0077\n",
            "PRETRAIN [23/25][145/157] Loss_G: 0.0078\n",
            "PRETRAIN [23/25][146/157] Loss_G: 0.0069\n",
            "PRETRAIN [23/25][147/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][148/157] Loss_G: 0.0072\n",
            "PRETRAIN [23/25][149/157] Loss_G: 0.0066\n",
            "PRETRAIN [23/25][150/157] Loss_G: 0.0067\n",
            "PRETRAIN [23/25][151/157] Loss_G: 0.0076\n",
            "PRETRAIN [23/25][152/157] Loss_G: 0.0068\n",
            "PRETRAIN [23/25][153/157] Loss_G: 0.0092\n",
            "PRETRAIN [23/25][154/157] Loss_G: 0.0059\n",
            "PRETRAIN [23/25][155/157] Loss_G: 0.0065\n",
            "PRETRAIN [23/25][156/157] Loss_G: 0.0073\n",
            "Time elapsed Epoch 23: 115 seconds\n",
            "PRETRAIN [24/25][0/157] Loss_G: 0.0101\n",
            "PRETRAIN [24/25][1/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][2/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][3/157] Loss_G: 0.0079\n",
            "PRETRAIN [24/25][4/157] Loss_G: 0.0084\n",
            "PRETRAIN [24/25][5/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][6/157] Loss_G: 0.0062\n",
            "PRETRAIN [24/25][7/157] Loss_G: 0.0072\n",
            "PRETRAIN [24/25][8/157] Loss_G: 0.0068\n",
            "PRETRAIN [24/25][9/157] Loss_G: 0.0065\n",
            "PRETRAIN [24/25][10/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][11/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][12/157] Loss_G: 0.0069\n",
            "PRETRAIN [24/25][13/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][14/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][15/157] Loss_G: 0.0066\n",
            "PRETRAIN [24/25][16/157] Loss_G: 0.0118\n",
            "PRETRAIN [24/25][17/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][18/157] Loss_G: 0.0059\n",
            "PRETRAIN [24/25][19/157] Loss_G: 0.0076\n",
            "PRETRAIN [24/25][20/157] Loss_G: 0.0063\n",
            "PRETRAIN [24/25][21/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][22/157] Loss_G: 0.0071\n",
            "PRETRAIN [24/25][23/157] Loss_G: 0.0059\n",
            "PRETRAIN [24/25][24/157] Loss_G: 0.0084\n",
            "PRETRAIN [24/25][25/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][26/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][27/157] Loss_G: 0.0061\n",
            "PRETRAIN [24/25][28/157] Loss_G: 0.0081\n",
            "PRETRAIN [24/25][29/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][30/157] Loss_G: 0.0062\n",
            "PRETRAIN [24/25][31/157] Loss_G: 0.0069\n",
            "PRETRAIN [24/25][32/157] Loss_G: 0.0064\n",
            "PRETRAIN [24/25][33/157] Loss_G: 0.0093\n",
            "PRETRAIN [24/25][34/157] Loss_G: 0.0070\n",
            "PRETRAIN [24/25][35/157] Loss_G: 0.0080\n",
            "PRETRAIN [24/25][36/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][37/157] Loss_G: 0.0059\n",
            "PRETRAIN [24/25][38/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][39/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][40/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][41/157] Loss_G: 0.0079\n",
            "PRETRAIN [24/25][42/157] Loss_G: 0.0064\n",
            "PRETRAIN [24/25][43/157] Loss_G: 0.0072\n",
            "PRETRAIN [24/25][44/157] Loss_G: 0.0074\n",
            "PRETRAIN [24/25][45/157] Loss_G: 0.0065\n",
            "PRETRAIN [24/25][46/157] Loss_G: 0.0070\n",
            "PRETRAIN [24/25][47/157] Loss_G: 0.0069\n",
            "PRETRAIN [24/25][48/157] Loss_G: 0.0071\n",
            "PRETRAIN [24/25][49/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][50/157] Loss_G: 0.0090\n",
            "PRETRAIN [24/25][51/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][52/157] Loss_G: 0.0090\n",
            "PRETRAIN [24/25][53/157] Loss_G: 0.0068\n",
            "PRETRAIN [24/25][54/157] Loss_G: 0.0086\n",
            "PRETRAIN [24/25][55/157] Loss_G: 0.0076\n",
            "PRETRAIN [24/25][56/157] Loss_G: 0.0069\n",
            "PRETRAIN [24/25][57/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][58/157] Loss_G: 0.0077\n",
            "PRETRAIN [24/25][59/157] Loss_G: 0.0069\n",
            "PRETRAIN [24/25][60/157] Loss_G: 0.0086\n",
            "PRETRAIN [24/25][61/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][62/157] Loss_G: 0.0079\n",
            "PRETRAIN [24/25][63/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][64/157] Loss_G: 0.0072\n",
            "PRETRAIN [24/25][65/157] Loss_G: 0.0065\n",
            "PRETRAIN [24/25][66/157] Loss_G: 0.0060\n",
            "PRETRAIN [24/25][67/157] Loss_G: 0.0071\n",
            "PRETRAIN [24/25][68/157] Loss_G: 0.0064\n",
            "PRETRAIN [24/25][69/157] Loss_G: 0.0074\n",
            "PRETRAIN [24/25][70/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][71/157] Loss_G: 0.0083\n",
            "PRETRAIN [24/25][72/157] Loss_G: 0.0092\n",
            "PRETRAIN [24/25][73/157] Loss_G: 0.0127\n",
            "PRETRAIN [24/25][74/157] Loss_G: 0.0191\n",
            "PRETRAIN [24/25][75/157] Loss_G: 0.0157\n",
            "PRETRAIN [24/25][76/157] Loss_G: 0.0121\n",
            "PRETRAIN [24/25][77/157] Loss_G: 0.0120\n",
            "PRETRAIN [24/25][78/157] Loss_G: 0.0123\n",
            "PRETRAIN [24/25][79/157] Loss_G: 0.0099\n",
            "PRETRAIN [24/25][80/157] Loss_G: 0.0102\n",
            "PRETRAIN [24/25][81/157] Loss_G: 0.0103\n",
            "PRETRAIN [24/25][82/157] Loss_G: 0.0090\n",
            "PRETRAIN [24/25][83/157] Loss_G: 0.0118\n",
            "PRETRAIN [24/25][84/157] Loss_G: 0.0090\n",
            "PRETRAIN [24/25][85/157] Loss_G: 0.0110\n",
            "PRETRAIN [24/25][86/157] Loss_G: 0.0096\n",
            "PRETRAIN [24/25][87/157] Loss_G: 0.0082\n",
            "PRETRAIN [24/25][88/157] Loss_G: 0.0109\n",
            "PRETRAIN [24/25][89/157] Loss_G: 0.0090\n",
            "PRETRAIN [24/25][90/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][91/157] Loss_G: 0.0092\n",
            "PRETRAIN [24/25][92/157] Loss_G: 0.0082\n",
            "PRETRAIN [24/25][93/157] Loss_G: 0.0089\n",
            "PRETRAIN [24/25][94/157] Loss_G: 0.0089\n",
            "PRETRAIN [24/25][95/157] Loss_G: 0.0083\n",
            "PRETRAIN [24/25][96/157] Loss_G: 0.0085\n",
            "PRETRAIN [24/25][97/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][98/157] Loss_G: 0.0090\n",
            "PRETRAIN [24/25][99/157] Loss_G: 0.0086\n",
            "PRETRAIN [24/25][100/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][101/157] Loss_G: 0.0072\n",
            "PRETRAIN [24/25][102/157] Loss_G: 0.0072\n",
            "PRETRAIN [24/25][103/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][104/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][105/157] Loss_G: 0.0087\n",
            "PRETRAIN [24/25][106/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][107/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][108/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][109/157] Loss_G: 0.0068\n",
            "PRETRAIN [24/25][110/157] Loss_G: 0.0081\n",
            "PRETRAIN [24/25][111/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][112/157] Loss_G: 0.0080\n",
            "PRETRAIN [24/25][113/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][114/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][115/157] Loss_G: 0.0074\n",
            "PRETRAIN [24/25][116/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][117/157] Loss_G: 0.0068\n",
            "PRETRAIN [24/25][118/157] Loss_G: 0.0077\n",
            "PRETRAIN [24/25][119/157] Loss_G: 0.0100\n",
            "PRETRAIN [24/25][120/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][121/157] Loss_G: 0.0081\n",
            "PRETRAIN [24/25][122/157] Loss_G: 0.0088\n",
            "PRETRAIN [24/25][123/157] Loss_G: 0.0072\n",
            "PRETRAIN [24/25][124/157] Loss_G: 0.0085\n",
            "PRETRAIN [24/25][125/157] Loss_G: 0.0071\n",
            "PRETRAIN [24/25][126/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][127/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][128/157] Loss_G: 0.0079\n",
            "PRETRAIN [24/25][129/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][130/157] Loss_G: 0.0061\n",
            "PRETRAIN [24/25][131/157] Loss_G: 0.0071\n",
            "PRETRAIN [24/25][132/157] Loss_G: 0.0076\n",
            "PRETRAIN [24/25][133/157] Loss_G: 0.0080\n",
            "PRETRAIN [24/25][134/157] Loss_G: 0.0086\n",
            "PRETRAIN [24/25][135/157] Loss_G: 0.0122\n",
            "PRETRAIN [24/25][136/157] Loss_G: 0.0066\n",
            "PRETRAIN [24/25][137/157] Loss_G: 0.0091\n",
            "PRETRAIN [24/25][138/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][139/157] Loss_G: 0.0075\n",
            "PRETRAIN [24/25][140/157] Loss_G: 0.0082\n",
            "PRETRAIN [24/25][141/157] Loss_G: 0.0083\n",
            "PRETRAIN [24/25][142/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][143/157] Loss_G: 0.0073\n",
            "PRETRAIN [24/25][144/157] Loss_G: 0.0084\n",
            "PRETRAIN [24/25][145/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][146/157] Loss_G: 0.0095\n",
            "PRETRAIN [24/25][147/157] Loss_G: 0.0091\n",
            "PRETRAIN [24/25][148/157] Loss_G: 0.0079\n",
            "PRETRAIN [24/25][149/157] Loss_G: 0.0066\n",
            "PRETRAIN [24/25][150/157] Loss_G: 0.0078\n",
            "PRETRAIN [24/25][151/157] Loss_G: 0.0087\n",
            "PRETRAIN [24/25][152/157] Loss_G: 0.0066\n",
            "PRETRAIN [24/25][153/157] Loss_G: 0.0083\n",
            "PRETRAIN [24/25][154/157] Loss_G: 0.0085\n",
            "PRETRAIN [24/25][155/157] Loss_G: 0.0067\n",
            "PRETRAIN [24/25][156/157] Loss_G: 0.0115\n",
            "Time elapsed Epoch 24: 115 seconds\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(niter):\n",
        "    t0 = time.time()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "        # generate masks\n",
        "        points_batch, mask_batch = get_points()\n",
        "        real_data = data[0].to(device)\n",
        "        batch_size = real_data.size(0)\n",
        "\n",
        "        temp = torch.from_numpy(mask_batch)\n",
        "        masks = temp.type(torch.FloatTensor).cuda()\n",
        "        if real_data.size(0) < batchSize:\n",
        "            masks_narrowed = masks.narrow(0, 0, real_data.size(0))\n",
        "            masked_data = real_data * (1 - masks_narrowed)\n",
        "        else:\n",
        "            masked_data = real_data * (1 - masks)\n",
        "\n",
        "        if epoch <= preniter:\n",
        "            optG.zero_grad()\n",
        "            gen_data = netG(masked_data)\n",
        "            errG = criterion2(gen_data, real_data)\n",
        "            errG.backward()\n",
        "            optG.step()\n",
        "\n",
        "            print('PRETRAIN [%d/%d][%d/%d] Loss_G: %.4f'\n",
        "                  % (epoch, niter, i, len(train_loader), errG.item()))\n",
        "\n",
        "            errG_all.update(errG.item())\n",
        "\n",
        "        else:\n",
        "            optD.zero_grad()\n",
        "            local_real_data = crop_local_patches(real_data, points_batch, batch_size)\n",
        "            label = torch.full((batch_size,), real_label, device=device)\n",
        "            out = netD(real_data, local_real_data)\n",
        "            errD_real = criterion(out, label)\n",
        "\n",
        "            # train Discriminator with fake samples\n",
        "            temp = torch.from_numpy(mask_batch)\n",
        "            masks = temp.type(torch.FloatTensor).cuda()\n",
        "            if real_data.size(0) < batchSize:\n",
        "                masks_narrowed = masks.narrow(0, 0, real_data.size(0))\n",
        "                masked_data = real_data * (1 - masks_narrowed)\n",
        "            else:\n",
        "                masked_data = real_data * (1 - masks)\n",
        "\n",
        "            gen_data = netG(masked_data)\n",
        "            label_f = label.clone()\n",
        "            label_f.fill_(fake_label)\n",
        "            local_gen_data = crop_local_patches(gen_data, points_batch, batch_size)\n",
        "            out = netD(gen_data.detach(), local_gen_data.detach())\n",
        "            errD_fake = criterion(out, label_f)\n",
        "            errD = (errD_real + errD_fake) * alpha\n",
        "            errD.backward()\n",
        "            optD.step()\n",
        "\n",
        "            # update Generator\n",
        "            optG.zero_grad()\n",
        "            out = netD(gen_data, local_gen_data)\n",
        "            errG = criterion2(gen_data, real_data)\n",
        "            errG.backward()\n",
        "            optG.step()\n",
        "\n",
        "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
        "                % (epoch, niter, i, len(train_loader), errD.item(), errG.item()))\n",
        "\n",
        "            errD_all.update(errD.item())\n",
        "            errG_all.update(errG.item())\n",
        "\n",
        "\n",
        "\n",
        "    print('Time elapsed Epoch %d: %d seconds'\n",
        "          % (epoch, time.time() - t0))\n",
        "\n",
        "    # save model parameters (last epoch)\n",
        "    #save_checkpoint({\n",
        "    #    'epoch': epoch + 1,\n",
        "    #    'state_dict': netG.state_dict(),\n",
        "    #    'optimizer': optG.state_dict(),\n",
        "    #}, epoch+1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(netG, 'G_weights')\n",
        "torch.save(netD, 'D_weights')\n"
      ],
      "metadata": {
        "id": "4mJvJISlF51X"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('outputs'):\n",
        "  os.makedirs('outputs')\n",
        ""
      ],
      "metadata": {
        "id": "4l4zhudcA-Fb"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valdir = os.path.join(dataPath, 'val')"
      ],
      "metadata": {
        "id": "NFkLzyMRSe5h"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(\n",
        "        dsets.ImageFolder(valdir, transforms.Compose([\n",
        "            transforms.CenterCrop(128),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                std=[0.5, 0.5, 0.5])\n",
        "        ])),\n",
        "        batch_size=batchSize, shuffle=True,\n",
        "        num_workers=workers, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ULmSi6IpSetE"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrained weights\n",
        "valG = Generator().to(device)\n",
        "state_dict = torch.load('G_weights')\n",
        "print(state_dict)\n",
        "#valG.load_state_dict(state_dict)\n",
        "valG.eval()\n",
        "#print(valG)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KgacEL8Svd6",
        "outputId": "e3ebcca9-1dab-4e79-a25f-8cbfaa058d89"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
            "    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
            "    (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (23): ReLU(inplace=True)\n",
            "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
            "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
            "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (34): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (38): ReLU(inplace=True)\n",
            "    (39): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (41): ReLU(inplace=True)\n",
            "    (42): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (44): ReLU(inplace=True)\n",
            "    (45): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (46): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (47): ReLU(inplace=True)\n",
            "    (48): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (49): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
              "    (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (23): ReLU(inplace=True)\n",
              "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
              "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
              "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (34): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (38): ReLU(inplace=True)\n",
              "    (39): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (41): ReLU(inplace=True)\n",
              "    (42): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (44): ReLU(inplace=True)\n",
              "    (45): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (46): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (47): ReLU(inplace=True)\n",
              "    (48): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (49): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_errstr = \"Mode is unknown or incompatible with input array shape.\"\n",
        "\n",
        "\n",
        "def bytescale(data, cmin=None, cmax=None, high=255, low=0):\n",
        "    if data.dtype == np.uint8:\n",
        "        return data\n",
        "\n",
        "    if high > 255:\n",
        "        raise ValueError(\"`high` should be less than or equal to 255.\")\n",
        "    if low < 0:\n",
        "        raise ValueError(\"`low` should be greater than or equal to 0.\")\n",
        "    if high < low:\n",
        "        raise ValueError(\"`high` should be greater than or equal to `low`.\")\n",
        "\n",
        "    if cmin is None:\n",
        "        cmin = data.min()\n",
        "    if cmax is None:\n",
        "        cmax = data.max()\n",
        "\n",
        "    cscale = cmax - cmin\n",
        "    if cscale < 0:\n",
        "        raise ValueError(\"`cmax` should be larger than `cmin`.\")\n",
        "    elif cscale == 0:\n",
        "        cscale = 1\n",
        "\n",
        "    scale = float(high - low) / cscale\n",
        "    bytedata = (data - cmin) * scale + low\n",
        "    return (bytedata.clip(low, high) + 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "def toimage(arr, high=255, low=0, cmin=None, cmax=None, pal=None,\n",
        "            mode=None, channel_axis=None):\n",
        "    data = np.asarray(arr)\n",
        "    if np.iscomplexobj(data):\n",
        "        raise ValueError(\"Cannot convert a complex-valued array.\")\n",
        "    shape = list(data.shape)\n",
        "    valid = len(shape) == 2 or ((len(shape) == 3) and\n",
        "                                ((3 in shape) or (4 in shape)))\n",
        "    if not valid:\n",
        "        raise ValueError(\"'arr' does not have a suitable array shape for \"\n",
        "                         \"any mode.\")\n",
        "    if len(shape) == 2:\n",
        "        shape = (shape[1], shape[0])  # columns show up first\n",
        "        if mode == 'F':\n",
        "            data32 = data.astype(np.float32)\n",
        "            image = Image.frombytes(mode, shape, data32.tostring())\n",
        "            return image\n",
        "        if mode in [None, 'L', 'P']:\n",
        "            bytedata = bytescale(data, high=high, low=low,\n",
        "                                 cmin=cmin, cmax=cmax)\n",
        "            image = Image.frombytes('L', shape, bytedata.tostring())\n",
        "            if pal is not None:\n",
        "                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n",
        "                # Becomes a mode='P' automagically.\n",
        "            elif mode == 'P':  # default gray-scale\n",
        "                pal = (np.arange(0, 256, 1, dtype=np.uint8)[:, np.newaxis] *\n",
        "                       np.ones((3,), dtype=np.uint8)[np.newaxis, :])\n",
        "                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n",
        "            return image\n",
        "        if mode == '1':  # high input gives threshold for 1\n",
        "            bytedata = (data > high)\n",
        "            image = Image.frombytes('1', shape, bytedata.tostring())\n",
        "            return image\n",
        "        if cmin is None:\n",
        "            cmin = np.amin(np.ravel(data))\n",
        "        if cmax is None:\n",
        "            cmax = np.amax(np.ravel(data))\n",
        "        data = (data*1.0 - cmin)*(high - low)/(cmax - cmin) + low\n",
        "        if mode == 'I':\n",
        "            data32 = data.astype(np.uint32)\n",
        "            image = Image.frombytes(mode, shape, data32.tostring())\n",
        "        else:\n",
        "            raise ValueError(_errstr)\n",
        "        return image\n",
        "\n",
        "    if channel_axis is None:\n",
        "        if (3 in shape):\n",
        "            ca = np.flatnonzero(np.asarray(shape) == 3)[0]\n",
        "        else:\n",
        "            ca = np.flatnonzero(np.asarray(shape) == 4)\n",
        "            if len(ca):\n",
        "                ca = ca[0]\n",
        "            else:\n",
        "                raise ValueError(\"Could not find channel dimension.\")\n",
        "    else:\n",
        "        ca = channel_axis\n",
        "\n",
        "    numch = shape[ca]\n",
        "    if numch not in [3, 4]:\n",
        "        raise ValueError(\"Channel axis dimension is not valid.\")\n",
        "\n",
        "    bytedata = bytescale(data, high=high, low=low, cmin=cmin, cmax=cmax)\n",
        "    if ca == 2:\n",
        "        strdata = bytedata.tostring()\n",
        "        shape = (shape[1], shape[0])\n",
        "    elif ca == 1:\n",
        "        strdata = np.transpose(bytedata, (0, 2, 1)).tostring()\n",
        "        shape = (shape[2], shape[0])\n",
        "    elif ca == 0:\n",
        "        strdata = np.transpose(bytedata, (1, 2, 0)).tostring()\n",
        "        shape = (shape[2], shape[1])\n",
        "    if mode is None:\n",
        "        if numch == 3:\n",
        "            mode = 'RGB'\n",
        "        else:\n",
        "            mode = 'RGBA'\n",
        "\n",
        "    if mode not in ['RGB', 'RGBA', 'YCbCr', 'CMYK']:\n",
        "        raise ValueError(_errstr)\n",
        "\n",
        "    if mode in ['RGB', 'YCbCr']:\n",
        "        if numch != 3:\n",
        "            raise ValueError(\"Invalid array shape for mode.\")\n",
        "    if mode in ['RGBA', 'CMYK']:\n",
        "        if numch != 4:\n",
        "            raise ValueError(\"Invalid array shape for mode.\")\n",
        "\n",
        "    # Here we know data and mode is correct\n",
        "    image = Image.frombytes(mode, shape, strdata)\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "R9WyEERSYiYv"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# sample a random batch and save 6 generated images for visualization\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    _, mask_batch = get_points()\n",
        "    x = data[0].to(device)\n",
        "\n",
        "    temp = torch.from_numpy(mask_batch)\n",
        "    masks = temp.type(torch.FloatTensor).cuda()\n",
        "    if x.size(0) < batchSize:\n",
        "        masks_narrowed = masks.narrow(0, 0, x.size(0))\n",
        "        masked_data = x * (1 - masks_narrowed)\n",
        "    else:\n",
        "        masked_data = x * (1 - masks)\n",
        "\n",
        "    generated_samples = netG(masked_data)\n",
        "\n",
        "    if (i % 10) == 0:\n",
        "        out_path = './outputs/sample_' + str(i) + '.png'\n",
        "\n",
        "        # save one of the generated images as example\n",
        "        toimage(generated_samples.view(-1, 3, 128, 128)[0].cpu().detach().numpy()).save(out_path)\n",
        "print(generated_samples[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XiQrT9USyrA",
        "outputId": "99b8e2ea-bc93-4ac5-b405-5d68499c44ec"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0115, -0.0123, -0.0135,  ..., -0.0122, -0.0134, -0.0138],\n",
            "        [-0.0063, -0.0132, -0.0123,  ..., -0.0134, -0.0132, -0.0178],\n",
            "        [-0.0087, -0.0136, -0.0143,  ..., -0.0131, -0.0138, -0.0166],\n",
            "        ...,\n",
            "        [-0.0066, -0.0142, -0.0120,  ..., -0.0138, -0.0132, -0.0171],\n",
            "        [-0.0101, -0.0156, -0.0156,  ..., -0.0150, -0.0148, -0.0173],\n",
            "        [-0.0099, -0.0167, -0.0141,  ..., -0.0156, -0.0142, -0.0170]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-146-cd9bf7f0465e>:102: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  strdata = np.transpose(bytedata, (1, 2, 0)).tostring()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZE4zO0XmTF0J"
      },
      "execution_count": 144,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}